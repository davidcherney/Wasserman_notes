{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66524a59-9402-4ed8-8495-eeb5c1ed8b81",
   "metadata": {},
   "source": [
    "-----\n",
    "(Comments on previous answers at the end of this answer.)\n",
    "\n",
    "TLDR; The OP has the Chi-squared test of a multinomial distribution in mind. Some answers and comments have Chi-squared tests of a Poisson distribution in mind. The reason for the denominator in question is specific to the case.\n",
    "\n",
    "To OP: \n",
    "\n",
    "You are right on both points; $(O_i-E_i)^2$ is a measure of **distance** between data point $O_i$ to expected value $E_i$ under a distribution. Lets call it the *hypothetical distribution* since the idea is to test the hypothesis that the data came from that distribution. \n",
    "\n",
    "The missing concept seems to be this: we want a measure of the **probability** of that distance. \n",
    "\n",
    "To speak of a probability we need a probability distribution. The \"chi-squared test\" uses the chi-squared distribution to characterize probability. A chi-squared test links the distance to a probability in a two step process\n",
    "1. construct approximately normally distributed random variables from the random variable for your hypothetical distribution\n",
    "2. sum the squares of those approximately normally distributed variables to get an approximately chi-squared distributed random variable. \n",
    "\n",
    "\n",
    "The examples below work up in complexity to \n",
    "1. demonstrate constructing \n",
    "    - approximately normal random variables from the hypothetical distribution\n",
    "    - approximately chi-squared random variables from the sum of their squares\n",
    "2. explain the answer the OP's question.\n",
    "    - The answer is for a chi-squared test of a categorical distribution $\\sum_{i=1}^k \\frac{(O_i-E_i)^2}{E_i}$ is chi-squared distributed.\n",
    "        - By contrast $\\sum_{i=1}^k \\frac{(O_i-E_i)^2}{V_i}$ with $V_i$ variance is not chi-squared distributed.\n",
    "        - this fact is, as far as I can tell, only understandable through the proof. \n",
    "            - I try to explore intuitive paths to the sensation of understanding after the examples. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f73a2-4d33-4ea6-9a95-e1d3a0268182",
   "metadata": {},
   "source": [
    "## Definitions \n",
    "\n",
    "**Definition:** If $X_1,...,X_k \\sim {\\cal N}(0,1)$ are iid then $\\sum\\limits_{j=1}^k X_j^2$ is <u>chi-squared distributed</u> with $k$ degrees of freedom. \n",
    "\n",
    "**Theorem:** The probability distribution for $\\chi^2_k$ has cumulative distribution function $F_{\\chi^2_{k}}  =\\frac1{2\\Gamma\\left(\\frac k2\\right)} \\left( \\frac{v}{2} \\right)^{k/2 -1} e^{âˆ’\\frac v2}$.\n",
    "\n",
    "If you don't feel comforable with that expression on the right, no worries, your calculator and computer are comfortable with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564be3eb-403f-424b-b50f-9126349071d5",
   "metadata": {},
   "source": [
    "## e.g. 1: Chi-squared test of standard unit normal distribution.\n",
    "\n",
    "You hypothesise that an experiement reveals a value for a random variable $X \\sim {\\cal N}(0,1)$ (that means $X$ is normally distributed with mean 1 and variance 0). You perform the experiment $n$ times and get values $x_1,...,x_n$. The chi-squared statistic takes the value $v =\\sum_{i=1}^n x_i^2$.  Accordng to you hypothesis\n",
    "- The probability (density) of getting the values $x_1,...,x_n$ is then,, $f_{\\chi^2_n}(v)$. - The probability of getting the value $v$ or larger is $1-F_{\\chi^2_n}(s)$ where $F_{\\chi^2_n}$ is the CDF.\n",
    "\n",
    "Reject your hypothesis if these probabilities are low. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df45bc-597b-4129-bdf2-b6b79b4997f9",
   "metadata": {},
   "source": [
    "## e.g. 2: Chi squared test of sample mean\n",
    "\n",
    "You hypothesise that an experiment revals a value for a random variable $X\\sim F$ where $F_{\\mu,\\sigma}$ is a (any) distribution with mean $\\mu$ and standard deviation $\\sigma$. You perform the experiment a large number $n$ times to obtain data $x_1,...,x_n$ with sample mean $\\bar{x}_n=\\frac1n \\sum_{i=1}^n x_i$. By the central limit theorem the \"centered and standard scaled\" $Z:=\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim {\\cal N}(0,1)$ for large $n$. This is one normally distributed random variable. Putting in the data, $Z$ takes on the value $z=\\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}$.  The chi-squared statistic takes on the value $v = z^2$. According to your hypothesis\n",
    "- The probability (PDF) of this value is $f_{\\chi^2_{1}} (v)$ \n",
    "- the probability of a value larger than $z^2$ is $1-F_{\\chi^2_1}(z^2)$.\n",
    "\n",
    "Reject if small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6af59-b0b5-4c46-8e23-e3415e4371e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## e.g. 3 : Chi squared test of normality with unknown mean \n",
    "\n",
    "You hypothesise that an experiment revals a value for a random variable $X\\sim {\\cal N}(\\mu,\\sigma)$, but\n",
    "- you do not hypothesise a value of $\\mu$ \n",
    "- you hypothesise a specific number for standard deviation $\\sigma$. \n",
    "\n",
    "You perform the experiment $n$ times to obtain data $x_1,...,x_n$ with sample mean $\\bar{x}_n=\\frac1n \\sum_{i=1}^n x_i$. You construct the random variables\n",
    "$Z_i = \\frac{X_i - \\bar{X}}{\\sigma}$. \n",
    "\n",
    "Here there is an unexpected twist (that is mirrored in the multinomial case below); you might think that $Z_1,...,Z_n$ are $n$ standard normal distributed variables, but they are not. \n",
    "- the $\\{Z_i\\}$ are not independent \n",
    "- a $Z_i$ does not have unit variance; $\\mathbb{V}(Z_i) =1-1\\frac1n$\n",
    "- the sum of the variances of them is $n-1$\n",
    "- you can eliminate one of them to get $n-1$ variables that are normally distributed.\n",
    "- actually have  $n-1$ independent standard unit normal distributed variables.. \n",
    "\n",
    "**Theorem (Cochrain's theorem):*** $\\sum_{i=1}^n Z_i^2 \\sim \\chi^2_{n-1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ab5ca-67ca-42fa-a912-98635f8584bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>\n",
    "**Proof:** \n",
    "</summary>\n",
    "Let $X=(X_1,...,X_n)$ and \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "Z &= \\frac1\\sigma(X_1 - \\bar{X},...,X_n- \\bar{X})^T\\\\\n",
    "&= \\frac1\\sigma (X-\\bar{X}\\vec1)\\\\\n",
    "&= \\frac1\\sigma(I - \\frac1n \\vec1\\vec1^T)X\\\\\n",
    "&= \\frac1\\sigma H X.\n",
    "\\end{array}\n",
    "$$\n",
    "Since $H:=I - \\frac1n \\vec1\\vec1^T$\n",
    "- is symmetric it is diagonalizable (according to the spectral theorem). \n",
    "- is a projection matrix that projects to the compliment of $\\text{Span}(\\vec 1)$,\n",
    "    - so $\\vec 1$ is an eigenvector of $H$ with eigenvalue $0$.\n",
    "    - all other eigenvalues are $1$\n",
    "    \n",
    "The matrix $H$ is diagonalized with a basis of the form $B=\\left( \\frac{1}{\\sqrt{n}} \\vec 1 , b_2,...,b_n\\right)$.\n",
    "Then the matrix and vector transform to\n",
    "$$\n",
    "D:= B^T H B  = \\text{Diag}(0,1,...,1)\\\\\n",
    "W:=B^T Z.\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "$$\\begin{array}{ll}\n",
    "Z^T Z \n",
    "&=  \\frac1{\\sigma^2} X^T H^T  HX\\\\\n",
    "&=  \\frac1{\\sigma^2} X^T  HX\\\\\n",
    "&=\\frac1{\\sigma^2}  X^T(B B^T)  H (B B^T)X\\\\\n",
    "% &=\\frac1{\\sigma^2} (B^TX)^T  (B^T HB) B^TX\\\\\n",
    "&=\\frac1{\\sigma^2} W ^TD W \\\\\n",
    "&=\\frac1{\\sigma^2} \\sum_{i=2}^n  w_i^2.\n",
    "\\end{array}\n",
    "$$\n",
    "Note that there are $n-1$ terms. It remains to be shown that each of those $n-1$ terms is a square of an independent standard unit normal distributed random variable. \n",
    "\n",
    "Since $W$ is a linear transformation of a normally distributed random vector $Z_i$, it is also a normally distributed random vector. \n",
    "\n",
    "The mean of $W$ is $\\mathbb{E}(W) = Q^T\\mathbb{E}(Z)=\\vec0$.\n",
    "\n",
    "The covariance matrix of $W$ is \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathbb{V}(W) \n",
    "&= Q^T \\mathbb{V}(Z)Q \\\\\n",
    "&= Q^T \\mathbb{V}\\left( \\frac{1}{\\sigma} HX\\right)Q \\\\\n",
    "&= \\frac{1}{\\sigma^2} Q^T H^T \\mathbb{V}\\left( X\\right) HQ \\\\\n",
    "&= \\frac{1}{\\sigma^2} Q^T H^T ( \\sigma^2 I) HQ \\\\\n",
    "& =  D \n",
    "\\end{array}\n",
    "$$\n",
    "$\\square$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce2a04-0e1b-465d-8408-f6a8958b7307",
   "metadata": {},
   "source": [
    "The statistic $V=\\sum_{i=1}^n Z^2$ takes on the value $ v = \\sum_{i=1}^n z_i^2$, and according to your hypothsis \n",
    "- the probability (PDF) of $V$ taking on the value $v$ is $f_{\\chi_{n-1}^2}(v)$, \n",
    "- the probability of getting a larger value than $v$ for $V$ is $1-F_{\\chi_{n-1}^2}(v)$.\n",
    "\n",
    "Reject if small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e066394-4966-4a1c-b481-4f5ca9891fab",
   "metadata": {},
   "source": [
    "## e.g. 4 Chi squared test of multinomial data\n",
    "\n",
    "You hypothesise that an experiment gives counts $X_1,...,X_k$ for the number of outcomes of $n$ draws in each of $k$ categories with the categories having probabilities $p_1,..,p_k$. You make $n$ draws and obtain data $x_1,...,x_k$. \n",
    "\n",
    "How do we re-arrange the $X_i$ into variables that are normally distributed and whose sum of squares constritute a chi-squared random variable?\n",
    "\n",
    "The naive approach is to consider that for a fixed draw and fixed category $j$ the draw is a Bernoulli trial for category $j$ with probability of success $p_j$. By the central limit theorem, the sum $X_j$ for $n$ draws from that category has mean $E_j=np_j$, variance $V_j = np_j(1-p_j)$, and standard normalized \n",
    "$$\n",
    "Z_j : = \\frac{X_i-E_j}{\\sqrt{V}_j} \\sim {\\cal N}(0,1)\\text{ as } n\\to \\infty .\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6f7b6-606a-4316-9fa1-f336715a2fe2",
   "metadata": {},
   "source": [
    "Things start to look like e.g. 3 in that \n",
    "- the $Z_i$ are not independent here because the $X_i$ are not independent; meaning\n",
    "    - not linearly independent since $\\sum_{i=1}^k X_n = n$, \n",
    "        - in other notation $\\vec1^T X = n$\n",
    "    - not statistically independent since $f_{X_1,X_2}(x_1,x_2) \\neq f_{X_1}(x_1)f_{X_2}(x_2)$ as one can see from the fact that the product of two binomial distributions is not a multinomial distribution. \n",
    "- we need to detrmine how many degrees of freedom there are in order to make use of a chi-squared distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7466ff7-e7d4-4672-b796-681a086d2cb7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80624db9-67cf-416d-8366-111ceafd0802",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "    <summary>Motivation: There is a trick that is a lot like Cochrain's theorem. </summary>\n",
    "    To motivate it lets start with \n",
    "$$\n",
    "X_i - E_i = X_i - np_i =X_i - \\left(\\vec1^T X\\right)p_i\\\\\n",
    "\\implies \n",
    "X - E = X-n \\vec p = X- \\left(\\vec1^T X\\right) \\vec p = \\left(I-\\vec{p} \\vec{1}^T\\right)X.\n",
    "$$\n",
    "We'd like to diagonalize the matrix $\\left(I-\\vec p \\vec1^T\\right)$ (which to the trained eye clearly has rank $k-1$) but it is not symmetric, disallowing use of the spectral theorem. Below I show ho to obtain a symmetric projectin matrix.\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab6f09-ae13-471f-ae21-acc427c06ec1",
   "metadata": {},
   "source": [
    "Introduce \n",
    "$$\n",
    "H=\\text{diag}\\left(\n",
    "        \\frac{1}{\\sqrt{p_1}} ,\\frac{1}{\\sqrt{p_2}},...,\\frac{1}{\\sqrt{p_k}} \n",
    "    \\right),\\\\\n",
    "\\hat p := H\\vec p = \\left(\\sqrt{p_1},\\sqrt{p_2},...,\\sqrt{p_k}\\right)\n",
    "$$\n",
    "and make note of the properties \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "(i) & H \\hat{p} = \\vec 1,\\\\\n",
    "(ii) & \\hat{p}^T\\hat{p} = 1,\\\\\n",
    "(iii) & \\mathbb{E}(HX) = H n \\vec p = n\\hat p,\\\\\n",
    "(iv) & \\hat{p}^T HX   = n.\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "A geometric way to think is that the new variable $HX$ varies around $n\\hat p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eea91b-1ccd-4395-92c7-42034de25490",
   "metadata": {},
   "source": [
    "Centering $HX$, we define \n",
    "$$\\begin{array}{ll}\n",
    "W &:= HX-\\mathbb{E}(HX) \\\\\n",
    "    &= HX - n\\hat{p} \\\\\n",
    "    &= HX - (\\hat{p}^THX)\\hat{p} \\\\\n",
    "    &= (I-\\hat{p}\\hat{p}^T)HX.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e9a3b-8cb3-46ad-9db0-e72bfaaf9fe9",
   "metadata": {},
   "source": [
    "Before moiving on, note that \n",
    "- the denominator of $W_i = \\frac{X_i}{E_i} - n\\sqrt{p_i} = \\frac{X_i-E_i}{\\sqrt{p_i}}$ **is not** the standard deviation $\\sqrt{np_j(1-p_j)}$ of $X_j$. \n",
    "- $W_i \\sim {\\cal N}\\left(0,\\sqrt{n(1-p)} \\right)$ for large $n$, **not** a standard unit normal.\n",
    "- $\\sum_{j=1}^k \\mathbb{V}(W_i) = n(k-1).$\n",
    "\n",
    "Something else is happening here. The analogy to Cochrain's theorem will bring it out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85138f-372d-46c2-9f96-1b59fe7d2ff6",
   "metadata": {},
   "source": [
    "**Theorem:** $V=\\sum_{i=1}^k W_i^2$ is distributed as $\\chi^2_{k-1}$. \n",
    "\n",
    "Please let me know if you find the name of this theorem. I formulated it and it's proof from looking at <a href src=\"https://ocw.mit.edu/courses/18-443-statistics-for-applications-fall-2006/resources/lecture11/\">MIT Open CourseWare</a> and then playing around. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d500285-51b5-48b0-9874-2dd06efabfe5",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>\n",
    "**Proof:** \n",
    "    </summary>\n",
    "\n",
    "Note that $P : = (I-\\hat{p}\\hat{p}^T)$ hs the properties\n",
    "- $P \\hat{p} =0$\n",
    "- $P b_j = b_j$\n",
    "- $P$ is symmetric, so the spectral theorem applies. \n",
    "    - $P$ has eigenvector $\\hat{p}$ with eigenvalue $0$ and \n",
    "    - all other eigenvalues 1. \n",
    "    \n",
    "Therefore there exists an orthonormal basis of the form \n",
    "\n",
    "$$\n",
    "B= \\left( \\hat{p},b_2,...,b_k\\right)\n",
    "$$\n",
    "such that \n",
    "$$\n",
    "D:=B^T P B = \\text{diag}(0,1,...,1)\n",
    "$$\n",
    "\n",
    "Then by defining\n",
    "$$\n",
    "Y := B^T W \\\\\n",
    "=B^T PHX \\\\\n",
    "=(PB)^T P HX \\\\\n",
    "= ( B D)^T PHX \\\\\n",
    "=(0,b_2,...,b_n)^T  W\\\\\n",
    "=D B^T  W\\\\\n",
    "= DY.\n",
    "$$\n",
    "\n",
    "Then the sum of squares\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "W^T W =  Y^T B^T BY \\\\\n",
    "= Y^T Y\\\\\n",
    "=(DY)^T   DY \\\\\n",
    "= \\sum_{j=2}^k Y_j^2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Note that there are only $k-1$ terms here. \n",
    "\n",
    "It remains to show that each of the $k-1$ terms is the square of a standard unit normal distributed random variable, and that those variables are independent. That is that $Y\\sim {\\cal N}(\\vec 0, D)$.  \n",
    "\n",
    "We start with the observation that $W$ is normally distributed (approximately for large $n$) so that the linear transformation of it $Y=B^T W$ is also normally distrubuted. All that remains is to show that the mean and variance are appropriate.\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathbb{E}(Y) \n",
    "    &= B^T P H \\mathbb{E}(X) \\\\\n",
    "    &=   B^T n H\\vec{p} \\\\\n",
    "    &=   n B^T P  \\hat{p} \\\\\n",
    "    &=   n B^T \\vec{0} \\\\\n",
    "    &=   \\vec{0} \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The variance of $Y_i$ is $1$ for $i=2,...k$ since\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathbb{V}(Y) \n",
    "&= B^T P H \\mathbb{V}(X) P^T H^T B \\\\\n",
    "&= B^T P H  (H^{-2} - \\vec{p}\\vec{p}^T) H P B \\\\\n",
    "&= B^T P  (I - \\hat{p}\\hat{p}^T)  P B \\\\\n",
    "&= B^T   (P - 0)   B \\\\\n",
    "& = (\\hat{p},b_2,...,b_n)^T (0,b_2,...,b_n)\\\\\n",
    "& = D. \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$\\square$\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3581666-8867-43d5-a630-bcc283f06860",
   "metadata": {},
   "source": [
    "Putting the data $x_1,...,x_n$ into $V$ gives $v$. According to your hypotesis\n",
    "- the probability (PDF) of $V$ taking the value $v$ is $f_{\\chi^2_{k-1}}(v)$. \n",
    "- The probability of $V$ being at least as large as $v$ is $1-F_{\\chi^2_{k-1}}(v)$.\n",
    "\n",
    "Reject the hypothesis if these probabilities are small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c133274-4d9a-4f12-86bf-aa60c92e75de",
   "metadata": {},
   "source": [
    "## e.g. 5 Chi squared test of Poisson \n",
    "\n",
    "You hypothesise that an experiment reveals a count of events of a certain kind in a unit of time (or distance) according to a Poisson distribution with mean $\\lambda$; the expectation value over the the unit of time is a constant $\\lambda$. For such a distribution the variance is also $\\lambda$. You perform the experiment $n$ times to obtain $x_1,...,x_n$. \n",
    "\n",
    "Since $X\\sim {\\cal N}(\\lambda,\\sqrt{\\lambda})$ for large $\\lambda$, we have \n",
    "$$\n",
    "\\frac{X_i - \\lambda}{\\sqrt{\\lambda}} \\sim {\\cal N}(0,1)\n",
    "$$\n",
    "and \n",
    "$$\n",
    "V:=\\sum_{i=1}^{n}\\frac{(X_i - \\lambda)^2}{{\\lambda}} \\sim \\chi^2_{n}.\n",
    "$$\n",
    "The value of $V$ on the data is $v= \\sum_{i=1}^n \\frac{(x_i-\\lambda)^2}{\\lambda}$. According to your hypothesis\n",
    "- The probability (PDF) of the data is $f_{\\chi^2_n}\\left( v\\right)$ \n",
    "- the probability that the value of  $V$ is at least as large as $v$ is $1-F_{\\chi^2_n}(v)$. \n",
    "\n",
    "Reject if small. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c7c20-4d66-46bd-9f4d-85d7229b1189",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "For a chi-squared test of a categorical distribution, the counts $X_1,...,X_k$ for the categories are not independent since $\\sum_{j=1}^k X_j =1$. Intuitively, there are $k-1$ degrees of freedom. Intuitively, you want to take the $k$ random variables $X_i - E_i$ and sum their squares with a normalization factor on each term that makes the variance work out. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889de79-6a91-476c-8880-f61a86e28b30",
   "metadata": {},
   "source": [
    "## Comments on other answers\n",
    "\n",
    "OP you seem to have only the multinomial case (e.g.4) in mind. \n",
    "\n",
    "@Jacob Maibach \n",
    "\n",
    "1. Noite that for the Poisson case $\\lambda$ is the same for all elements of the sample. You seem to have another case where $k$ different experiments are hypothesised to have $k$ different Poisson distributions with means $E_i, ...,E_k$, and you run each experiment once. That is not likely to be what the OP had in mind. \n",
    "\n",
    "2. It is false that $\\frac{O_i - E_1}{\\sqrt{E_i}}$  is normally distributed with unit variance. \n",
    "    - For the multinomial case, it is ${\\cal N}(0,n-E_i)$ distributed in the limit of large $n$. \n",
    "    - For the $k$ Possion distributions case, it is ${\\cal N}(0,1)$ distributed in the limit of large $E_i$. \n",
    "\n",
    "@ ilcj and everyone saying \"to normalize the numerator\"... that is qualitatively correct, but the question is quantitative.\n",
    "- Why $E_i$ in the deonominator? and \n",
    "    - not $O_i$ or \n",
    "    - not $E_i^2$  \n",
    "    - not $nE_i(1-E_i)$, the variance\n",
    "\n",
    "In the end, that answer comes only from the theorem I prove in e.g. 4 and is very specific to the multinomial case; the total . \n",
    "\n",
    "Trying to be helpful by pointing out a common mistake I have seen in many areas of math... you get in trouble when you do not pay attention to ownsership. The problem this page faced was that the phrase \"the chi squared test\" was never given ownership. Namely, the page should have said \"the chi squared test of a multinomial distribution.\" Distringuishing whether the owner of the test was a multinomial distribution, a normal distribution, a poisson distribution, etc makes a lot of difference, as my examples show. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b2be0-601e-4fd4-ad4c-3941bd2b107c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
