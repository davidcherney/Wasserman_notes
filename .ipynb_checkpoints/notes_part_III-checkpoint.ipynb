{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff9b80c-bd0e-4840-b66e-8867af45aea7",
   "metadata": {},
   "source": [
    "# Part 3: Statistical Models and Methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e106bbb-f5c8-4018-b169-ccb44e17d048",
   "metadata": {},
   "source": [
    "# ch 13 Linear and Logistic Regression \n",
    "\n",
    "**Theorem**. Under the assumption of Normaly distributed residuals, the least squares estimator is also the maximum likelihood estimator.\n",
    "\n",
    "maximizing $l(β_0, β_1, σ)$ over $σ$, yields the mle\n",
    "$\\hat σ^2 = \\frac1n \\sum\\limits_{i=1}^n \\epsilon_i^2$ where $\\epsilon_i$ is the $i$th residual. \n",
    "\n",
    "An unbiased estimate of σ2 is\n",
    "$\\hat σ^2 = \\frac{a}{n-2} \\sum\\limits_{i=1}^n \\epsilon_i^2$ where $\\epsilon_i$ is the $i$th residual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b3aff-d289-43d5-b7a3-5a9c29b0a0cc",
   "metadata": {},
   "source": [
    "## 13.3 Propertie of Lease Squared Estimators\n",
    "**Theorem** Let $\\hat β^T = (\\hat β_0 , \\hatβ_1 )^T$ denote the least squares estimators for the parameteris in the fit $Y= \\beta_0 + \\beta_1 X$. Then,\n",
    "$$\n",
    "\\mathbb{E} ( \\hat β | X^n ) =\n",
    "\\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right)\\\\\n",
    "\\mathbb{V} ( \\hat β | X^n ) \n",
    "= \n",
    "\\frac{\\sigma^2/n}{s^2_X} \n",
    "\\left(\\begin{array}{cc} \n",
    "\\frac1n \\sum_i X_i^2 - \\bar X_n^2 &  -\\bar X_n \\\\ \n",
    "-\\bar X_n & 1\n",
    "\\end{array}\\right)\\\\\n",
    "$$\n",
    "where $s^2_X = n^{−1} \\sum\\limits_{i=1}^n (X_i−\\bar X_n)^ 2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c1bae-f8bf-4896-9f69-91f42a3b38a0",
   "metadata": {},
   "source": [
    "**Theorem** Under appropriate conditions we have:\n",
    "1. (Consistency): $\\hat β_0 \\stackrel{P}{\\to} β_0$ and $β_1\\stackrel{P}{\\to} β_1$.\n",
    "2. (Asymptotic Normality): $\\frac{\n",
    "        \\hat \\beta_0−\\beta_0\n",
    "        }\n",
    "        {\n",
    "        \\hat{\n",
    "            \\text{se} \n",
    "            }\n",
    "         ( \\hat \\beta_0) \n",
    "         }  \n",
    "\\stackrel{Dist}{\\to} N(0,1)\n",
    "$\n",
    "and \n",
    "$\\frac{\\hat β_1−β_1}{\\hat{\\text{se}}( \\hat \\beta_1)} \\stackrel{Dist}{\\to} N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c40ff-e9b4-4389-8a20-4569dca9acd8",
   "metadata": {},
   "source": [
    "## 13.4 Prediction\n",
    "Having estimated $\\hat r(x) = \\hat β_0 + \\hat β_1 x$ from data $(X_1, Y_1), . . . , (X_n, Y_n)$, \n",
    "we observe the value $X = x_∗$ of the covariate for a new subject and we want to predict their outcome $Y_∗$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82999d8-967c-4a0f-b9a5-daa667fff2ca",
   "metadata": {},
   "source": [
    "An estimate of $Y∗$ is\n",
    "$$\\hat Y_∗ = \\hat β_0 + \\hat β_1 x_∗.$$ \n",
    "How much variance do you expect in this prediction among different samiles $X_1,...,X_n$? Using the formula for the variance of the sum of two random variables,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27043026-95f8-4f3d-a959-6f044dfcb3c5",
   "metadata": {},
   "source": [
    "$$\\mathbb{V}(\\hat Y_∗) = \\mathbb{V}( \\hat β_0 + \\hat β_1 x_∗) \\\\\n",
    "= \\mathbb{V}(\\hat β_0) + x^2_∗ \\mathbb{V}(\\hat β_1) + 2x_∗\\text{Cov}(\\hat β_0, \\hatβ_1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc85652-a526-4c03-8493-f6800720fa84",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\frac{\\sigma^2/n}{s_X^2}\\left(\n",
    "\\frac1n \\sum\\limits_i X_i^2 -\\bar X_n^2   \n",
    "+ x_*^2 \n",
    "+ 2 x_* \\bar X_n \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d0af1-aea6-4870-baca-47d03ad84d8e",
   "metadata": {},
   "source": [
    "13.5 subsection on multiple regression\n",
    "\n",
    "pretty easy.\n",
    "\n",
    "# 13.6 Model Selection\n",
    "\n",
    "A smaller model with fewer covariates has two advantages\n",
    "1. it might give better predictions than a big model and \n",
    "2. it is more parsimonious (simpler). \n",
    "\n",
    "Generally, as you add more variables to a regression, the bias of the predictions decreases and the variance increases.\n",
    "\n",
    "In model selection there are two problems: \n",
    "1. assigning a “score” to each model which measures, in some sense, how good the model is, and \n",
    "2. searching through all the models to find the model with the best score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe04e91-9850-4a2c-be8f-6a94e3a5e81f",
   "metadata": {},
   "source": [
    " Let $S \\subset \\{1, . . . , k\\}$ and let $X_S = \\{X_j : j \\in S\\}$ denote a subset of the covariates.\n",
    " \n",
    "Our goal is to choose $S$ to minimize the prediction risk\n",
    "$$\n",
    " R(S) = \\sum\\limits_{i=1}^n E(\\hat{Y}_i(S) − Y^∗_i)^2\n",
    "$$\n",
    "\n",
    "where $Y_i^∗$ denotes the value of a future observation of $Y_i$ at covariate value $X_i$. (what?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99a11f-0682-407c-b89d-3eab2ec51f0c",
   "metadata": {},
   "source": [
    "The training error is defined to be \n",
    "$$\n",
    "\\hat R_{\\text{tr}}(S) = \\sum\\limits_{i=1}^n\\left(\\hat{Y}_i(S) - Y_i\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610dfb5f-d01e-4561-b0ad-780893451859",
   "metadata": {},
   "source": [
    "**Theorem** The training error is a downward-biased estimate of the prediction risk; $$\n",
    "R(S) =\n",
    "\\mathbb{E}\\left(\\hat{R}_{tr} (S) \\right)<  R(S)\n",
    "$$ \n",
    "because\n",
    "$$\n",
    "\\text{bias}( R_{tr}(S)) = \\mathbb{E}(R_{tr}(S)) − R(S) = −2 \\sum_{i=1} \\text{Cov}(\\hat Y_i, Y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ca5c0-4016-43d3-bd40-8f6333c36942",
   "metadata": {},
   "source": [
    "**Defnition** The estimate of the risk \n",
    "$\\hat R(S) = \\hat R_\\text{tr}(S) + 2\\vert S \\vert \\hat\\sigma^2$ \n",
    "where $\\hat\\sigma^2$ is the estimate of $\\sigma^2$ obtained from the full model (with all covariates in the model) is **Mallow’s $C_p$ statistic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035b676-5f28-49e6-aad9-8f66c972e6d8",
   "metadata": {},
   "source": [
    "Think of it as lack of fit + complexity penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef91d6-913c-4514-83d8-f4ec28fa11f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3acbc2e4-606c-48a8-b063-e7a412e314c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2849172e-6bb7-4647-b53b-be075c7d5fae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "030239ca-0fbf-4102-bf29-2e69e9f6bafa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "212b5162-de59-4d03-9960-90a6252471a0",
   "metadata": {},
   "source": [
    "# 14 Multivariate Models \n",
    "\n",
    "# 15 Inference About Independence \n",
    "\n",
    "# 16 Causal Inference\n",
    " \n",
    " \n",
    "# 17 Directed Graphs and Conditional Independence\n",
    "\n",
    "# 18 Undirected Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848ca50-bf2c-4c3d-87f4-88a05e2d7c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
