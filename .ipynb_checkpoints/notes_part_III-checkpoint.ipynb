{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff9b80c-bd0e-4840-b66e-8867af45aea7",
   "metadata": {},
   "source": [
    "# Part 3: Statistical Models and Methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112544c-9e7f-4127-b09b-4d6f44b14947",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ch 13 Linear and Logistic Regression \n",
    "\n",
    "**Definition:** A <u>regression function</u> for a response random variable $Y$ and predictor random variable $X$ is the function $r$ dermined by their joint distribution $f_{X,Y}$ by \n",
    "$$\n",
    "r(x) = \\mathbb{E}_{Y|X}(Y|X=x) = \\int y f_{Y|X}(y|x)dy \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ff250-cb28-4e13-bc70-dba2b43ba6c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Definition:** In <u>regression analysis</u> we use a random sample $(X_1,Y_1),...,(X_n,Y_n)$ of the random vector $(X,Y)$ to create an estimator $\\hat{r}$ of the regression function $r$ of $(X,Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4736b5-a889-4315-8efd-2b5b696e8eae",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "There are two ways to form these estimators;\n",
    "1. parameteric estimation\n",
    "    - create a estimator of the regression function that is an element of a parameterized model $\\{r_\\theta | \\theta \\in \\Theta\\}$.\n",
    "2. Nonparametric estimation, ch20 and 21\n",
    "\n",
    "**Definition:** In a <u>simple linear regression model</u> one looks for a regression function in the parameterized model $\\{\\beta_1x+\\beta_0|\\beta_1,\\beta_0 \\in \\mathbb{R}\\}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a905b94-d5c5-49b8-99a5-2c5abdfa3dc3",
   "metadata": {},
   "source": [
    "Wasserman adds $\\mathbb{E}(\\epsilon_i|X_i) = 0,~\\mathbb{V}(\\epsilon_i|X_i)=\\sigma^2$ here, but I am unclear if these conditions are trully needed for a simple linear regression model with minimum assumptions. I try to proceed without it... and seem to succeed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6224a-22ac-4fa1-9d8e-bfb81258a4a1",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "**Definition:** The <u>predictor</u> using the estimator $\\hat{\\theta}$ from the regression model $\\{r_\\theta | \\theta\\in \\Theta\\}$ is $r_{\\hat{\\theta}}$. \n",
    "\n",
    "Let $\\hat{\\beta}_i$ be estimators of $\\beta_i$ in a simple linear regression model. The predictor using these estimators is the affine function $r_{\\hat{\\beta_i}}$ such that $r_{\\hat{\\beta_i}}(x)=\\hat{\\beta_1}x + \\hat{\\beta}_0$. \n",
    "\n",
    "Terminology: The predictor is also called the <u>fitted model</u>, or in the simple linear case the <u>fitted line</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c1de7-1064-4eb5-9ca2-7d1489df7018",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "**Definition:** The <u>predicted values</u> from the predictor $r_{\\hat{\\beta_i}}$ for the random sample $(X_1,Y_1),...,(X_n,Y_n)$ are the random variables $\\left\\{r_{\\hat{\\beta_i}}(X_i)|i=1,...,n\\right\\}$.\n",
    "\n",
    "Ambiguity: I think the first option, but I can't tell. \n",
    "- Is $X_2$ used simultaneously in $\\beta_1$ and in the argument of $r_{\\hat{\\beta_i}}(X_2)$ so that if I'm calculating $\\mathbb{E}_{X_2}r_{\\hat{\\beta_i}}(X_2)$ I need to take into account both places that $X_2$ occurs? \n",
    "- Or am I meant to think of a observatino for the random sample and then put a random variable in ?\n",
    "\n",
    "**Notation:** The predicted values for the random sample $\\{(X_1,Y_1)|i=1,...,n\\}$ are denoted $\\hat{Y}_i:=r_{\\hat{\\beta_i}}(X_i)$.\n",
    "\n",
    "**Definition** The <u>residuals</u>  are $\\left\\{ \\hat{\\epsilon_i} = Y_i - \\hat{Y}_i|i=1,...,n\\right\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b361dc-6924-4adb-aef9-a137136b30d9",
   "metadata": {},
   "source": [
    "While we are not going to look for the distribution $f_{X,Y}$, we are going to examine the residuals as a way to estimate the quantities\n",
    "$$\n",
    "\\epsilon _i = Y_i - (\\beta_1 X_i +\\beta_0) \\, . \n",
    "$$\n",
    "I think, in particular, that we will make assumptions about the $\\epsilon_i$ and then see if the observation for the random sample is of low proability under that assumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907fb59-94c8-4934-9789-82e201b6cafd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2ba1fb8-0ab3-415b-95ea-95d9faca1058",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<details>\n",
    "    <summary>I do not know how to reconcile this with $Y=r(x)+\\epsilon$.</summary>\n",
    "Attempting to reconcile... \n",
    "    \n",
    "When taking an observation $y$ of $Y$ there will be error $e$ in $r(x)$ so that \n",
    "$$\n",
    "y = r(x) + \\epsilon\n",
    "$$\n",
    "Promoting $y$ and $e$ to random variables $Y$ and $\\epsilon$, \n",
    "$$\n",
    "Y= r(x) +\\epsilon \\\\ \n",
    "Y = \\mathbb{E}_{Y|X=x}(Y) + \\epsilon \\\\\n",
    "\\,.\n",
    "$$\n",
    "Thus, our assumption amounts to the assumption that there is a random variable $\\epsilon$ that.... No, **the equation above feels like we define $\\epsilon$ in terms of $f_{X,Y}$** since that determines $r(x)$ and $Y$. \n",
    "\n",
    "I think this serves as a definition of $\\epsilon$. But I think we should be introducing a new random variable so that there is a new joint distribution $f(x,y,e)$ and I think that we will use that 3 variable distribution to characterize the probability that data came from a linear relationship. \n",
    "\n",
    "Since $x$ is just an observation of $X$, and since the relationship holds for all values of $x$, we can generalize to the random variable $X$.  \n",
    "$$\n",
    "Y=\\beta_1X + \\beta_0 +\\epsilon\n",
    "$$\n",
    "Each random variable from a random sample will potentially have a different error term $\\epsilon_i$ so that \n",
    "$$\n",
    "Y_i= \\beta_1 X_i +\\beta_0 +\\epsilon_i \\, . \n",
    "$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2f075-b135-49e4-be89-32ba64adfe51",
   "metadata": {
    "tags": []
   },
   "source": [
    "The most common hypothesis about the $\\epsilon_i$ is that $\\mathbb{E}(\\epsilon_i) = 0 $ and $\\mathbb{V}(\\epsilon_i) = \\sigma^2$, and these quantities are constant with respect to $i$. Before proceeding, we need estimators of $\\beta$.\n",
    "\n",
    "\n",
    "## 13.2 Least squares estimator and MLE\n",
    "\n",
    "### Least Squares Estimator\n",
    "\n",
    "**Definition:** The <u>least squares estimator</u> of $\\beta$ is the minimizer of the sum of squares of residuals $\\sum_i\\hat{\\epsilon_i}$.\n",
    "\n",
    "That last quantity is called the residual sum of squares, RSS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69d29c-d8e5-45e9-a40d-a14bd445724a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "**Theorem:** The least squares estimator $\\hat{\\beta}$ of $\\beta$ under the sample $(X,Y)^n$ is \n",
    "$$\n",
    "\\hat{\\beta}\n",
    "=\\left(\\begin{array}{c}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_0 \\end{array}\\right)  \n",
    "=\\left( \\begin{array}{c}  \n",
    "\\frac{\\sum_{i=1}^n (X_i - \\bar{X}_n)(Y_i - \\bar{Y}_n)}{\\sum_{j=1}^n (X_j - \\bar{X}_n)^2}\n",
    "\\\\\n",
    "\\bar{Y}_n - \\beta_1 \\bar{X}_n\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf0326-668d-4b2f-82f7-f900dfd50fbd",
   "metadata": {},
   "source": [
    "**Proof:** Let $X^n = (X_1,...,X_n)$, $Y^n=(Y_1,...,Y_n)$. Let $1 = (1,...,1)$ so that we can define the averages $\\bar{X}_n = \\frac{X^n 1^T}{n}$, $\\bar{Y}_n =\\frac{ Y 1^T}{n}$. Using them, we defined the centered random sample $\\check{X} = X^n - \\bar{X}_n 1$, $\\check{Y}= Y^n - \\bar{Y}_n 1$. \n",
    "\n",
    "Note the properties $ \\check{X}^n1^T =0$ and $ \\check{Y}^n 1^T=0 $.\n",
    "\n",
    "Inserting these centered random samles into the objective function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3209263-f2ad-49b3-82cf-768b87c00388",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n \\hat{\\epsilon}^2_i \n",
    "    & = (Y^n - \\beta_1 X^n - \\beta_0 1^T)^2\\\\\n",
    "    & = \\left( \\check{Y}^n + \\bar{Y}_n 1 \n",
    "        -\\beta_1 \\check{X}^n -\\beta_1 \\bar{X}_n 1 -\\beta_0 1\\right)^2\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "The stationary equation with respect to $\\beta_0$ is \n",
    "$$0= \\left( \\check{Y}^n + \\bar{Y}_n 1 \n",
    "        -\\beta_1 \\check{X}^n -\\beta_1 \\bar{X}_n 1 -\\beta_0 1\\right)\n",
    "        1^T\\\\\n",
    "\\iff 0=  0 + \\bar{Y}_n n \n",
    "        -0 -\\beta_1 \\bar{X}_n n -\\beta_0 n\\\\\n",
    "\\iff \\beta_0  =  \\bar{Y}_n   -\\beta_1 \\bar{X}_n \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0724a9-b020-4694-a1c0-2a46ca3e9356",
   "metadata": {},
   "source": [
    "Putting this back into the objective function gives \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n \\hat{\\epsilon}^2_i \n",
    "    & = \\left( \\check{Y}^n -\\beta_1 \\check{X}^n \\right)^2 \\, .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e203b-0e36-44c5-955f-10e4a7ae40e3",
   "metadata": {},
   "source": [
    "The stationary equation with respect to $\\beta_1$ is then \n",
    "$$\n",
    "0 = \\left( \\check{Y}^n -\\beta_1 \\check{X}^n \\right) (\\check{X}^n)^T \\\\\n",
    "\\iff \\beta_1 = \\frac{ \n",
    "    \\check{X}^n \\left( \\check{Y}^n\\right)^T\n",
    "    }{\n",
    "    \\check{X}^n \\left(\\check{X}^n\\right)^T \n",
    "    } \\, .\n",
    "$$\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca21eb-0630-43a0-8b9d-30b4816ed31a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### MLE\n",
    "\n",
    "Now we add the assumption that $\\epsilon_i|X_i \\sim {\\cal N}(0,\\sigma^2)$.\n",
    "\n",
    "Note the difference between the phrase \"normally distributed residuals\" which refers to $\\hat{\\epsilon}_i$, and \"normally distributed residuals\"\n",
    "\n",
    "**Theorem**. Under the assumption of normaly distributed $\\epsilon_i$, the least squares estimator is also the maximum likelihood estimator.\n",
    "\n",
    "**Proof:** First consider that if $X$ if fixed to $x$ then $Y - \\beta_1 x -\\beta_0 = \\epsilon \\sim {\\cal N}(0,\\sigma^2)$ and therefore $Y\\sim {\\cal N}(\\beta_1 x +\\beta_0)$. That is, $f_{Y|X} = {\\cal N}(\\beta_1X + \\beta_0,\\sigma^2)$. \n",
    "\n",
    "Although we do not know $f_{X,Y}$ we can say that the likelihood of the random sample $(X,Y)^n$ is \n",
    "$$\n",
    "\\begin{align*}\n",
    "{\\cal L}(\\beta,\\sigma) \n",
    "    & = \\prod\\limits_{i=1}^n f_X(X_i,Y_i) \\\\\n",
    "    & =  \\prod\\limits_{i=1}^n f_X(X_i) f(Y_i|X_i) \\\\\n",
    "    & \\approx  \\frac{1}{\\sqrt{2\\pi}\\sigma^n}\\exp\\left( \n",
    "         \\sum\\limits_{j=1}^n \\frac{ -\\left(Y_i - \\mathbb{E}(Y_i|X_i)\\right)^2}{2\\sigma^2}\\right) \\\\\n",
    "    & =  \\frac{1}{\\sqrt{2\\pi}\\sigma^n}\\exp\\left( \n",
    "         \\sum\\limits_{j=1}^n \\frac{ -\\left(Y_i - r(X_i) \\right)^2}{2\\sigma^2}\\right) \\\\\n",
    "    & =  \\frac{1}{\\sqrt{2\\pi}\\sigma^n}\\exp\\left( \n",
    "         \\sum\\limits_{j=1}^n \\frac{ -\\left(Y_i - \\beta_1 X_i - \\beta_0 \\right)^2}{2\\sigma^2}\\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where we have used that $f_X$ does not depend on $\\beta,\\sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8e3e2-b9bd-40e1-ab50-3b413cc79d5f",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "The log likelihood is then\n",
    "$$\n",
    "\\begin{align*}\n",
    "l(\\beta,\\sigma) \n",
    " & = -\\sum\\limits_{i=1}^n \\frac{ \\left(Y_i - \\beta_1 X_i - \\beta_0 \\right)^2}{2\\sigma^2} \n",
    "- n\\log\\sigma\\\\\n",
    " & = -\\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^n \\hat{\\epsilon_i}^2\n",
    "- n\\log \\sigma \\, . \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Clearly the stationay equations for $\\beta_1,\\beta_0$ are the same as for least squares.\n",
    "\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be505b68-9526-448e-82bd-631ebb9c1cd9",
   "metadata": {},
   "source": [
    "We further get an MLE from the parameter $\\sigma$ under this assumption of normal residuals;\n",
    "$$\n",
    "\\hat{\\sigma}_{\\text{MLE}} = \\frac1n \\sum_{i=1}^n \\hat{\\epsilon_i}^2 \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582cb55-2a0e-4317-a86b-07af2f39cf7f",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "This MLE is biased; an unbiased estimator is \n",
    "$$\n",
    "\\hat{\\sigma} = \\frac1{n-2} \\sum_{i=1}^n \\hat{\\epsilon_i}^2 \\,.\n",
    "$$\n",
    "I do not know why the 2 is there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dbd6b0-e5a6-4595-a2c1-5eeefba9c0ec",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da937350-c22d-4f07-af2c-989378b000aa",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108e5e69-9e06-4544-a3aa-dac81471a665",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "114c8986-6afb-4767-9678-ef09d0a1ff14",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "An unbiased estimate of $\\sigma^2$ is\n",
    "$\\hat \\sigma^2 = \\frac{1}{n-2} \\sum\\limits_{i=1}^n \\hat{\\epsilon}_i^2$ where $\\hat{\\epsilon}_i$ is the $i$th residual.My understanding of this does not extend beyon having two relations between the data, $\\beta_0,\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b3aff-d289-43d5-b7a3-5a9c29b0a0cc",
   "metadata": {},
   "source": [
    "## 13.3 Properties of Lease Squared Estimators\n",
    "**Theorem** Let $\\hat{\\beta}^T = (\\hat{\\beta}_0 , \\hat{\\beta}_1 )^T$ denote the least squares estimators for the parameteris in the fit $Y= \\beta_0 + \\beta_1 X$. Then,\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "(i) & \\mathbb{E} \\left( \\hat{\\beta} | X^n \\right) =\n",
    "\\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right),\\\\\n",
    "(ii) & \\mathbb{V} ( \\hat{\\beta} | X^n ) \n",
    "= \n",
    "\\frac{\\sigma^2/n}{s^2_X} \n",
    "\\left(\\begin{array}{cc} \n",
    "\\frac1n \\sum_i X_i^2 - \\bar X_n^2 &  -\\bar X_n \\\\ \n",
    "-\\bar X_n & 1\n",
    "\\end{array}\\right)\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "where $s^2_X = n^{−1} \\sum\\limits_{i=1}^n (X_i−\\bar X_n)^ 2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c1bae-f8bf-4896-9f69-91f42a3b38a0",
   "metadata": {},
   "source": [
    "**Theorem** Under appropriate conditions we have:\n",
    "1. (Consistency): $\\hat{\\beta}_0 \\stackrel{P}{\\to} \\beta_0$ and $\\hat{\\beta}_1\\stackrel{P}{\\to} \\beta_1$.\n",
    "2. (Asymptotic Normality): $\\frac{\n",
    "        \\hat \\beta_0−\\beta_0\n",
    "        }\n",
    "        {\n",
    "        \\hat{\n",
    "            \\text{se} \n",
    "            }\n",
    "         ( \\hat \\beta_0) \n",
    "         }  \n",
    "\\stackrel{Dist}{\\to} N(0,1)\n",
    "$\n",
    "and \n",
    "$\\frac{\\hat{\\beta}_1−\\beta_1}{\\hat{\\text{se}}( \\hat \\beta_1)} \\stackrel{Dist}{\\to} N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c40ff-e9b4-4389-8a20-4569dca9acd8",
   "metadata": {},
   "source": [
    "## 13.4 Prediction\n",
    "Having estimated $\\hat r(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ from data $(X_1, Y_1), . . . , (X_n, Y_n)$, \n",
    "we observe the value $X = x_∗$ of the covariate for a new subject and we want to predict their outcome $Y_∗$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82999d8-967c-4a0f-b9a5-daa667fff2ca",
   "metadata": {},
   "source": [
    "An estimate of $Y_∗$ is\n",
    "$$\\hat{Y}_∗ = \\hat{ \\beta  }_0 + \\hat{ \\beta  }_1 x_∗.$$ \n",
    "How much variance do you expect in this prediction among different samples $X_1,...,X_n$? Using the formula for the variance of the sum of two random variables,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27043026-95f8-4f3d-a959-6f044dfcb3c5",
   "metadata": {},
   "source": [
    "$$\\mathbb{V}(\\hat Y_∗) = \\mathbb{V}( \\hat  \\beta_0 + \\hat  \\beta_1 x_∗) \\\\\n",
    "= \\mathbb{V}(\\hat  \\beta_0) + x^2_∗ \\mathbb{V}(\\hat  \\beta_1) + 2x_∗\\text{Cov}(\\hat  \\beta_0, \\hat \\beta_1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc85652-a526-4c03-8493-f6800720fa84",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\frac{\\sigma^2/n}{s_X^2}\\left(\n",
    "\\frac1n \\sum\\limits_i X_i^2 -\\bar X_n^2   \n",
    "+ x_*^2 \n",
    "+ 2 x_* \\bar X_n \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6d017-c985-43c4-8ec4-976c63648acd",
   "metadata": {},
   "source": [
    "I want to campare this to my ISL notes, but do not seem to have them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1e68d-6116-4980-8fe7-a0e952cece3f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "13.5 subsection on multiple regression\n",
    "\n",
    "pretty easy.\n",
    "\n",
    "# 13.6 Model Selection\n",
    "\n",
    "A smaller model with fewer covariates has two advantages\n",
    "1. it might give better predictions than a big model and \n",
    "2. it is more parsimonious (simpler). \n",
    "\n",
    "Generally, as you add more variables to a regression, the bias of the predictions decreases and the variance increases.\n",
    "\n",
    "In model selection there are two problems: \n",
    "1. assigning a “score” to each model which measures, in some sense, how good the model is, and \n",
    "2. searching through all the models to find the model with the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc6b34-5676-47fb-8522-89a9ee4d7981",
   "metadata": {},
   "source": [
    "Let \n",
    "- $k$ be the number of covatiates available.  \n",
    "- $S \\subset \\{1, . . . , k\\}$ \n",
    "- ${\\cal X}_S = \\{X_j : j \\in S\\}$ denote a subset of the covariates. \n",
    "- $X_s$ the $|S|\\times n$ data matrix \n",
    "- $Y$ the response data\n",
    "- ${\\beta}_S$ be the coefficients for the model with covariates ${\\cal X}_S$\n",
    "- $\\hat{\\beta}_S$ be their OLS estimators\n",
    "    - a function of $(X_s,Y)$, the training data. \n",
    "- $\\hat{r}_S$ the OLS estimator of the linear function with covariates ${\\cal X}_S$.\n",
    "    - a function of $(X_s,Y)$, the training data.\n",
    "- $\\hat{Y}_i(S):= \\hat{r}_S(X_i)$ for all $i \\in \\{1,...,n\\}$. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e4ac4-fae0-40d3-9495-7b0e9ff9b55c",
   "metadata": {},
   "source": [
    "Our goal is to choose $S$ to minimize the prediction risk\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    " R(S) & = \\sum\\limits_{i=1}^n \\mathbb{E}_{(X_S,Y,X^*_S,Y^*)} \\left(\\hat{Y}_i(S) - Y^*_i\\right)^2\\\\\n",
    "      & = \\sum\\limits_{i=1}^n \\mathbb{E}_{(X_S,Y,X_s^*,Y^*)} \\left(\\hat{r}_S(X_i^*) - Y^*_i\\right)^2\n",
    "\\end{array}\n",
    "$$\n",
    "where $(X^*_i,Y^*_i)$ are future observations of $Y_i$ at covariate value $X_i$. (what?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a9714-1626-4589-9693-77da76e1ca58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Two possibilities:\n",
    "1. There are two sets of random variables, \n",
    "    - training random variables to determine $\\hat{r}_S$\n",
    "    - the random variables $(X_i^*,Y_i^*)$ over which the expectations are computed.\n",
    "        - pros: There is a reason for the seperate notation $Y^*$.\n",
    "        - cons: \n",
    "            - $R(S)$ still has RVs in it after the expectation; it is a statistic. \n",
    "                - unless expectation is over both sets of RVs\n",
    "            - why the same $n$?\n",
    "        \n",
    "2. There is one set of random variables\n",
    "    - $\\hat{r}_S$ is determined by $(X_s,Y)$\n",
    "    - The expectation is over the same random variables\n",
    "        - pros: simpler\n",
    "        - cons: why the differen notation with a star? \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99a11f-0682-407c-b89d-3eab2ec51f0c",
   "metadata": {},
   "source": [
    "**Definition:** The <u>training error</u> is \n",
    "$$\n",
    "\\hat R_{\\text{tr}}(S) = \\sum\\limits_{i=1}^n\\left(\\hat{Y}_i(S) - Y_i\\right)^2 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610dfb5f-d01e-4561-b0ad-780893451859",
   "metadata": {},
   "source": [
    "**Theorem** The training error is a downward-biased estimate of the prediction risk; \n",
    "$$\n",
    "\\mathbb{E}\\left(\\hat{R}_{tr} (S) \\right)<  R(S)\n",
    "$$ \n",
    "because\n",
    "$$\n",
    "\\text{bias}( R_{tr}(S)) = \\mathbb{E}\\big( R_{tr}(S) \\big) - R(S)  = −2 \\sum_{i=1} \\text{Cov}(\\hat Y_i, Y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85cf00-c5c5-4d8b-88d9-763db987beb4",
   "metadata": {},
   "source": [
    "Conceptually, this bias happens because the data is used twice\n",
    "- to estimate the coefficientes $\\beta$ as $\\hat{\\beta}$\n",
    "- to estimate the risk $R(S)$ as $\\hat{R}_{\\text{tr}}$.\n",
    "\n",
    "When there are many covariates in the model (when $|S|$ is large) this covariance term is large and thus the bias is large.\n",
    "\n",
    "Here are some estimators of risk that avoid this pathology. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ca5c0-4016-43d3-bd40-8f6333c36942",
   "metadata": {},
   "source": [
    "## Mallows’s \n",
    "**Defnition** The estimate of the risk \n",
    "$\\hat R(S) = \\hat R_\\text{tr}(S) + 2\\vert S \\vert \\hat\\sigma^2$ \n",
    "where $\\hat\\sigma^2$ is the estimate of $\\sigma^2$ obtained from the full model (with all covariates in the model) is **Mallows’s $C_p$ statistic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035b676-5f28-49e6-aad9-8f66c972e6d8",
   "metadata": {},
   "source": [
    "Think of it as lack of fit + complexity penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef91d6-913c-4514-83d8-f4ec28fa11f6",
   "metadata": {},
   "source": [
    "## AIC\n",
    "**Definition:** The <u>Akaike Information Criterion</u> is the choice of $S$ as \n",
    "$$\n",
    "\\underset{S}{\\text{argmax}} \\left( {\\cal l}_S - |S| \\right) \n",
    "$$\n",
    "where ${\\cal l}_S$ is the (me: expected) log-likelihood of the model $ {\\cal F} = \\{ f(\\cdot;\\theta)| \\theta \\in \\Theta\\}$ evaluated at the MLE $\\hat{\\theta}_{\\text{mle}}$. That is, \n",
    "$$\n",
    "{\\cal l}_S = \\sum\\limits_{i=1}^n f(x) \\log f(X_i;\\hat{\\theta}_{\\text{MLE}}).\n",
    "$$\n",
    "\n",
    "What is meant by $f(x)$ there? Does this quantity depend on $\\theta$? \n",
    "\n",
    "There seems to be too little infor to figure this out. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbc2e4-606c-48a8-b063-e7a412e314c2",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "**Definition:** In <u> leave-one-out cross-validation</u> the risk estimator is \n",
    "$$\n",
    "R_{\\text{CV}}\\sum_{i=1}^n\\left( Y_i - \\hat{Y}_{(i)}\\right)^2\n",
    "$$\n",
    "where $\\hat{Y}_{(i)}$ is the model obtained from the set of $n-1$ random variables \n",
    "$\\big\\{  (X_j,Y_j) | j\\in \\{1,...,n\\} - \\{i\\} \\big\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849172e-6bb7-4647-b53b-be075c7d5fae",
   "metadata": {},
   "source": [
    "**Definition:** In <u>k-fold cross-validation</u> the training set is partitioned into $k$ parts $S_1,...,S_k$ and construct the predictors $\\hat{Y}_j$ by training on $S_{(j)} := \\cup_{j'\\neq j} S_{j'}$, and use risk estimator \n",
    "$$\n",
    "R_{k\\text{CV}} \n",
    "= \n",
    "\\frac1k \\sum_{j=1}^k \n",
    "\\sum_{(X_i,Y_i)\\in S_{(k)} } (Y_i - \\hat{Y}_{(j)} (X_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83ab73-3d6a-45ee-96ce-7e4914d6f1d5",
   "metadata": {},
   "source": [
    "## Bayesian information criterion\n",
    "\n",
    "**Definition:** In <u>Bayesian information criterion</u> the choice of $S$ is \n",
    "$$\n",
    "\\underset{S}{\\text{argmax} } \\,\\text{BIC} (S) := \n",
    "\\underset{S}{\\text{argmax} } \\left(l_s - \\frac{|S|}{2}\\log n\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b137f-160d-4fc9-84bf-f16931b7cb06",
   "metadata": {},
   "source": [
    "Choosing the model with highest BIC is like choosing the model with highest posterior probability because if $[S_1,...,s_m]$ are some models and one puts\n",
    "- a prior $\\mathbb{P}(S_i)=\\frac 1m$ on the categorical distribution across the $m$ models\n",
    "- a smooth prior on each model\n",
    "$$\n",
    "\\mathbb{P}(S_j|{\\text{data}}) \\approx \\frac{ e^{\\text{BIC}(S_j)}}{\\sum_{j'} e^{\\text{BIC}(S_{j'})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccbd2f-0abb-4913-97c9-a7337fd5bbb8",
   "metadata": {},
   "source": [
    "The BIC score also has an information-theoretic interpretation in terms of something called minimum description length. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a123b-cb49-4199-a760-cbc0a84275d8",
   "metadata": {},
   "source": [
    "## 13.7 Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff948e-031c-40aa-9273-74bbeb5496e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59b56dcc-e5b6-4cb6-83f0-478dd313ef19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd792171-6e1e-45c3-bf83-54c639d8ea29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eff2733b-6e91-4fc1-835f-fd4b20c36056",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c49da3-17f6-454b-a145-ed6fdf4a68b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd107ef-f65a-4480-9d52-867f4cc8c34b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 14 Multivariate Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995f77e-2ed4-4f0b-8c06-3f587a2bbaf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "A confidece interval for the correlation between $X_1$ and $X_2$ can be obtained by the following method by Fisher:\n",
    "\n",
    "1. Estimate the correlation \n",
    "$\\rho = \\frac{\\mathbb{E}\\left((X_1 - \\mathbb{E}(X_1) \\right)\\left(X_2 - \\mathbb{E}(X_2) \\right)}{\\sigma_1,\\sigma_2}$ with the non-parametric plug in estimator \n",
    "$\\hat{\\rho} := \\frac{\\sum_{i=1}^n\\left(X_{1i} - \\bar{X}_1 \\right)\\left( X_{2i} - \\bar{X}_2 \\right)}{s_1 s_2}$\n",
    "    - $s_i : = \\frac{1}{n-1}\\sum_{i=1}^n\\left(X_{1i} - \\bar{X}_1 \\right)^2$ and the $n-1$ is what makes this the \"non-parametric\" plug in estimator; the parametric one has $\\frac1n$.\n",
    "2. Compute $\\hat{\\theta} = \\frac12 \\left(  \\log (1+\\hat \\rho) - \\log(1-\\hat{\\rho})  \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc163e58-3abe-425c-882f-ce7d7dac5705",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. Compute $\\hat{\\text{se}}(\\hat{\\theta})$. It is $\\frac{1}{\\sqrt{n-3}}$. \n",
    "\n",
    "4. A $1-\\alpha$ confidence interval for $\\hat{\\theta}$ is \n",
    "$(a,b) = \n",
    "\\left( \n",
    "\\hat{\\theta} - \\frac{Z_{\\frac{\\alpha}{2}}}{\\sqrt{n-3}}\\, , \\, \n",
    "\\hat{\\theta} + \\frac{Z_{\\frac{\\alpha}{2}}}{\\sqrt{n-3}}\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af81caf-a2de-4d5d-93bb-bc6306da166e",
   "metadata": {
    "tags": []
   },
   "source": [
    "5. Invert the transform in step 2 with $f^{-1}(z) = \\frac{e^{2z}-1}{e^{2z}+1}$ to obtain the $1-\\alpha$ confiodence interval for $\\rho$\n",
    "$$\n",
    "\\left( \n",
    " \\frac{e^{2a}-1}{e^{2a}+1}\n",
    "\\, , \\, \n",
    " \\frac{e^{2b}-1}{e^{2b}+1}\n",
    "\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd29096-b365-409c-935b-bb1c4eea7ae7",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e053fb01-88a4-4baf-83f0-a83eeb0bd894",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9750e01a-2eee-448b-bd5c-6fe4099667df",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2155998-36cb-4447-909d-1026b3f9c085",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80731b23-d10e-45d0-8989-ac7287054a32",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 15 Inference About Independence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d338f7ff-e2c5-4bed-bd0f-ebbb26671e1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "When Y and Z are not independent, we say that they are dependent or associated or related. \n",
    "\n",
    "1. How do we test if two random variables are independent?\n",
    "2. How do we estimate the strength of dependence between two random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433b54a-cfaa-4ba3-a4e8-77eb12e758b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Odds\n",
    "**Definition:** The <u>odds</u> of the event $A$ is $\\text{odds}(A):= \\frac{\\mathbb{P}(A)}{1-\\mathbb{P}(A)}$. \n",
    "\n",
    "Starting with a pair of binary random variables that we wish to measure the probability of dependence, we will go conceptual;\n",
    "- let $E$ be exposure to something (smoking, exercise)\n",
    "- let $D$ be an outcome (disease, ability to do a flip)\n",
    "\n",
    "$$\n",
    "\\text{odds}(D|E) = \\frac{\\mathbb{P}(D|E)}{1-\\mathbb{P}(D|E)}\\\\\n",
    "\\text{odds}(D|E^c) = \\frac{\\mathbb{P}(D|E^c)}{1-\\mathbb{P}(D|E^c)}\n",
    "$$\n",
    "\n",
    "**Definition:** The <u>odds ratio</u> is $\\psi = \\frac{\\text{odds}(D|E)}{\\text{odds} (D|E^c)}$\n",
    "\n",
    "**Definition:** The <u>log odds ratio</u> is $\\gamma = \\log \\psi = \\log \\frac{\\text{odds}(D|E)}{\\text{odds} (D|E^c)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e2f3d-516c-4c5c-b510-760c1efe3866",
   "metadata": {
    "tags": []
   },
   "source": [
    "Order the events $(E,D)$ and consider the probabilites $p_{ij}$ and counts after $n$ draws for the 4 outcomes as in the tables below.\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "&D^c & D \\\\\\hline\n",
    "E^c  & p_{00}&p_{01} \\\\\n",
    "E    & p_{10}& p_{11}\\\\\n",
    "\\end{array}\n",
    "\\,, \\,\\,\n",
    "\\begin{array}{c|cc}\n",
    "&D^c & D \\\\\\hline\n",
    "E^c  & X_{00}&X_{01} \\\\\n",
    "E    & X_{10}& X_{11}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac25a7-f768-493e-b118-c8429c339f72",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\psi \n",
    "&= \n",
    "\\frac{ \\text{odds}(D|E)}  {\\text{odds}(D|E^c)}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    \\frac{\\mathbb{P}(D|E)}{1-\\mathbb{P}(D|E)}\n",
    "    }{\n",
    "    \\frac{\\mathbb{P}(D|E^c)}{1-\\mathbb{P}(D|E^c)}\n",
    "    }\n",
    "\\\\&=\n",
    "\\frac{\n",
    "    \\frac{\\mathbb{P}(D,E) /\\mathbb{P}(E)}{1-\\mathbb{P}(D,E)/\\mathbb{P}(E)}\n",
    "    }{\n",
    "    \\frac{\\mathbb{P}(D,E^c) /\\mathbb{P}(E^c)}{1-\\mathbb{P}(D,E)/\\mathbb{P}(E^c)}\n",
    "    }\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    \\frac{\\mathbb{P}(D,E) }{/\\mathbb{P}(E)-\\mathbb{P}(D,E)}\n",
    "    }{\n",
    "    \\frac{\\mathbb{P}(D,E^c) }{\\mathbb{P}(E^c)-\\mathbb{P}(D,E)}\n",
    "    }\n",
    "\\\\\n",
    "&=\n",
    "\\frac{ \n",
    "    \\frac{p_{11}}{p_{01}+p_{11}-p_{11}}\n",
    "    }\n",
    "    {\n",
    "    \\frac{p_{10}}{p_{00}+p_{01}-p_{01}}\n",
    "    }\n",
    "\\\\\n",
    "&=\\frac{ \n",
    "    \\frac{p_{11}}{p_{01}}\n",
    "    }\n",
    "    {\n",
    "    \\frac{p_{10}}{p_{00}}\n",
    "    }\n",
    "\\\\\n",
    "&=\\frac{p_{00} p_{11}}{p_{01}p_{10}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a565f5-9337-4144-a6a8-97023ab369ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "To construct estimators we need to know how we sample. There are three methods.\n",
    "\n",
    "**Notation:** $p_{i\\cdot}: = \\sum_jp_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bfdcc-d8f1-4be7-9b36-b817e71625fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Thee methods of sampling\n",
    "\n",
    "**Definition** In <u>multinomial sampling</u> we sample people from a population and record their exposure and disease status so the observable is the number of peopele in each class $X=(X_{00},X_{01},X_{10},X_{11}) \\sim \\text{Multinomial}(n,p)$ with $p=(p_{00},p_{01},p_{10},p_{11})$.\n",
    "\n",
    "And since the MLEs $\\hat{p}_{ij} = \\frac{X_{ij}}{n}$ we have the estimator\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\psi} \n",
    " &=\\frac{\\hat{p}_{00} \\hat{p}_{11}} {\\hat{p}_{01}\\hat{p}_{10}}\\\\\n",
    " &= \\frac{X_{00}X_{11}}{X_{01}X_{10}}\\,.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1d0ac-4ae2-4227-ae02-ea0991ab565b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Definition:** In <u>cohort sampling</u> we get $x_{1\\cdot}$ exposed and $x_{0\\cdot}$ unexposed people and count the number with disease in each group, so the observables are\n",
    "$$\n",
    "X_{01} \\sim \\text{Binomial}(x_{0\\cdot}, \\mathbb{P}(D|E^c)) \\\\\n",
    "X_{11} \\sim \\text{Binomial}(x_{1\\cdot}, \\mathbb{P}(D|E  )).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7234a-f906-4afa-b57b-71575343ef74",
   "metadata": {
    "tags": []
   },
   "source": [
    "While we can not estimate every quantity in the table, we can estimate the two we need to calculate the odds ratio:\n",
    "$$\n",
    "\\hat{\\mathbb{P}}(D|E^c) = \\frac{ X_{01} }{x_{0\\cdot}},~\n",
    "\\widehat{\\text{odds}}(D|E^c) = \\frac{ X_{01} }{x_{0\\cdot} - X_{01}}\\, ,\n",
    "\\\\\n",
    "\\hat{\\mathbb{P}}(D|E) \n",
    "= \\frac{X_{11}}{x_{1\\cdot}},~\n",
    "\\widehat{\\text{odds}}(D|E) = \\frac{ X_{11} }{x_{1\\cdot} - X_{11}} \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e29c1-8f72-4d6e-99d6-475407e0e70c",
   "metadata": {
    "tags": []
   },
   "source": [
    "From that we can calculate \n",
    "$$\n",
    "\\hat{\\psi} \n",
    "=\\frac{\\widehat{\\text{odds}}(D|E)}{\\widehat{\\text{odds}}(D|E^c)}\n",
    "= \\frac{X_{00}X_{11}}{X_{01}X_{10}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d5edc-52aa-4a03-b1c8-af8ae061a8ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Definition:** In <u> retrospective sampling</u> we get some $x_{\\cdot 1}$ diseased\n",
    "and $x_{\\cdot 0}$ non-diseased people and we observe how many are exposed, giving\n",
    "$$\n",
    "X_{10}\\sim \\text{Binomial}(x_{\\cdot 0},\\mathbb{P}(E|D^c)) \\\\\n",
    "X_{11}\\sim \\text{Binomial}(x_{\\cdot 1},\\mathbb{P}(E|D  )).\n",
    "$$\n",
    "So we have estimators \n",
    "$$\n",
    "\\hat{\\mathbb{P}}(E|D^c) = \\frac{X_{10}}{x_{\\cdot0}} ,~ \n",
    "    \\widehat{\\text{odds}}(E|D^c) = \\frac{X_{10}}{x_{\\cdot0} - X_{10}}\n",
    "\\\\\n",
    "\\hat{\\mathbb{P}}(E|D  ) = \\frac{X_{11}}{x_{\\cdot1}},~\n",
    "        \\widehat{\\text{odds}}(E|D) = \\frac{X_{11}}{x_{\\cdot1} - X_{11}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b424fec4-6921-4332-b670-ad25eeb4a20a",
   "metadata": {
    "tags": []
   },
   "source": [
    "and from those we can calculate \n",
    "$$\n",
    "\\hat{\\psi} \n",
    "=\\frac{\\widehat{\\text{odds}}(E|D)}{\\widehat{\\text{odds}}(E|D^c)}\n",
    "= \\frac{X_{00}X_{11}}{X_{01}X_{10}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095ce14-3138-4a67-aff9-f4bca4bd30e2",
   "metadata": {},
   "source": [
    "**Note:**, The meaning of $\\psi$ changes depending on the sampling methods\n",
    "- $\\frac{\\widehat{\\text{odds}}(D|E)}{\\widehat{\\text{odds}}(D|E^c)}$ for multinomial sampling \n",
    "- $\\frac{\\widehat{\\text{odds}}(E|D)}{\\widehat{\\text{odds}}(E|D^c)}$ for retorspective sampling\n",
    "- $\\frac{\\widehat{\\text{odds}}(D|E)}{\\widehat{\\text{odds}}(D|E^c)}$ for cohort sampling \n",
    "\n",
    "but we use the same estimator in each case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2a227-ee04-4a77-a3c7-8098533bfd01",
   "metadata": {},
   "source": [
    "**Theorem:** The following are equivalent (characterizations of independence):\n",
    "1. $D\\perp E$\n",
    "2. $p_{ij} = p_{i\\cdot}p_{j\\cdot}$ for all $i,j$\n",
    "3. $\\psi =1$\n",
    "4. $\\gamma =0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c547cf-5d59-46f0-a405-dbf076590ccb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing\n",
    "\n",
    "We want a statistic to test\n",
    "$$\n",
    "H_0: D\\perp E\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcdf27b-4bbe-4d71-8ab0-2e96e965e0c0",
   "metadata": {},
   "source": [
    "Note that under $H_0$ we can form an estimator $\\hat{E}_{ij}$ of the expected value $E_{ij}$ of $X_{ij}$; \n",
    "since $E_{ij} = np_{ij}$ we estimate\n",
    "$$\\begin{align*}\n",
    "\\hat{E}_{ij}    = n\\hat{p}_{ij} \n",
    "                \\stackrel{H_0}{=} n\\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j} \n",
    "                = n \\frac{X_{i\\cdot}}{n}\\frac{X_{\\cdot j}}{n}\n",
    "\\end{align*}\n",
    "$$\n",
    "Note also that for disambiguation of expectation value and its estimatorI decided to put a hat on this, Wasserman did not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d563235-6072-44b2-9cec-4d4d56541590",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pearson test for independece\n",
    "**Definition** The Person statistic for independence is \n",
    "$$\n",
    "U =\\sum_{i,j} \\frac{(X_{ij} - \\hat{E}_{ij})^2}{\\hat{E}_{ij}} \\, .\n",
    "$$\n",
    "\n",
    "**Theorem:** Under $H_0$ the random variable $U$ is asymptotically distributed as $\\chi^2_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23a2a0-1bef-4eeb-8287-d13f6fce85e2",
   "metadata": {},
   "source": [
    "**Question:** This is like having 4 categories, and the Pearson test of categorical distributions would have $4-1=3$ degrees of freedom. Why is there just one? \n",
    "\n",
    "**Answer:** Wikipedia says that in the case of two discrete random variables $C,D$ where $C$ takes on $m$ values and $D $ takes on $n$ values, the degrees of freedom for the chi-squared test statistic for independence of the two variables is $(m-1)(n-a)$. The inuitive argument given is about the contingency table; once you know $X_{i\\cdot}$ then you only need to know $m-1$ elements of a column for $j$ to fill in the missing entry in the columns, and similarly for $X_{\\cdot j}$ and a row. I suspect that there is a proof  for this that is  like Cochrain's theorem or what must becalled Pearson's theorem about the test of multinomial data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d562f7d-243a-41f1-976b-350190905efa",
   "metadata": {},
   "source": [
    "**Perason test for independence** A level $\\alpha$ test is obtained by rejecting $H_0$ when $U > F^{-1}_{\\chi^2_1}(1-\\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19611afe-1631-4062-aeac-0e550c0a85cb",
   "metadata": {},
   "source": [
    "### Likelihood ratio test\n",
    "\n",
    "**Definition:**  The <u> Likelihood ratio test statistic</u> is\n",
    "$$\n",
    "T = 2\\sum_{ij} X_{ij} \\log \\frac{X_{ij}X_{\\cdot \\cdot}}{X_{i\\cdot}X_{\\cdot j}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cd876-8b34-4194-abb1-876f49659934",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Wilks theorem\n",
    "**Theorem (Wilks's Theorem):** Under $H_0$ the statistic $T$ is asymptotically distributed as $\\chi^2_1$. \n",
    "\n",
    "A proof was constructed by <a href src=\"https://www.jstor.org/stable/2957648\">Wilks</a> in 1938. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a232d2-8b4d-41b7-ada3-5953e80d27e3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Here is an outline</summary> from what I piece together from <a href src=\"https://stats.stackexchange.com/questions/52829/why-is-a-likelihood-ratio-test-distributed-chi-squared\">stack exchange</a>.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fe9b6-f490-4fe3-b434-d27173a15836",
   "metadata": {},
   "source": [
    "- the argument of the log is $\\frac{\\hat{p}_{ij}}{\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}}$, which is one if $H_0$ is true and the estimate is perfect. \n",
    "\n",
    "- $\\sum_{ij}X_{ij}\\log \\left(\\frac{p_{ij}}{p_{i\\cdot} p_{\\cdot j}}\\right) = \\log \\prod_{ij}(p_{ij})^{X_{ij}} -  \\log \\prod_{ij}(p_{i\\cdot}p_{\\cdot j})^{X_{ij}}= \\log {\\cal l}(H_1,X)-\\log {\\cal l}(H_0,X)$ where ${\\cal l}$ is the log likelihood of the data $X$. \n",
    "\n",
    "The central idea is about Taylor expanding about the MLE. Since it takes me through some new teriroty on Taylor expansion of functions centered on optima subject to constraints, lets do it!\n",
    "    \n",
    "##### Taylor expand about constrained optimum\n",
    "In general, if we optimize $f$ subject to contraint $g$ then we solve \n",
    "$$\\begin{array}{c}\n",
    "(x^*, \\lambda^*) = \\underset{x,\\lambda}{\\text{argmax}}\\, {\\cal L}(x,\\lambda)\\\\\n",
    "{\\cal L}(x,\\lambda) = f(x)+\\lambda g(x)\\, .\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e8415-86bd-4afa-bba7-fefa0d04e9a4",
   "metadata": {},
   "source": [
    "analytically by solving\n",
    "$$\n",
    "\\nabla_x {\\cal L}(x,\\lambda) = 0 \\\\\n",
    "g(x)=0 \\,.\n",
    "$$\n",
    "\n",
    "But in this case Taylor expanding $f$ about the maximizer $x$ does not yield a zero first order term;\n",
    "$$\n",
    "f(x) \\approx f(x^*) + \\nabla f(x^*)(x-x^*) + \\cdots \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43409e8d-14d0-4556-91a6-42c1eee2e93d",
   "metadata": {},
   "source": [
    "Thus, we instead expand ${\\cal L}$ about $x^*$ to obtain\n",
    "$$\n",
    "\\begin{align*}\n",
    "{\\cal L}(x,\\lambda^*) \n",
    "    &\\approx \\color{blue}{{\\cal L}(x^*, \\lambda^*)} \n",
    "    + \\color{red}{\\nabla {\\cal L}(x^*) (x-x^*)}\n",
    "    + \\frac12 (x-x^*)^T \\nabla^T\\nabla {\\cal L} (x^*)(x-x^*)  \\\\\n",
    "    & = \\color{blue}{f(x^*) + \\lambda^*g(x^*) }\n",
    "    +\\color{red}{0}\n",
    "        + \\frac12 (x-x^*)^T H_f(x^*)(x-x^*)\n",
    "        +\\frac12 \\lambda^* (x-x^*)^T H_g (x^*)(x-x^*)\\\\\n",
    "    & = \\color{blue}{f(x^*)+0 }\n",
    "        + \\frac12 (x-x^*)^T H_f(x^*)(x-x^*)\n",
    "        +\\frac12 \\lambda^* (x-x^*)^T H_g (x^*)(x-x^*)\\\\\n",
    "\\, .\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45999fb-9e9e-4309-8fbb-fc3934b9ea84",
   "metadata": {},
   "source": [
    "##### MLE for p\n",
    "\n",
    "Under the assumption $H_0$, with the statistic $X_{ij}$ in place of data, with the objective function being the log likelyhood of the data and the constraints $\\sum_i p_{i\\cdot}=1, \\sum_jp_{\\cdot j}=1$ the Lagrangian is\n",
    "$$\n",
    "{\\cal L}(p_{i \\cdot},p_{\\cdot j},\\lambda,\\mu) \n",
    "    =  \\log \\prod_{ij} (p_{i\\cdot}p_{\\cdot_j})^{X_{ij}} \n",
    "        + \\lambda \\left(\\sum_{i}p_{i \\cdot} -1 \\right) \n",
    "        + \\mu \\left(\\sum_{j}p_{ \\cdot j} -1 \\right)  \\,.\n",
    "$$\n",
    "\n",
    "The stationary equation for $p_{k\\cdot}$ is\n",
    "$$\n",
    "\\partial_{p_{k\\cdot}}{\\cal L}(p) =0\\\\\n",
    "\\Leftrightarrow \n",
    "\\sum_{ij} \\frac{X_{ij}}{p_{i \\cdot} p_{\\cdot j}} \\delta_{ki}p_{\\cdot j} +\\lambda =0 \\\\\n",
    "\\Leftrightarrow \\sum_{j} \\frac{X_{kj}}{p_{\\cdot k}} = -\\lambda \\\\\n",
    "\\Leftrightarrow \\sum_{j} X_{kj}  = -\\lambda p_{\\cdot k} \\\\\n",
    "\\Rightarrow \\sum_{kj} X_{kj}  = -\\lambda  \\\\\n",
    "\\Rightarrow n  = -\\lambda  \\\\\n",
    "\\Rightarrow p_{k \\cdot} = \\frac{ \\sum_j X_{i j}}{n} =\\frac{X_{k\\cdot}}{n} \\,.\n",
    "$$\n",
    "\n",
    "Similarly for $p_{\\cdot k}$. Therefore the MLEs are\n",
    "$$\n",
    "\\hat{p}_{k \\cdot} = \\frac{X_{k\\cdot}}{n}\\\\\n",
    "\\hat{p}_{ \\cdot k } = \\frac{X_{\\cdot k}}{n} \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279c60b-713c-41bd-8151-cece0b7d58b8",
   "metadata": {},
   "source": [
    "Under $H_1$, the Lagrangian is \n",
    "$$\n",
    "{\\cal L}_0(p,\\lambda) = \\sum_{ij}X_{ij} \\log p_{ij} +\\lambda\\left( \\sum_{ij} p_{ij} -1 \\right).\n",
    "$$\n",
    "\n",
    "with stationary equations\n",
    "$$\n",
    "\\frac{X_{ij}}{p_{ij}} + \\lambda =0\n",
    "\\iff X_{ij} = -\\lambda p_{ij}\n",
    "\\implies n = -\\lambda\n",
    "\\implies \\hat{p}_{ij} = \\frac{X_{ij}}{n} \\, .\n",
    "$$\n",
    "\n",
    "##### Taylor expand H_1 Lagrangian about MLE\n",
    "\n",
    "The Hessian of ${\\cal L}_1(p,\\lambda^*)$ (with respect to $p$) is diagonal. \n",
    "In the Taylor expansion of ${\\cal L}$ about the MLE we evaluate it at $\\hat{p}_{ij}$. \n",
    "$$\n",
    "\\begin{align*}\n",
    "H_{{\\cal L}_1}(\\hat{p}_{ij}) \n",
    "&= - \\text{diag} \\left(\n",
    "        \\frac{X_{11}}{\\hat{p}_{11}^2},..., \\frac{X_{mn}}{\\hat{p}_{mn}^2}\n",
    "        \\right)\\\\\n",
    "&= - n\\,\\text{diag} \\left(\n",
    "        \\frac{1}{\\hat{p}_{11}},..., \\frac{1}{\\hat{p}_{mn}}\n",
    "        \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb7fd1-399e-4f51-a946-e32e3288d2b2",
   "metadata": {},
   "source": [
    "So the Taylor expansion about $\\hat{p}_{ij}$ is\n",
    "$$\n",
    "{\\cal L}(p,\\lambda^*) \\approx \\sum_{ij} X_{ij} \\log \\hat{p}_{ij} -\\frac{n}{2}\\sum_{ij}\\frac{(p_{ij} - \\hat{p}_{ij})^2}{\\hat{p}_{ij}} \\, .\n",
    "$$\n",
    "\n",
    "Maximizing this approximation over $H_0$ means evaluating this at the MLE for $H_0$, which is $\\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j}$, we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "{\\cal L}(\\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j},\\lambda^*) \n",
    "&\\approx \\sum_{ij} X_{ij} \\log \\hat{p}_{ij} \n",
    "    -\\frac{n}{2}\\sum_{ij}\\frac{(\\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j} - \\hat{p}_{ij})^2}\n",
    "    {\\hat{p}_{ij}}\\\\\n",
    "&=    \\sum_{ij} X_{ij} \\log \\hat{p}_{ij} \n",
    "    -\\frac{1}{2}\\sum_{ij}\\frac{(\\hat{E}_{ij} - X_{ij})^2}\n",
    "    {n\\hat{p}_{ij}}\\\\\n",
    "    \\, .\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1338bb8-2512-468f-a907-92df61a61b0f",
   "metadata": {},
   "source": [
    "Oh no! Since the denominator was evaluated $\\hat{p}_{ij}$ by virtue of coming from the Hessian, the denominator is NOT $\\hat{E}_{ij}$! \n",
    "\n",
    "Oh joyful day of struggle :) \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1f4a0-2aba-47ac-a5e3-75a0b02062bd",
   "metadata": {},
   "source": [
    "**Likelihood ratio test:** \n",
    "A level $\\alpha$ test is obtained by rejecting $H_0$ when $T>F^{-1}_{\\chi^1_1}(1-\\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be4c33-1e46-4645-87c3-592a195b1d4a",
   "metadata": {},
   "source": [
    "### Wald test\n",
    "Further, since the MLE of $\\phi$ and $\\gamma$ are $\\hat{\\phi} , \\hat \\gamma$ as above, they are both asymptotically normally distributed. Thus we can use a wald test on the standard unit normal distributed \n",
    "$$\n",
    "W = \\frac{\\hat{\\gamma}}{\\text{se}(\\hat \\gamma)},\n",
    "$$\n",
    " rejcting when \n",
    "$$\n",
    "0 \\notin \\left(\\hat \\gamma - \\text{se}(\\hat \\gamma) Z_{\\alpha/2},\\,\\hat \\gamma + \\text{se}(\\hat \\gamma) Z_{\\alpha/2} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a067ce3-df87-46d8-b24d-d3bed13a222f",
   "metadata": {},
   "source": [
    "## One discrete, one continuous\n",
    "\n",
    "Let $Y:\\Omega \\to \\{1,..,I\\}$ and $Z:\\Omega \\to \\mathbb{R}$. Let $F_i(z) = \\mathbb{P}(Z<z|Y=i)$. \n",
    "\n",
    "**Theorem:** $Y\\perp Z \\Leftrightarrow F_i=F_j \\, \\forall i,j$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f57dd7-d201-4c94-9660-d74a089e2fa2",
   "metadata": {},
   "source": [
    "Therefore, the hypothesis $Y\\perp Z$ can be framed as \n",
    "$$\n",
    "H_0: F_1 = F_2=...=F_I.\n",
    "$$\n",
    "\n",
    "### Kolmogorov-Smirnov test: \n",
    "\n",
    "Let $\\hat{F}_i(z) = \\frac1{n_i}\\sum_{i=1}^n I(Z<z)I(Y=i)$ be the emperical CDFs of $F_i$ and $F_j$. Define the statistic \n",
    "$$\n",
    "D_{ij} = \\sup_{x}\\left|\\hat{F}_i(x) - \\hat{F}_j(x) \\right|.\n",
    "$$\n",
    "Let $H(t) = 1-2\\sum\\limits_{i=1}^\\infty (-1)^{j-1}e^{-2j^2 t^2}$.\n",
    "\n",
    "**Theorem:** Under $H_0$ the probability $\\mathbb{P}\\left(\\sqrt{\\frac{n_in_j}{n_i+n_j}} D_{ij}< t\\right) = H(t)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f9e34-6fd0-4ffd-b5ca-facae1cc3bf1",
   "metadata": {},
   "source": [
    "The test is then to reject $H_0$ if $D > \\sqrt{\\frac{n_i +n_j}{n_i n_j}} H^{-1}(1-\\alpha)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08b545-c385-405c-b5d8-82a16f564801",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d4e22b-038a-4ce3-8ac8-c6b6075d4018",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a07cc54-29ee-40b7-a161-66df8c37ded4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869e2ef4-2cf3-4c7d-9e06-d0eeb549a654",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8591b054-6bd6-4b8d-ad89-da214aad1edb",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 16 Causal Inference\n",
    " \n",
    "May 25 2024... I'm having a hard time reading my own writing below. it might be best to re-do notes on this chapter from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19cf4c-d647-4cc8-baa1-401571df462a",
   "metadata": {},
   "source": [
    "## Binary case\n",
    "Suppose that $X$ is a binary treatment variable where $X = 1$ means “treated” and $X = 0 $ means “not treated.”\n",
    "\n",
    "Let $Y$ be some outcome variable such as presence or absence of disease.\n",
    "\n",
    "We introduce two new random variables $(C_0,C_1)$, called potential outcomes; \n",
    "- $C_0$ is the outcome if the subject is not treated (X = 0) and \n",
    "- $C_1$ is the outcome if the subject is treated (X = 1).\n",
    "\n",
    "**Definition:** The <u>consistency relationship</u> is \n",
    "$$\n",
    "Y = \\left\\{  \n",
    "\\begin{array}{cc}\n",
    "C_0 {\\text{ if }} X=0 \\\\ \n",
    "C_1 {\\text{ if }} X=1 \\\\ \n",
    "\\end{array}\n",
    "\\right\\} :=C_X.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f99340-189a-44eb-a7da-34de9be3d893",
   "metadata": {},
   "source": [
    "The consistency relationship appears to me to be a deterministic ideal. Stochastic processes will imperfectly realize this relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a46263-208f-4649-a19d-b3388cf7d669",
   "metadata": {},
   "source": [
    "## Counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d09b0-13f0-46f1-a590-68c15d8e0304",
   "metadata": {},
   "source": [
    "Note that when $X=0$ we do not observe $C_1$, and so we do not get to form estimates of $\\mathbb{P}(C_1|X=0)$. There are, however, 4 classes if $Y\\in\\{0,1\\}$:\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\\hline\n",
    "\\text{type} & C_0 & C_1 \\\\\\hline\n",
    "\\text{Survivor} & 1 &1 \\\\ \n",
    "\\text{Responder} & 0 & 1 \\\\\n",
    "\\text{Anti-responder} &1& 0 \\\\\n",
    "\\text{Doomed} & 0 &0\\\\\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bfee8-0f2d-4b07-b2ee-3a1a8228021e",
   "metadata": {},
   "source": [
    "**Definition:** A <u>couterfactual</u> is a potential outcome, or the state of affairs that would have happened in the absence of the cause. \n",
    "\n",
    "Namely, $C_0|(X=1)$ is a counterfactual; it is the outcome (a person) would have experienced if counter to the fact of them recieving treatment 0 they had recieved treatment 1. Thus $\\mathbb{P}(C_0|X=1)$ is the probability of the outcome $C_0$ given that there was tretment given. \n",
    "\n",
    "Similarly for $C_1|X=0$. Since counterfactuals are the outcomes for events ($X$ values) that did not happen, they are fundamentally impossible to observre. Hence the name. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557096c3-abf4-4362-85e2-384597a6c43d",
   "metadata": {},
   "source": [
    "**Definition:** The <u>average causal effect</u>\n",
    "$\\theta = \\mathbb{E}(C_1) − \\mathbb{E}(C_0)$.\n",
    "\n",
    "Note that knowledge of counterfactuals is, in general, required to know the average causal effect; $\\mathbb{E}(C_0)$ is estimated over all patients including those with $X=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461dca7-6c69-4e48-ac37-036ce4903881",
   "metadata": {},
   "source": [
    "**Definition:** The <u>association</u> $\\alpha = \\mathbb{E}(Y|X = 1)−\\mathbb{E}(Y|X = 0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513ca1b-8a3d-477a-a18a-8fe53517ab45",
   "metadata": {},
   "source": [
    "**Theorem:** Association is not equal to causation.\n",
    "\n",
    "Proof (By counter example): If the data is augmented by the counterfactuals (marked with $*$ below) to yield the following table then\n",
    "$$\n",
    "\\theta = 0\\\\\n",
    "\\alpha = 1 .\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc}\n",
    "X& Y & C_0 &C_1 \\\\\\hline\n",
    "0 & 0 & 0 & 0^* \\\\ \n",
    "0 & 0 & 0 & 0^* \\\\ \\hline\n",
    "1 & 1 & 1^* & 1 \\\\ \n",
    "1 & 1 & 1^* & 1 \\\\ \n",
    "\\end{array} \n",
    "$$\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546c709-7c95-47f4-82d5-cc92c316c9c7",
   "metadata": {},
   "source": [
    "As an interpretation, say there is a medicine that has no effect on an illness; \n",
    "- rows 1 and 2 represent people who are  ill, they take a medicine, they do not get better, and they would not have gotten better without the medicine.\n",
    "- rows 3 and 4 represent people who are not ill, they do not take a medicine, they remain well, and they would  have remained well.\n",
    "\n",
    "Looking at association alone, one might think the medicine helped against the illness. It is the counterfactuals that show this is not the case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951f3cd-0c9f-40bf-aefc-4c102e1b46e8",
   "metadata": {},
   "source": [
    "**Theorem:** If we randomly assign subjects to treatment and $P(X = 0) > 0 \\wedge  P(X = 1) > 0$, then \n",
    "- $α = \\theta  $  \n",
    "- the emperical probability allow a consistent estimator of the average causal effect as the emperical association; $\\hat{\\theta} = \\hat{\\mathbb{E}}(Y|X=1)−\\hat{\\mathbb{E}}(Y|X=0)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2b69a-c1d7-41ae-92f0-3ad1bc6e3049",
   "metadata": {},
   "source": [
    "**Definition:** The <u>conditional causal effect</u> on the value $z$ of the random variable $Z$ is $\\theta_z =E(C_1|Z =z)−E(C_0|Z =z)$.\n",
    "\n",
    "For example, if $Z$ denotes gender with values $Z = 0$ (women) and $Z = 1$ (men), then $\\theta_0$ is the causal effect among women and $\\theta_1$ is the causal effect among men. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58b532-ed65-44de-a79f-d7cad0a1cb16",
   "metadata": {},
   "source": [
    "## Continuous case\n",
    "If $X$ is continuous then \n",
    "- the <u>consistency relation</u> is $Y ≡ C(X)$  \n",
    "- the <u>regression function</u>, which measures association, is $r(x) = \\mathbb{E}(Y |X = x)$, whose emperical estimator is the line of best fit to the set of $N$ points $\\cup_{i=1}^N\\{(x_i,C_i(x_i)) \\}$, which contains no counterfactuals.\n",
    "- the <u>causal regression function</u> is $\\theta  (x) = \\mathbb{E}(C(x))$, \n",
    "    - by this I mean the function whose emperical estimator is the average over the $N$ patients of the functions $C_i$ with graph $\\{(x,C_i(x))| x\\in{\\cal X}\\}$; that is $\\hat{\\theta  }(x) = \\frac1N \\sum_\\limits{_i=1}^N C_i(x)$. \n",
    "    - Patient $I$ recieves dose $x_i$, so we only observe $C_i(x_i)$ for patient $i$, and the rest of the graph of $C_i$ for each patient $i$ is counterfactual. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59760c7d-ce8f-481c-a7b7-d281cc7251d2",
   "metadata": {},
   "source": [
    "**Theorem:** In general, $\\theta  (x)\\neq r(x)$. However, when X is randomly assigned, $\\theta  (x) = r(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c738a6f-fb50-4875-b774-4c2103c72325",
   "metadata": {},
   "source": [
    "## 16.3 Observational Studies and Confounding\n",
    "**Definition:** A study in which treatment (or exposure) is  not randomly assigned is called an <u>observational study</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520e551-13c8-4046-a55e-ab13c7abe09a",
   "metadata": {},
   "source": [
    "In general, the potential outcome $C$ is not independent of treatment X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d29e18-9d84-487a-a544-fa051f2739ca",
   "metadata": {},
   "source": [
    "However, suppose we could find groupings of subjects such that, within groups, $X$ and $\\{C(x) : x ∈ {\\cal X}\\}$ are independent. This would happen if the subjects are very similar within groups. For example, suppose we find people who are very similar in age, gender, educational background, and ethnic background. Among these people we might feel it is reasonable to assume that the choice of X is essentially random. \n",
    "\n",
    "**Definition:** If the (vector of) random variable $Z$ satisfies $\\{C(x): x∈{\\cal X}\\}\\perp X|Z$  then $Z$ is called (a) <u>confounding variable(s)</u>. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c405e-c35a-4e06-82b4-106c0818f7d4",
   "metadata": {},
   "source": [
    "**Definition:** Confounding variables may exist, but we may not observe them; if we do not then we call them <u>unmeasured confounding variables</u>. \n",
    "\n",
    "**Definition:** If there are no such variables we say that there is <u>no unmeasured confounding</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b33a7c-7160-44b3-a587-e2257dbd1131",
   "metadata": {},
   "source": [
    "A note on tricky notation below; $X=x$ should be thought of as refering to data carried forward into emperical estimates. \n",
    "\n",
    "Recall that \n",
    "- $\\theta  (x) = \\mathbb{E}(C(x))=\\int \\mathbb{E}(C(x)|Z=z) dF_Z(Z=z)$ is the causal regression function. \n",
    "- $r(x) = \\mathbb{E}(Y |X = x)=\\int \\mathbb{E}(C(x)|Z=z,X=x) dF_Z(Z=z|X=x)$ is the regression function, which measures association\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897849d-550c-4583-a322-199c92105b8f",
   "metadata": {},
   "source": [
    "**Theorem:** If $Z$ is a counfounding variable (meaning $\\{C(x): x∈{\\cal X}\\}\\perp X|Z$) then \n",
    "$$\\theta(x) = \\int \\mathbb{E}[C(x)|X=x,Z=z]dF_Z(z) = \\int \\mathbb{E}(Y|X=x,Z=z)dF_Z(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f4542-d0c0-43b8-98a2-3fe7769fe294",
   "metadata": {},
   "source": [
    "That is, the condition for confounding variables is sufficient for caluculation of the causal regression function $\\theta$ from emperical data. \n",
    "\n",
    "**Definition:** The relationship $\\theta(x) =  \\int \\mathbb{E}(Y|X=x,Z=z)dF_Z(z)$ is the <u>adjusted treatment effect</u>.\n",
    "\n",
    "The process of computing adjusted treatment effects is called adjusting (or controlling) for confounding. \n",
    "\n",
    "\n",
    "Further, for any consistent estimator $\\hat{q}(x,z)$ of $\\mathbb{E}(C(x)|X=x,Z=z)$ we have the consistent emperical estimate \n",
    "$$\n",
    "\\hat{\\theta}(x) = \\frac1N \\sum_{i=1}^N \\hat{q}(x,Z_i). \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca946a-5da6-4476-b455-91757cc57e6d",
   "metadata": {},
   "source": [
    "In particular, note that if $\\hat{q}$ is linear (meaning that the causal regression function is linear so that it has a linear consistent estimator) and \n",
    "$$\\hat{q}(x,z) = a_0 +a_1 x+a_2 z$$\n",
    "then the induced consistent estimator of the causal regressin function is\n",
    "$$\\hat{\\theta}(x) = \\hat{a_0} + \\hat{a_1}x+ \\hat{a_2} \\hat{Z}_N$$\n",
    "where the coefficients $a_i$ are from ordinary least squares regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09546daa-a113-48e4-96c6-38a8a6126e2d",
   "metadata": {},
   "source": [
    "### Warning\n",
    "One single observational study is not, by itself, strong evidence. \n",
    "\n",
    "Even after adjusting for some confounders, we cannot be sure that there are no unmeasured confounding variables that we missed; observational studies must be treated with healthy skepticism. Consider them believable when\n",
    "- the results are replicated in many studies, \n",
    "- each of the studies controlled for plausible confounding variables, \n",
    "- there is a plausible scientific explanation for the existence of a causal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05881a96-124e-4135-8866-4f385b7bd9bf",
   "metadata": {},
   "source": [
    "## 16.4 Simpson's paradox\n",
    "\n",
    "Sometimes it appears that there is a treatment which is e.g. good for men, good for women, but bad overall. This may show up in the following triplet\n",
    "- $P(Y =1|X=1,Z=0)>P(Y =1|X=0,Z=0)$ (good for women)\n",
    "- $P(Y =1|X=1,Z=1)>P(Y =1|X=0,Z=1) $ ( good for men)\n",
    "- $P(Y = 1|X = 1) < P(Y = 1|X = 0)$ \n",
    "\n",
    "The problem is that the third can not be interpreted as \"bad overall\"; that statement would come from \n",
    "$$\n",
    "P(C_1 = 1) < P(C_0 = 1) \n",
    "$$. \n",
    "\n",
    "Ran outta time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820ef60-afd4-44ef-b78c-a7d88c856e42",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0313f95a-9332-425d-9fbf-71ab6657f4ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d85162b6-0a1e-4275-85f5-9428ec026276",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e20f166-87de-40bc-89cf-2837aa3f3f1f",
   "metadata": {},
   "source": [
    " # 17 Directed Graphs and Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84331cca-ce6c-4705-95f0-69b781568640",
   "metadata": {},
   "source": [
    "Aside: Baysian Networks is a poor chice of name for DAGs with probability distributions because statistical inference for such DAGs can be performed with either frequentist or Bayesian methods, so the term is misleading. \n",
    "\n",
    "---\n",
    "dfn: parent,child\n",
    "\n",
    "Notation: The set of all parents of $X$ is denoted by $\\pi_X$ or $\\pi(X)$.\n",
    "\n",
    "**Definition:** A sequence of adjacent vertices staring with $X$ and ending with $Y$ but ignoring the direction of the arrows is called an <u>undirected path</u>.\n",
    "\n",
    "**Definition:** $X$ is an <u>ancestor</u> of $Y$ if there is a directed path from X to Y $X = Y$. In such a case we also say that Y is a <u>descendant</u> of X.\n",
    "\n",
    "Dfn: collider\n",
    "\n",
    "Dfn: When the variables pointing into the collider are not adjacent, we say that the collider is an <u>unshielded collider</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab0945-14ff-4e0b-9420-8af127115550",
   "metadata": {},
   "source": [
    "**Definition:** If ${\\cal G}$ is a DAG with vertices $V=(X_1,...,X_k)$ and the distribution $f_V$ for $V$ satisfies $f_V(v) = \\prod_{i=1}^k f_{X_i}(x_i|\\pi_{X_i})$ then <u>${\\cal G}$ represents $f_V$</u>. \n",
    "    \n",
    "aka $f$ is Markov to ${\\cal G}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b8c2a-cfe0-41f3-adc6-950eb29bf6ad",
   "metadata": {},
   "source": [
    "e.g. The graph below, ${\\cal G}=(V,E)$, represents $V$ if\n",
    "$$\n",
    "f_V(v)=f_V(x,y,z,w) = f(x)f(y)f(z|x,y)f(w|z).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ba6fd-e76d-4c2e-b175-af993bf531cd",
   "metadata": {},
   "source": [
    "<img src=\"images/dag.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aeea09-5c06-40ff-be77-635d782e98e9",
   "metadata": {},
   "source": [
    "**Notation:** The set of distributions represented by ${\\cal G}$ is denoted $M({\\cal G})$.\n",
    "\n",
    "I do not have any examples of two distributions represented by the same DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da496a-1cb1-478f-b225-d209c995d145",
   "metadata": {},
   "source": [
    "**Notation:** $\\tilde{W}$ denotes all the other variables except the parents and descendants of $W$.\n",
    "\n",
    "**Definition:** If ${\\cal G}=(V,E)$ has distribution $f_V$ that satisfies $W\\perp \\tilde{W} | \\pi_W$ for all $W\\in V$ then $f_V$ satisfies the <u>markov condition</u>.\n",
    "\n",
    "Roughly, the Markov condition is that each vertex depends only on its parents and descendants; no further ancestors matter. Note that children are not the only descendants that may matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6214f51-0fd0-41d2-90f5-bb91a72627f9",
   "metadata": {},
   "source": [
    "---\n",
    "e.g. In the image above \n",
    "- $\\tilde(X) = Y$ and $\\pi_X =\\{\\}$, \n",
    "- $\\tilde{W}=(X,Y)$ and $\\pi_W=\\{Z\\}$. \n",
    "\n",
    "Thus, a distribution on that DAG satisfies the Markov condition if \n",
    "- $X \\perp Y$ \n",
    "- $W \\perp(X,Y) | Z$.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b575aa2-1a98-4783-ac48-8581f92441c9",
   "metadata": {},
   "source": [
    "**Theorem:** The graph ${\\cal G}$ represents $f_V$ iff the markov condition holds.\n",
    "\n",
    "Said again: \n",
    "- $W\\perp \\tilde{W} | \\pi_W$ for all $W\\in V$ $\\Leftrightarrow$ $f_V(v) = \\prod_{i=1}^k f_{X_i}(x_i|\\pi_{X_i})$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00073f-a779-4654-9570-e861e55e7513",
   "metadata": {},
   "source": [
    "---\n",
    "e.g. In the graph ${\\cal G}=(V,E)$ below TFAE\n",
    "- $f_{A,B,C,D}(a,b,c,d) = f_A(a)f_B(b|a)f_C(c|a)f_D(d|b,c)f_E(e|d)$\n",
    "- $B\\perp C|A$ and $D\\perp A | (B,C)$ and $E\\perp(A,B,C)|D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d95b0a-2347-46b4-87f4-77e8c2be8d9e",
   "metadata": {},
   "source": [
    "<img src=\"images/dag2.png\" width=\"300\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edbc0c-d204-40a4-8a21-be9c55c467bc",
   "metadata": {},
   "source": [
    "There can be more indepencence relations not listed among those in the Markov condition, but implies by them. How do we find them? With the tool of \"directed separation\" or \"d-separation\" for short. \n",
    "\n",
    "I do not explicitly list the rules for d-separation here due to lack of energy and motivation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234e993-54aa-4845-b7bf-2c8974fdbde6",
   "metadata": {},
   "source": [
    "It is possible for two different graphs to have the same indepencence relations. There is a theorem about that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdda132-f244-419c-8420-061feb5de081",
   "metadata": {},
   "source": [
    "Involved topics not covered in this text: \n",
    "\n",
    "Given data $V_1,...,V_n$ from a distribution $f_V$ consistent with ${\\cal G}$, \n",
    "- how do we estimate $f_V$ ?\n",
    "- how do we estimate ${\\cal G}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814811a1-d400-4d2b-afe3-1149d7123e4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89d84e01-394e-48a5-adea-1cf282525fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 18 Undirected Graphs\n",
    "\n",
    "# 19 Log-Liner Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c3f22-2714-4732-9231-785224a28af9",
   "metadata": {},
   "source": [
    "If $X=(X_1,...,X_m)\\sim f_X$ is a discrete random vector and $X_j$ takes on $r_j$ values $0,1,...,r_j - 1$, then $X$ is categorically distributed with $N=\\prod_{j=1}^m r_j$ categories and multinomial parameter $p=(p_1,...,p_N)$ determined by $f_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7f5e0-cd0e-4af5-a8c0-28d3dc61b17f",
   "metadata": {},
   "source": [
    "Let $S=\\{1,...,m\\}$, and for each $A\\subset S$ let $X_A = (X_i|i\\in A)$. \n",
    "\n",
    "e.g. $X_{\\{1,3,7\\}} = (X_1,X_3,X_7)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88280f0d-41d8-44fa-b5d4-65593680ac50",
   "metadata": {},
   "source": [
    "**Theorem:** $\\log f_X(x) = \\sum\\limits_{A\\subset S}\\psi_A(x_A)$ where \n",
    "- $\\psi_{\\{\\}}$ is constant\n",
    "- $\\psi_A$  is independent of the components of $X_{A^c}$. \n",
    "- If $i\\in A$ and $X_i=0$ then $\\psi_A = 0$\n",
    "    - i.e. if any of the components of the argument are zero then the term is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea1d50-1d1c-4b19-91a1-069e719d1d3d",
   "metadata": {},
   "source": [
    "**Definition:** The expansion guaranteed by that theorem is <u>the log-linear expansion</u> of $f_X$. \n",
    "\n",
    "For whatever reason the term \"log linear model\" keeps being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50735de3-2b35-44bc-b98b-71e812f6062a",
   "metadata": {},
   "source": [
    "---\n",
    "e.g. Say $X\\sim \\text{Cat}(3,p)$ so $f_X(x) = \\prod\\limits_{i=0}^2 p_i^{I(x-i)}$. \n",
    "\n",
    "Then \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\log f_X(x) &= \\sum\\limits_{i=0}^2 I(x-i) \\log (p_i)\\\\\n",
    "            &= \\log(p_0) + \\sum\\limits_{i=1}^2 I(x-i) \\log \\frac{p_i}{p_0}\n",
    "\\end{array}\n",
    "$$\n",
    "so that we can have\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\psi_{\\{\\}} &= \\log p_0\\\\\n",
    "\\psi_{\\{1\\}} &=  \\sum_{i=1}^2 I(x-i) \\log \\frac{p_i}{p_0}.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Indeed the latter is zero when $x=0$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f41ba-311c-42b2-b1d0-de30cab7bfeb",
   "metadata": {},
   "source": [
    "e.g. Say $X=(X_1,X_2)$ with $X_1 \\perp X_2$, \n",
    "- $X_1 \\sim \\text{cat}(r_1,p)$ and \n",
    "- $X_2\\sim \\text{cat}(r_2,q)$.\n",
    "\n",
    "Then \n",
    "$$\n",
    "f_X(x_1,x_2)\\stackrel{\\text{indep}} {=}f_{X_1}(x_1)f_{X_2}(x_2) =  p_{x_1} p_{x_2} = \\prod\\limits_{i=1}^{r_1} p_i^{I(x_1-i)} \\prod\\limits_{j=1}^{r_2}q_j^{I(x_2-j)}\n",
    "$$ \n",
    "so that\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\log f_X(x_1,x_2) \n",
    "    &= \\sum\\limits_{i=0}^{r_1-1} I(x_1-i)\\log(p_i )\n",
    "        + \\sum\\limits_{j=0}^{r_2-1}I(x_2-j)\\log(q_j)\\\\\n",
    "    & = \\log(p_0q_0)\n",
    "    + \\sum\\limits_{i=1}^{r_1-1} I(x_1-i)\\log(p_i/p_0 )\n",
    "    + \\sum\\limits_{j=1}^{r_2-1}I(x_2-j)\\log(q_j/q_0)\n",
    "    \\end{array}\n",
    "$$\n",
    "\n",
    "with \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\psi_{\\{\\}} = \\log p_0q_0\\\\\n",
    "\\psi_{\\{1\\}} =   \\sum\\limits_{i=1}^{r_1-1} I(x_1-i)\\log(p_i/p_0 )\\\\\n",
    "\\psi_{\\{2\\}} = \\sum\\limits_{j=1}^{r_2-1}I(x_2-j)\\log(q_j/q_0)\n",
    "\\end{array}\n",
    "$$\n",
    "And there is no need for $\\psi_{\\{1,2\\}}$.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad7969-9e97-418f-8afb-2887253ec7b9",
   "metadata": {},
   "source": [
    "e.g. Say $X=(X_1,X_2)$ with $X_1 \\not\\perp X_2$, \n",
    "- $X_1:\\Omega \\to \\{0,...,r_1 -1\\},~X_2:\\Omega \\to \\{0,...,r_2 -1\\}$\n",
    "- denote $f_X(x_1,x_2) = p_{x_1 x_2} = \\prod\\limits_{i=1}^{r_1} p_{ij}^{I(x_1-i)I(x_2-j)} $.\n",
    "- therefore $X \\sim \\text{Cat}(r_1r_2,p)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902269b1-0560-4d96-9b44-ea0ae7398393",
   "metadata": {},
   "source": [
    "so that \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\log f_X(x_1,x_2) \n",
    "    & = \\sum\\limits_{i,j} I(x_1-i)I(x_2-j)\\log(p_{ij} ) \\\\\n",
    "    & = \n",
    "    \\log(p_{00})\\\\\n",
    "    & ~~+ \\sum\\limits_{i=1}^{r_1-1} \n",
    "         I(x_1-i)\\log\\left(\\frac{q_{i0}}{q_{00}}\\right)\\\\\n",
    "    & ~~+ \\sum\\limits_{j=1}^{r_2-1} \n",
    "         I(x_2-j)\\log\\left(\\frac{q_{0j}}{q_{00}}\\right)\\\\\n",
    "    & ~~+ \\sum\\limits_{i=1}^{r_1-1} \\sum\\limits_{j=1}^{r_2-1} \n",
    "         I(x_1-i)I(x_2-j)\\log\\left(\\frac{ \n",
    "                 p_{ij}p_{00}\n",
    "                 }{\n",
    "                 p_{i0}p_{0j}\n",
    "                 }\\right)\n",
    "    % + \\cdots\n",
    "    \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e454ad1-dc70-4352-a6b2-821b61fa6796",
   "metadata": {},
   "source": [
    "with \n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\psi_{\\{\\}} = \\log p_{00}\\\\\n",
    "\\psi_{\\{1\\}} =    \\sum\\limits_{i=1}^{r_1-1} \n",
    "         I(x_1-i)\\log\\left(\\frac{q_{i0}}{q_{00}}\\right)\\\\\n",
    "\\psi_{\\{2\\}} = \n",
    "    \\sum\\limits_{j=1}^{r_2-1} \n",
    "         I(x_2-j)\\log\\left(\\frac{q_{0j}}{q_{00}}\\right)\\\\\n",
    "\\psi_{\\{1,2\\}} = \n",
    "    \\sum\\limits_{i=1}^{r_1-1} \\sum\\limits_{j=1}^{r_2-1} \n",
    "         I(x_1-i)I(x_2-j)\\log\\left(\\frac{ \n",
    "                 p_{ij}p_{00}\n",
    "                 }{\n",
    "                 p_{i0}p_{0j}\n",
    "                 }\\right)\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c8a0f-9033-4c7a-b6f4-2d802755b6f6",
   "metadata": {},
   "source": [
    "And you can imagine how having a 3rd variable will require more terms.. I'm struggling to articulate... that correct the lower order terms. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91841be7-2804-4b17-899a-1f6ba960d96a",
   "metadata": {},
   "source": [
    "The parameters change from ${\\cal P} = \\{ p | \\sum p_{i}^n =1\\}$ to  $\\Theta = \\{ \\beta =(\\beta_1,...,\\beta_N)| \\beta = \\beta(p), p\\in {\\cal P}\\}$ where the functions are like $\\beta_{1} = \\log\\frac{p_1}{p_0}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfaeaf-8582-44b1-9314-e98db1f58920",
   "metadata": {},
   "source": [
    "Ok, but why did we develop this technology? \n",
    "\n",
    "1. The log linear expansion of the multiple categorical distribution allows a characterization of independence \n",
    "\n",
    "**Theorem:** Let $\\{a,b,c\\}$ be a partition of $\\{1,...,m\\}$. Then $X_b \\perp \\!\\!\\! \\perp X_c | X_a$ iff $\\psi_A =0$ for all $A$ such that $A \\cap B \\neq \\{\\}$ and $A \\cap C \\neq \\{\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca719f-3850-493b-bdac-e1795350d916",
   "metadata": {},
   "source": [
    "There is a proof, but I'm not convinced I love this theorem.\n",
    "\n",
    "## Graphical log-linear models\n",
    "\n",
    "The log-linear expansion of the multiple categorical distribution allows for a charicterization of graphical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb339a-5b33-47a7-b5be-5170ccf577b2",
   "metadata": {},
   "source": [
    "**Definition::** If there exists some ${\\cal G}=(V,E)$ such that $$\n",
    "\\psi_A = 0 \\iff \\{i,j\\} \\in A \\bigwedge \\{i,j\\} \\notin E$$ \n",
    "then $f$ is a <u>graphical model</u>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d215d3-ce53-40d1-8f24-8151c337b3c7",
   "metadata": {},
   "source": [
    "Objection: Why call this a model? Models are sets of distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce2536-1532-4cb0-81e2-547bd518a3e5",
   "metadata": {},
   "source": [
    "Alterative thinking: if you can add a non-zero term to the log-linear model and the graph does not change, then the model is not graphical.\n",
    "\n",
    "----\n",
    "e.g. Say \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log f(x) = \n",
    "&\\psi_{\\{\\}} \\\\\n",
    "&+ \\psi_{\\{1\\}} + +\\psi_{\\{2\\}} + \\psi_{\\{3\\}} +  \\psi_{\\{4\\}} \\\\\n",
    "&+\\psi_{\\{1,2\\}} + \\psi_{\\{2,3\\}} +\\psi_{\\{2,4\\}} + \\psi_{\\{3,4\\}} \\, .\n",
    "\\end{align*} \n",
    "$$\n",
    "The graph for this model is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59adbea-3c7d-4f75-bc44-84407b4039d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T15:09:30.952339Z",
     "start_time": "2023-06-25T15:09:30.746564Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"332pt\" height=\"67pt\"\n",
       " viewBox=\"0.00 0.00 332.00 67.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 63)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-63 328,-63 328,4 -4,4\"/>\n",
       "<!-- 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"117\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 1&#45;&#45;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.4,-18C65.54,-18 78.48,-18 89.62,-18\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"207\" cy=\"-41\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-35.95\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 2&#45;&#45;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&#45;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.51,-24.41C154.62,-27.57 169.19,-31.38 181.31,-34.55\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"297\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"297\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">4</text>\n",
       "</g>\n",
       "<!-- 2&#45;&#45;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&#45;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.23,-15.93C155.26,-15.16 168.25,-14.38 180,-14 203.99,-13.23 210.01,-13.23 234,-14 245.75,-14.38 258.74,-15.16 269.77,-15.93\"/>\n",
       "</g>\n",
       "<!-- 3&#45;&#45;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&#45;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M232.51,-34.59C244.62,-31.43 259.19,-27.62 271.31,-24.45\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Graph at 0x7fc0c0942100>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz as gr\n",
    "\n",
    "g = gr.Graph(graph_attr={\"rankdir\": \"LR\"})\n",
    "g.edge(\"1\", \"2\")\n",
    "g.edge(\"2\", \"3\")\n",
    "g.edge(\"2\", \"4\")\n",
    "g.edge(\"3\", \"4\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4b453-7d21-4f25-9e5c-fcac7f90ea5c",
   "metadata": {},
   "source": [
    "We can create a new model $f_1$ without changing the graph by adding a non-zero term\n",
    "$$\n",
    "\\log f_1 = log (f) + \\psi_{\\{2,3,4\\}}.\n",
    "$$\n",
    "Therefore $f$ is not graphical. \n",
    "\n",
    "On the other hand, adding any term to $f_1$ will change the graph; \n",
    "- the remaining possibilities for cardinality 2 set $\\{1,3\\}, \\{1,4\\}$ require new edges\n",
    "- the remaining cardinality 3 sets have subsets $\\{i,j\\}\\notin E$\n",
    "- the cardinality 4 set is not alowed because it is a superset of $\\{1,3\\} \\notin E$. \n",
    "\n",
    "Note that the presence of $\\psi_{\\{2,3,4\\}}$ means that the strength of association between 2 and 3 depends on 4. That is what makes the graph a representation of interaction with all the terms of $f_1$. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded222ca-05a1-4638-bdbf-d309f15540b2",
   "metadata": {},
   "source": [
    "## Hierarchical Log-Linear Models\n",
    "\n",
    "**Definition:** A log linear model is <u>hierarchical</u> if for all $A\\subset S$ \n",
    "$$\\psi_A = 0 \\implies  (\\forall B \\supset A) \\,\\psi_B = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be88fb-6486-43a3-a4a5-b07ce5711e1d",
   "metadata": {},
   "source": [
    "All graphical models are hierarchical, but not all hierarchical models are graphical. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70630e67-50ae-4dda-bec8-843f522c4444",
   "metadata": {},
   "source": [
    "--- \n",
    "e.g. Say \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log f = &\\psi_{\\{\\}} \\\\\n",
    "    &+ \\psi_{\\{1\\}}+ \\psi_{\\{2\\}} +\\psi_{\\{3\\}} \\\\\n",
    "    &+ \\psi_{\\{1,2\\}} + \\psi_{\\{2,3\\}}+ \\psi_{\\{3,1\\}} \\\\\n",
    "    \\end{align*}\n",
    "$$\n",
    "The only set $A\\subset \\{1,2,3\\}$ with $\\psi_A =0$ is $S=\\{1,2,3\\}$. Since $S$ has no supersets,  the model is hierarchical. However, it is not graphical since a nonzero $\\psi_S$ could be addedwithout changing the graph. The graph is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df25177-456b-4847-a7c9-4e8bb0306c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-25T15:09:30.952339Z",
     "start_time": "2023-06-25T15:09:30.746564Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"242pt\" height=\"67pt\"\n",
       " viewBox=\"0.00 0.00 242.00 67.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 63)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-63 238,-63 238,4 -4,4\"/>\n",
       "<!-- 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"117\" cy=\"-41\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-35.95\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 1&#45;&#45;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&#45;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.51,-24.41C64.62,-27.57 79.19,-31.38 91.31,-34.55\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"207\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 2&#45;&#45;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&#45;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.51,-34.59C154.62,-31.43 169.19,-27.62 181.31,-24.45\"/>\n",
       "</g>\n",
       "<!-- 3&#45;&#45;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&#45;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M179.77,-15.93C168.74,-15.16 155.75,-14.38 144,-14 120.01,-13.23 113.99,-13.23 90,-14 78.25,-14.38 65.26,-15.16 54.23,-15.93\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Graph at 0x7fc0c0942d90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gr.Graph(graph_attr={\"rankdir\": \"LR\"})\n",
    "g.edge(\"1\", \"2\")\n",
    "g.edge(\"2\", \"3\")\n",
    "g.edge(\"3\", \"1\")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5f98d-9b3f-4638-837c-bee5419d37b8",
   "metadata": {},
   "source": [
    "## Model Generators\n",
    "\n",
    "This appears to just be a notation thing best learned by examples.\n",
    "\n",
    "---\n",
    "e.g. for $X=(X_1,X_2,X_3)$.\n",
    "\n",
    "- 1.2.3 means $\\log f =  \\psi_{\\{\\}} + \\psi_{\\{1\\}}+ \\psi_{\\{2\\}}+ \\psi_{\\{3\\}} + \\psi_{\\{1,2\\}} + \\psi_{\\{2,3\\}}+ \\psi_{\\{3,1\\}} + \\psi_{\\{1,2,3\\}}$\n",
    "    - This is called a saturated model; such models correspond to fitting an unconstrained model. \n",
    "    - terms of order lower than 3 need to be included to make the model hierarchial.\n",
    "- 1.2 + 1.3 means $\\log f =  \\psi_{\\{\\}} + \\psi_{\\{1\\}}+ \\psi_{\\{2\\}}+ \\psi_{\\{3\\}} + \\psi_{\\{1,2\\}} +  \\psi_{\\{1,3\\}}$.\n",
    "    - $\\{1,2,3\\}$ can not be included because it is a superset of the missing $\\{2,3\\}$\n",
    "- 1+2+3 means $\\log f =  \\psi_{\\{\\}} + \\psi_{\\{1\\}}+ \\psi_{\\{2\\}}+ \\psi_{\\{3\\}} $\n",
    "    - this is the mutually independent model. \n",
    "- 1.2 in the context of 3 variables means $\\log f =  \\psi_{\\{\\}} + \\psi_{\\{1\\}}+ \\psi_{\\{2\\}} + \\psi_{\\{1,2\\}}$.\n",
    "    - $X_3$ is forced to have a uniform distribution here, in the sense that $X_3|(X_1,X_2)$ is uniformly distributed.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c834357-e51d-4f66-84e8-2afb44894903",
   "metadata": {},
   "source": [
    "## Fitting \n",
    "\n",
    "We are examining a context where $X=(X_1,...,X_m)$. A random sample of size $n$ then has the form \n",
    "$\\{(X_{i1},...,X_{im})\\,| i=1,...,n \\}$. I will name \n",
    "- the $i$th element of the sample ${\\cal X}_i$,  \n",
    "- ${\\cal X} = ({\\cal X}_1,...,{\\cal X}_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ad27d-fc6d-4b14-ae55-8c9e1b3a29fc",
   "metadata": {},
   "source": [
    "The log likelihood of a random sample when we use the parameters $\\beta$ is \n",
    "$$\n",
    "\\sum_{i=1}^n \\log f ({\\cal X}_i ; \\beta) \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303259f-a707-4757-8321-3053d55ea290",
   "metadata": {},
   "source": [
    "Choice of model is choice of log linear model. How do we use this to determine the best model to use? \n",
    "\n",
    "For each model $M$ let $\\hat{l}_M$ be the log-likelyhood evaluated at the MLE. Let $M_{\\text{sat}}$ be the saturated model. \n",
    "\n",
    "$$H_0: M,\\\\\n",
    "H_1: M_{\\text{sat}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709a2cf-61b7-4b4b-ac2e-f28dcc5b525c",
   "metadata": {},
   "source": [
    "**Definition:** The <u>deviance</u> of model $M$ is  $\\text{dev}(M) = 2|\\hat{l}_M - \\hat{l}_{M_{\\text{sat}}}|\\, .$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200e627-1a19-40e7-ab66-7915624c0349",
   "metadata": {},
   "source": [
    "The deviance is the likelihood ratio test statistic for testing $H_0$ vs $H_1$, as you saw in ch15 using optimization with constraint for multinomial distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e1fa4-dd46-44ab-9154-f30b5d279f7b",
   "metadata": {},
   "source": [
    "**Theorem:** $\\text{dev}(M) \\stackrel{d}{\\to} \\chi^2_{\\nu}$ where $\\nu$ is the difference in number of parameters in the saturated model and the model $M$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587370b-a2e1-46cd-b58a-a74ac11732db",
   "metadata": {},
   "source": [
    "So, a chi squared test can be used on $H_0$ for each $M$. \n",
    "\n",
    "You can \n",
    "- test every $M$ and use the one with the lowest deviance if that one passes the test\n",
    "- test only hierarchical log-lihnear models, and keep the one with thelowest deviance if that passes your test.\n",
    "- Use AIC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fe20b-bdff-4eaa-a8c9-0751b410edb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb36d666-e0d1-41db-9d39-bcf9a892b20e",
   "metadata": {},
   "source": [
    "# 20 Nonparametric Curve Estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
