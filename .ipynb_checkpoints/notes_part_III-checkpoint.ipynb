{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff9b80c-bd0e-4840-b66e-8867af45aea7",
   "metadata": {},
   "source": [
    "# Part 3: Statistical Models and Methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e106bbb-f5c8-4018-b169-ccb44e17d048",
   "metadata": {},
   "source": [
    "# ch 13 Linear and Logistic Regression \n",
    "\n",
    "**Theorem**. Under the assumption of Normaly distributed residuals, the least squares estimator is also the maximum likelihood estimator.\n",
    "\n",
    "Maximizing the log likelihood $l(\\beta_0, \\beta_1, \\sigma)$ over $\\sigma$ yields the mle\n",
    "$\\hat \\sigma^2 = \\frac1n \\sum\\limits_{i=1}^n \\epsilon_i^2$ where $\\epsilon_i$ is the $i$th residual. \n",
    "\n",
    "An unbiased estimate of $\\sigma^2$ is\n",
    "$\\hat \\sigma^2 = \\frac{1}{n-2} \\sum\\limits_{i=1}^n \\hat{\\epsilon}_i^2$ where $\\hat{\\epsilon}_i$ is the $i$th residual.My understanding of this does not extend beyon having two relations between the data, $\\beta_0,\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b3aff-d289-43d5-b7a3-5a9c29b0a0cc",
   "metadata": {},
   "source": [
    "## 13.3 Properties of Lease Squared Estimators\n",
    "**Theorem** Let $\\hat{\\beta}^T = (\\hat{\\beta}_0 , \\hat{\\beta}_1 )^T$ denote the least squares estimators for the parameteris in the fit $Y= \\beta_0 + \\beta_1 X$. Then,\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "(i) & \\mathbb{E} \\left( \\hat{\\beta} | X^n \\right) =\n",
    "\\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\end{array}\\right),\\\\\n",
    "(ii) & \\mathbb{V} ( \\hat{\\beta} | X^n ) \n",
    "= \n",
    "\\frac{\\sigma^2/n}{s^2_X} \n",
    "\\left(\\begin{array}{cc} \n",
    "\\frac1n \\sum_i X_i^2 - \\bar X_n^2 &  -\\bar X_n \\\\ \n",
    "-\\bar X_n & 1\n",
    "\\end{array}\\right)\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "where $s^2_X = n^{−1} \\sum\\limits_{i=1}^n (X_i−\\bar X_n)^ 2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c1bae-f8bf-4896-9f69-91f42a3b38a0",
   "metadata": {},
   "source": [
    "**Theorem** Under appropriate conditions we have:\n",
    "1. (Consistency): $\\hat{\\beta}_0 \\stackrel{P}{\\to} \\beta_0$ and $\\hat{\\beta}_1\\stackrel{P}{\\to} \\beta_1$.\n",
    "2. (Asymptotic Normality): $\\frac{\n",
    "        \\hat \\beta_0−\\beta_0\n",
    "        }\n",
    "        {\n",
    "        \\hat{\n",
    "            \\text{se} \n",
    "            }\n",
    "         ( \\hat \\beta_0) \n",
    "         }  \n",
    "\\stackrel{Dist}{\\to} N(0,1)\n",
    "$\n",
    "and \n",
    "$\\frac{\\hat{\\beta}_1−\\beta_1}{\\hat{\\text{se}}( \\hat \\beta_1)} \\stackrel{Dist}{\\to} N(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c40ff-e9b4-4389-8a20-4569dca9acd8",
   "metadata": {},
   "source": [
    "## 13.4 Prediction\n",
    "Having estimated $\\hat r(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ from data $(X_1, Y_1), . . . , (X_n, Y_n)$, \n",
    "we observe the value $X = x_∗$ of the covariate for a new subject and we want to predict their outcome $Y_∗$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82999d8-967c-4a0f-b9a5-daa667fff2ca",
   "metadata": {},
   "source": [
    "An estimate of $Y_∗$ is\n",
    "$$\\hat{Y}_∗ = \\hat{ \\beta  }_0 + \\hat{ \\beta  }_1 x_∗.$$ \n",
    "How much variance do you expect in this prediction among different samples $X_1,...,X_n$? Using the formula for the variance of the sum of two random variables,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27043026-95f8-4f3d-a959-6f044dfcb3c5",
   "metadata": {},
   "source": [
    "$$\\mathbb{V}(\\hat Y_∗) = \\mathbb{V}( \\hat  \\beta_0 + \\hat  \\beta_1 x_∗) \\\\\n",
    "= \\mathbb{V}(\\hat  \\beta_0) + x^2_∗ \\mathbb{V}(\\hat  \\beta_1) + 2x_∗\\text{Cov}(\\hat  \\beta_0, \\hat \\beta_1).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc85652-a526-4c03-8493-f6800720fa84",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\frac{\\sigma^2/n}{s_X^2}\\left(\n",
    "\\frac1n \\sum\\limits_i X_i^2 -\\bar X_n^2   \n",
    "+ x_*^2 \n",
    "+ 2 x_* \\bar X_n \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6d017-c985-43c4-8ec4-976c63648acd",
   "metadata": {},
   "source": [
    "I want to campare this to my ISL notes, but do not seem to have them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1e68d-6116-4980-8fe7-a0e952cece3f",
   "metadata": {},
   "source": [
    "13.5 subsection on multiple regression\n",
    "\n",
    "pretty easy.\n",
    "\n",
    "# 13.6 Model Selection\n",
    "\n",
    "A smaller model with fewer covariates has two advantages\n",
    "1. it might give better predictions than a big model and \n",
    "2. it is more parsimonious (simpler). \n",
    "\n",
    "Generally, as you add more variables to a regression, the bias of the predictions decreases and the variance increases.\n",
    "\n",
    "In model selection there are two problems: \n",
    "1. assigning a “score” to each model which measures, in some sense, how good the model is, and \n",
    "2. searching through all the models to find the model with the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc6b34-5676-47fb-8522-89a9ee4d7981",
   "metadata": {},
   "source": [
    "Let \n",
    "- $k$ be the number of covatiates available.  \n",
    "- $S \\subset \\{1, . . . , k\\}$ \n",
    "- ${\\cal X}_S = \\{X_j : j \\in S\\}$ denote a subset of the covariates. \n",
    "- $X_s$ the $|S|\\times n$ data matrix \n",
    "- $Y$ the response data\n",
    "- ${\\beta}_S$ be the coefficients for the model with covariates ${\\cal X}_S$\n",
    "- $\\hat{\\beta}_S$ be their OLS estimators\n",
    "    - a function of $(X_s,Y)$, the training data. \n",
    "- $\\hat{r}_S$ the OLS estimator of the linear function with covariates ${\\cal X}_S$.\n",
    "    - a function of $(X_s,Y)$, the training data.\n",
    "- $\\hat{Y}_i(S):= \\hat{r}_S(X_i)$ for all $i \\in \\{1,...,n\\}$. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e4ac4-fae0-40d3-9495-7b0e9ff9b55c",
   "metadata": {},
   "source": [
    "Our goal is to choose $S$ to minimize the prediction risk\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    " R(S) & = \\sum\\limits_{i=1}^n \\mathbb{E}_{(X_S,Y,X^*_S,Y^*)} \\left(\\hat{Y}_i(S) - Y^*_i\\right)^2\\\\\n",
    "      & = \\sum\\limits_{i=1}^n \\mathbb{E}_{(X_S,Y,X_s^*,Y^*)} \\left(\\hat{r}_S(X_i^*) - Y^*_i\\right)^2\n",
    "\\end{array}\n",
    "$$\n",
    "where $(X^*_i,Y^*_i)$ are future observations of $Y_i$ at covariate value $X_i$. (what?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a9714-1626-4589-9693-77da76e1ca58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Two possibilities:\n",
    "1. There are two sets of random variables, \n",
    "    - training random variables to determine $\\hat{r}_S$\n",
    "    - the random variables $(X_i^*,Y_i^*)$ over which the expectations are computed.\n",
    "        - pros: There is a reason for the seperate notation $Y^*$.\n",
    "        - cons: \n",
    "            - $R(S)$ still has RVs in it after the expectation; it is a statistic. \n",
    "                - unless expectation is over both sets of RVs\n",
    "            - why the same $n$?\n",
    "        \n",
    "2. There is one set of random variables\n",
    "    - $\\hat{r}_S$ is determined by $(X_s,Y)$\n",
    "    - The expectation is over the same random variables\n",
    "        - pros: simpler\n",
    "        - cons: why the differen notation with a star? \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99a11f-0682-407c-b89d-3eab2ec51f0c",
   "metadata": {},
   "source": [
    "**Definition:** The <u>training error</u> is \n",
    "$$\n",
    "\\hat R_{\\text{tr}}(S) = \\sum\\limits_{i=1}^n\\left(\\hat{Y}_i(S) - Y_i\\right)^2 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610dfb5f-d01e-4561-b0ad-780893451859",
   "metadata": {},
   "source": [
    "**Theorem** The training error is a downward-biased estimate of the prediction risk; \n",
    "$$\n",
    "\\mathbb{E}\\left(\\hat{R}_{tr} (S) \\right)<  R(S)\n",
    "$$ \n",
    "because\n",
    "$$\n",
    "\\text{bias}( R_{tr}(S)) = \\mathbb{E}\\big( R_{tr}(S) \\big) - R(S)  = −2 \\sum_{i=1} \\text{Cov}(\\hat Y_i, Y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85cf00-c5c5-4d8b-88d9-763db987beb4",
   "metadata": {},
   "source": [
    "Conceptually, this bias happens because the data is used twice\n",
    "- to estimate the coefficientes $\\beta$ as $\\hat{\\beta}$\n",
    "- to estimate the risk $R(S)$ as $\\hat{R}_{\\text{tr}}$.\n",
    "\n",
    "When there are many covariates in the model (when $|S|$ is large) this covariance term is large and thus the bias is large.\n",
    "\n",
    "Here are some estimators of risk that avoid this pathology. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ca5c0-4016-43d3-bd40-8f6333c36942",
   "metadata": {},
   "source": [
    "## Mallows’s \n",
    "**Defnition** The estimate of the risk \n",
    "$\\hat R(S) = \\hat R_\\text{tr}(S) + 2\\vert S \\vert \\hat\\sigma^2$ \n",
    "where $\\hat\\sigma^2$ is the estimate of $\\sigma^2$ obtained from the full model (with all covariates in the model) is **Mallows’s $C_p$ statistic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035b676-5f28-49e6-aad9-8f66c972e6d8",
   "metadata": {},
   "source": [
    "Think of it as lack of fit + complexity penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef91d6-913c-4514-83d8-f4ec28fa11f6",
   "metadata": {},
   "source": [
    "## AIC\n",
    "**Definition:** The <u>Akaike Information Criterion</u> is the choice of $S$ as \n",
    "$$\n",
    "\\underset{S}{\\text{argmax}} \\left( {\\cal l}_S - |S| \\right) \n",
    "$$\n",
    "where ${\\cal l}_S$ is the (me: expected) log-likelihood of the model $ {\\cal F} = \\{ f(\\cdot;\\theta)| \\theta \\in \\Theta\\}$ evaluated at the MLE $\\hat{\\theta}_{\\text{mle}}$. That is, \n",
    "$$\n",
    "{\\cal l}_S = \\sum\\limits_{i=1}^n f(x) \\log f(X_i;\\hat{\\theta}_{\\text{MLE}}).\n",
    "$$\n",
    "\n",
    "What is meant by $f(x)$ there? Does this quantity depend on $\\theta$? \n",
    "\n",
    "There seems to be too little infor to figure this out. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbc2e4-606c-48a8-b063-e7a412e314c2",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "**Definition:** In <u> leave-one-out cross-validation</u> the risk estimator is \n",
    "$$\n",
    "R_{\\text{CV}}\\sum_{i=1}^n\\left( Y_i - \\hat{Y}_{(i)}\\right)^2\n",
    "$$\n",
    "where $\\hat{Y}_{(i)}$ is the model obtained from the set of $n-1$ random variables \n",
    "$\\big\\{  (X_j,Y_j) | j\\in \\{1,...,n\\} - \\{i\\} \\big\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849172e-6bb7-4647-b53b-be075c7d5fae",
   "metadata": {},
   "source": [
    "**Definition:** In <u>k-fold cross-validation</u> the training set is partitioned into $k$ parts $S_1,...,S_k$ and construct the predictors $\\hat{Y}_j$ by training on $S_{(j)} := \\cup_{j'\\neq j} S_{j'}$, and use risk estimator \n",
    "$$\n",
    "R_{k\\text{CV}} \n",
    "= \n",
    "\\frac1k \\sum_{j=1}^k \n",
    "\\sum_{(X_i,Y_i)\\in S_{(k)} } (Y_i - \\hat{Y}_{(j)} (X_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83ab73-3d6a-45ee-96ce-7e4914d6f1d5",
   "metadata": {},
   "source": [
    "## Bayesian information criterion\n",
    "\n",
    "**Definition:** In <u>Bayesian information criterion</u> the choice of $S$ is \n",
    "$$\n",
    "\\underset{S}{\\text{argmax} } \\,\\text{BIC} (S) := \n",
    "\\underset{S}{\\text{argmax} } \\left(l_s - \\frac{|S|}{2}\\log n\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b137f-160d-4fc9-84bf-f16931b7cb06",
   "metadata": {},
   "source": [
    "Choosing the model with highest BIC is like choosing the model with highest posterior probability because if $[S_1,...,s_m]$ are some models and one puts\n",
    "- a prior $\\mathbb{P}(S_i)=\\frac 1m$ on the categorical distribution across the $m$ models\n",
    "- a smooth prior on each model\n",
    "$$\n",
    "\\mathbb{P}(S_j|{\\text{data}}) \\approx \\frac{ e^{\\text{BIC}(S_j)}}{\\sum_{j'} e^{\\text{BIC}(S_{j'})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccbd2f-0abb-4913-97c9-a7337fd5bbb8",
   "metadata": {},
   "source": [
    "The BIC score also has an information-theoretic interpretation in terms of something called minimum description length. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a123b-cb49-4199-a760-cbc0a84275d8",
   "metadata": {},
   "source": [
    "## 13.7 Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff948e-031c-40aa-9273-74bbeb5496e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59b56dcc-e5b6-4cb6-83f0-478dd313ef19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd792171-6e1e-45c3-bf83-54c639d8ea29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eff2733b-6e91-4fc1-835f-fd4b20c36056",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c49da3-17f6-454b-a145-ed6fdf4a68b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd107ef-f65a-4480-9d52-867f4cc8c34b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 14 Multivariate Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995f77e-2ed4-4f0b-8c06-3f587a2bbaf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "A confidece interval for the correlation between $X_1$ and $X_2$ can be obtained by the following method by Fisher:\n",
    "\n",
    "1. Estimate the correlation \n",
    "$\\rho = \\frac{\\mathbb{E}\\left((X_1 - \\mathbb{E}(X_1) \\right)\\left(X_2 - \\mathbb{E}(X_2) \\right)}{\\sigma_1,\\sigma_2}$ with the non-parametric plug in estimator \n",
    "$\\hat{\\rho} := \\frac{\\sum_{i=1}^n\\left(X_{1i} - \\bar{X}_1 \\right)\\left( X_{2i} - \\bar{X}_2 \\right)}{s_1 s_2}$\n",
    "    - $s_i : = \\frac{1}{n-1}\\sum_{i=1}^n\\left(X_{1i} - \\bar{X}_1 \\right)^2$ and the $n-1$ is what makes this the \"non-parametric\" plug in estimator; the parametric one has $\\frac1n$.\n",
    "2. Compute $\\hat{\\theta} = \\frac12 \\left(  \\log (1+\\hat \\rho) - \\log(1-\\hat{\\rho})  \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc163e58-3abe-425c-882f-ce7d7dac5705",
   "metadata": {
    "tags": []
   },
   "source": [
    "3. Compute $\\hat{\\text{se}}(\\hat{\\theta})$. It is $\\frac{1}{\\sqrt{n-3}}$. \n",
    "\n",
    "4. A $1-\\alpha$ confidence interval for $\\hat{\\theta}$ is \n",
    "$(a,b) = \n",
    "\\left( \n",
    "\\hat{\\theta} - \\frac{Z_{\\frac{\\alpha}{2}}}{\\sqrt{n-3}}\\, , \\, \n",
    "\\hat{\\theta} + \\frac{Z_{\\frac{\\alpha}{2}}}{\\sqrt{n-3}}\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af81caf-a2de-4d5d-93bb-bc6306da166e",
   "metadata": {
    "tags": []
   },
   "source": [
    "5. Invert the transform in step 2 with $f^{-1}(z) = \\frac{e^{2z}-1}{e^{2z}+1}$ to obtain the $1-\\alpha$ confiodence interval for $\\rho$\n",
    "$$\n",
    "\\left( \n",
    " \\frac{e^{2a}-1}{e^{2a}+1}\n",
    "\\, , \\, \n",
    " \\frac{e^{2b}-1}{e^{2b}+1}\n",
    "\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd29096-b365-409c-935b-bb1c4eea7ae7",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e053fb01-88a4-4baf-83f0-a83eeb0bd894",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9750e01a-2eee-448b-bd5c-6fe4099667df",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2155998-36cb-4447-909d-1026b3f9c085",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80731b23-d10e-45d0-8989-ac7287054a32",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 15 Inference About Independence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d338f7ff-e2c5-4bed-bd0f-ebbb26671e1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "When Y and Z are not independent, we say that they are dependent or associated or related. \n",
    "\n",
    "1. How do we test if two random variables are independent?\n",
    "2. How do we estimate the strength of dependence between two random variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433b54a-cfaa-4ba3-a4e8-77eb12e758b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Odds\n",
    "**Definition:** The <u>odds</u> of the event $A$ is $\\text{odds}(A):= \\frac{\\mathbb{P}(A)}{1-\\mathbb{P}(A)}$. \n",
    "\n",
    "Starting with a pair of binary random variables that we wish to measure the probability of dependence, we will go conceptual;\n",
    "- let $E$ be exposure to something (smoking, exercise)\n",
    "- let $D$ be an outcome (disease, ability to do a flip)\n",
    "\n",
    "$$\n",
    "\\text{odds}(D|E) = \\frac{\\mathbb{P}(D|E)}{1-\\mathbb{P}(D|E)}\\\\\n",
    "\\text{odds}(D|E^c) = \\frac{\\mathbb{P}(D|E^c)}{1-\\mathbb{P}(D|E^c)}\n",
    "$$\n",
    "\n",
    "**Definition:** The <u>odds ratio</u> is $\\psi = \\frac{\\text{odds}(D|E)}{\\text{odds} (D|E^c)}$\n",
    "\n",
    "**Definition:** The <u>log odds ratio</u> is $\\gamma = \\log \\psi = \\log \\frac{\\text{odds}(D|E)}{\\text{odds} (D|E^c)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e2f3d-516c-4c5c-b510-760c1efe3866",
   "metadata": {
    "tags": []
   },
   "source": [
    "Consider the probabilites $p_{ij}$ and counts after $n$ draws for the 4 outcomes as in the tables below.\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "&D^c & D \\\\\\hline\n",
    "E^c  & p_{00}&p_{01} \\\\\n",
    "E    & p_{10}& p_{11}\\\\\n",
    "\\end{array}\n",
    "\\,, \\,\\,\n",
    "\\begin{array}{c|cc}\n",
    "&D^c & D \\\\\\hline\n",
    "E^c  & X_{00}&X_{01} \\\\\n",
    "E    & X_{10}& X_{11}\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac25a7-f768-493e-b118-c8429c339f72",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\psi \n",
    "&= \n",
    "\\frac{ \\text{odds}(D|E)}  {\\text{odds}(D|E^c)}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    \\frac{\\mathbb{P}(D|E)}{1-\\mathbb{P}(D|E)}\n",
    "    }{\n",
    "    \\frac{\\mathbb{P}(D|E^c)}{1-\\mathbb{P}(D|E^c)}\n",
    "    }\n",
    "\\\\&=\n",
    "\\frac{\n",
    "    \\frac{\\mathbb{P}(D,E) /\\mathbb{P}(E)}{1-\\mathbb{P}(D,E)/\\mathbb{P}(E)}\n",
    "    }{\n",
    "    \\frac{\\mathbb{P}(D,E^c) /\\mathbb{P}(E^c)}{1-\\mathbb{P}(D,E)/\\mathbb{P}(E^c)}\n",
    "    }\n",
    "\\\\\n",
    "&=\n",
    "\\frac{\n",
    "    \\frac{\\mathbb{P}(D,E) }{/\\mathbb{P}(E)-\\mathbb{P}(D,E)}\n",
    "    }{\n",
    "    \\frac{\\mathbb{P}(D,E^c) }{\\mathbb{P}(E^c)-\\mathbb{P}(D,E)}\n",
    "    }\n",
    "\\\\\n",
    "&=\n",
    "\\frac{ \n",
    "    \\frac{p_{11}}{p_{01}+p_{11}-p_{11}}\n",
    "    }\n",
    "    {\n",
    "    \\frac{p_{10}}{p_{00}+p_{01}-p_{01}}\n",
    "    }\n",
    "\\\\\n",
    "&=\\frac{ \n",
    "    \\frac{p_{11}}{p_{01}}\n",
    "    }\n",
    "    {\n",
    "    \\frac{p_{10}}{p_{00}}\n",
    "    }\n",
    "\\\\\n",
    "&=\\frac{p_{00} p_{11}}{p_{01}p_{10}}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a565f5-9337-4144-a6a8-97023ab369ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "To construct estimators we need to know how we sample. There are three methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b21a4-ac6e-4bf0-8379-d7c00b37611e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Thee methods of sampling\n",
    "\n",
    "**Definition** In <u>multinomial sampling</u> we sample people from a population and record their exposure and disease status so $X=(X_{00},X_{01},X_{10},X_{11}) \\sim \\text{Multinomial}(n,p).$\n",
    "\n",
    "And since the MLEs $\\hat{p}_{ij} = \\frac{X_{ij}}{n}$ we have the estimator\n",
    "$$\n",
    "\\hat{\\psi} = \\frac{X_{00}X_{11}}{X_{01}x_{10}}\n",
    "$$\n",
    "\n",
    "**Definition:** In <u>cohort sampling</u> we get $x_{1\\cdot}$ exposed and $x_{0\\cdot}$ unexposed people and count the number with disease in each group, so\n",
    "$$\n",
    "X_{01} \\sim \\text{Binomial}(x_{0\\cdot}, \\mathbb{P}(D|E^c)) \\\\\n",
    "X_{11} \\sim \\text{Binomial}(x_{1\\cdot}, \\mathbb{P}(D|E  )).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7234a-f906-4afa-b57b-71575343ef74",
   "metadata": {
    "tags": []
   },
   "source": [
    "While we can not estimate every quantity in the table, we can estimate the two we need to calculate the odds ratio:\n",
    "$$\n",
    "\\hat{\\mathbb{P}}(D|E^c) = \\frac{ X_{01} }{x_{0\\cdot}},~\n",
    "\\widehat{\\text{odds}}(D|E^c) = \\frac{ X_{01} }{x_{0\\cdot} - X_{01}}\n",
    "\\\\\n",
    "\\hat{\\mathbb{P}}(D|E) \n",
    "= \\frac{X_{11}}{x_{1\\cdot}},~\n",
    "\\widehat{\\text{odds}}(D|E) = \\frac{ X_{11} }{x_{1\\cdot} - X_{11}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e29c1-8f72-4d6e-99d6-475407e0e70c",
   "metadata": {
    "tags": []
   },
   "source": [
    "From that we can calculate \n",
    "$$\n",
    "\\hat{\\psi} \n",
    "=\\frac{\\widehat{\\text{odds}}(D|E)}{\\widehat{\\text{odds}}(D|E^c)}\n",
    "= \\frac{X_{00}X_{11}}{X_{01}X_{10}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d5edc-52aa-4a03-b1c8-af8ae061a8ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Definition:** In <u> retrospective sampling</u> we get some $x_{\\cdot 1}$ diseased\n",
    "and $x_{\\cdot 0}$ non-diseased people and we observe how many are exposed, giving\n",
    "$$\n",
    "X_{10}\\sim \\text{Binomial}(x_{\\cdot 0},\\mathbb{P}(E|D^c)) \\\\\n",
    "X_{11}\\sim \\text{Binomial}(x_{\\cdot 1},\\mathbb{P}(E|D  )).\n",
    "$$\n",
    "So we have estimators \n",
    "$$\n",
    "\\hat{\\mathbb{P}}(E|D^c) = \\frac{X_{10}}{x_{\\cdot0}} ,~ \n",
    "    \\widehat{\\text{odds}}(E|D^c) = \\frac{X_{10}}{x_{\\cdot0} - X_{10}}\n",
    "\\\\\n",
    "\\hat{\\mathbb{P}}(E|D  ) = \\frac{X_{11}}{x_{\\cdot1}},~\n",
    "        \\widehat{\\text{odds}}(E|D) = \\frac{X_{11}}{x_{\\cdot1} - X_{11}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b424fec4-6921-4332-b670-ad25eeb4a20a",
   "metadata": {
    "tags": []
   },
   "source": [
    "and from those we can calculate \n",
    "$$\n",
    "\\hat{\\psi} \n",
    "=\\frac{\\widehat{\\text{odds}}(E|D)}{\\widehat{\\text{odds}}(E|D^c)}\n",
    "= \\frac{X_{00}X_{11}}{X_{01}X_{10}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095ce14-3138-4a67-aff9-f4bca4bd30e2",
   "metadata": {},
   "source": [
    "So, The meaning of $\\psi$ changes a little, but we use the same estimator in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2a227-ee04-4a77-a3c7-8098533bfd01",
   "metadata": {},
   "source": [
    "**Theorem:** The following are equivalent (characterizations of independence):\n",
    "1. $D\\perp E$\n",
    "2. $p_{ij} = p_{i\\cdot}p_{j\\cdot}$ for all $i,j$\n",
    "3. $\\psi =1$\n",
    "4. $\\gamma =0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350b996-aa94-4e71-b9f3-e3d26451723a",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "We want a statistic to test\n",
    "$$\n",
    "H_0: D\\perp E\n",
    "$$\n",
    "\n",
    "### Pearson test for independece\n",
    "**Definition** The Person statistic for independence is \n",
    "$$\n",
    "U =\\sum_{i,j} \\frac{X_{ij} - E_{ij}}{E_{ij}}\\\\\n",
    "E_{ij} = \\frac{X_{i\\cdot}X_{\\cdot j}}{n} .\n",
    "$$\n",
    "\n",
    "**Theorem:** Under $H_0$ the random variable $U$ is asymptotically distributed as $\\chi^2_1$. \n",
    "\n",
    "To see a little better, note that under $H_0$ we have $\\hat{p}_{ij} = \\hat{p}_{i\\cdot}\\hat{p}_{\\cdot j} = \\frac{X_{i\\cdot}X_{\\cdot j}}{n^2}$ and the estimated value of $X_{ij}$ is $n\\hat{p}_{ij}$.\n",
    "\n",
    "**Perason test for independence** A level $\\alpha$ test is obtained by rejecting $H_0$ when $U > F^{-1}_{\\chi^2_1}(1-\\alpha)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19611afe-1631-4062-aeac-0e550c0a85cb",
   "metadata": {},
   "source": [
    "### Likelihood ratio test\n",
    "\n",
    "**Definition:**  The <u> Likelihood ratio test statistic</u> is\n",
    "$$\n",
    "T = 2\\sum_{ij} X_{ij} \\log \\frac{X_{ij}X_{\\cdot \\cdot}}{X_{i\\cdot}X_{\\cdot j}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706fb821-9b3f-49f5-ae9f-b1eab259ccb1",
   "metadata": {},
   "source": [
    "**Theorem:** Under $H_0$ the statistic $T$ is asymptotically distributed as $\\chi^2_1$. \n",
    "\n",
    "To see more clearly, note that the argument of the log is $\\frac{\\hat{p}_{ij}}{\\hat{p}_{i\\cdot} \\hat{p}_{\\cdot j}}$, which is one if the estimate is perfect. Taylor expanding about 1, we have that $\\log(x) \\approx 0 + (x-1)$, so \n",
    "$$\n",
    "T \\approx \n",
    "\\sum_{ij} X_{ij}\\left( \\frac{X_{ij} n}{X_{i\\cdot}X_{\\cdot j}} -1 \\right)\\\\\n",
    "=\\sum_{ij} X_{ij}\\left(\\frac{X_{ij} -E_{ij} }{ E_{ij}}  \\right)\n",
    "$$\n",
    "eh... not quite\n",
    "\n",
    "**Likelihood ratio test** \n",
    "A level $\\alpha$ test is obtained by rejecting $H_0$ when $T>F^{-1}_{\\chi^1_1}(1-\\alpha)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be4c33-1e46-4645-87c3-592a195b1d4a",
   "metadata": {},
   "source": [
    "### Wald test\n",
    "Further, since the MLE of $\\phi$ and $\\gamma$ are $\\hat{\\phi} , \\hat \\gamma$ as above, they are both asymptotically normally distributed. Thus we can use a wald test on the standard unit normal distributed \n",
    "$$\n",
    "W = \\frac{\\hat{\\gamma}}{\\text{se}(\\hat \\gamma)},\n",
    "$$\n",
    " rejcting when \n",
    "$$\n",
    "0 \\notin \\left(\\hat \\gamma - \\text{se}(\\hat \\gamma) Z_{\\alpha/2},\\,\\hat \\gamma + \\text{se}(\\hat \\gamma) Z_{\\alpha/2} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a067ce3-df87-46d8-b24d-d3bed13a222f",
   "metadata": {},
   "source": [
    "## One discrete, one continuous\n",
    "\n",
    "Let $Y:\\Omega \\to \\{1,..,I\\}$ and $Z:\\Omega \\to \\mathbb{R}$. Let $F_i(z) = \\mathbb{P}(Z<z|y=i)$. \n",
    "\n",
    "**Theorem:** $Y\\perp Z \\Leftrightarrow F_i=F_j \\, \\forall i,j$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f57dd7-d201-4c94-9660-d74a089e2fa2",
   "metadata": {},
   "source": [
    "Therefore, the hypothesis $Y\\perp Z$ can be framed as \n",
    "$$\n",
    "H_0: F_1 = F_2=...=F_I.\n",
    "$$\n",
    "\n",
    "### Kolmogorov-Smirnov test: \n",
    "\n",
    "Let $\\hat{F}_i(z) = \\frac1{n_i}\\sum_{i=1}^n I(Z<z)I(Y=i)$ and define the statistic \n",
    "$$\n",
    "D_{ij} = \\sup_{x}|F_i(x) - F_j(x) |.\n",
    "$$\n",
    "Let $H(t) = 1-2\\sum\\limits_{i=1}^\\infty (-1)^{j-1}e^{-2j^2 t^2}$\n",
    "\n",
    "**Theorem:** Under $H_0$ the probability $\\mathbb{P}\\left(\\sqrt{\\frac{n_in_j}{n_i+n_j}} D_{ij}< t\\right) = H(t)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f9e34-6fd0-4ffd-b5ca-facae1cc3bf1",
   "metadata": {},
   "source": [
    "The test is then to reject $H_0$ if $D > \\sqrt{\\frac{n_i +n_j}{n_i n_j}} H^{-1}(1-\\alpha)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08b545-c385-405c-b5d8-82a16f564801",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d4e22b-038a-4ce3-8ac8-c6b6075d4018",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a07cc54-29ee-40b7-a161-66df8c37ded4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869e2ef4-2cf3-4c7d-9e06-d0eeb549a654",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8591b054-6bd6-4b8d-ad89-da214aad1edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 16 Causal Inference\n",
    " \n",
    "May 25 2024... I'm having a hard time reading this. it might be best to re-do notes on this chapter from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19cf4c-d647-4cc8-baa1-401571df462a",
   "metadata": {},
   "source": [
    "## Binary case\n",
    "Suppose that $X$ is a binary treatment variable where $X = 1$ means “treated” and $X = 0 $ means “not treated.”\n",
    "\n",
    "Let $Y$ be some outcome variable such as presence or absence of disease.\n",
    "\n",
    "We introduce two new random variables $(C_0,C_1)$, called potential outcomes; \n",
    "- $C_0$ is the outcome if the subject is not treated (X = 0) and \n",
    "- $C_1$ is the outcome if the subject is treated (X = 1).\n",
    "\n",
    "**Definition:** The <u>consistency relationship</u> is \n",
    "$$\n",
    "Y = \\left\\{  \n",
    "\\begin{array}{cc}\n",
    "C_0 {\\text{ if }} X=0 \\\\ \n",
    "C_1 {\\text{ if }} X=1 \\\\ \n",
    "\\end{array}\n",
    "\\right\\} :=C_X.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f99340-189a-44eb-a7da-34de9be3d893",
   "metadata": {},
   "source": [
    "The consistency relationship appears to me to be a deterministic ideal. Stochastic processes will imperfectly realize this relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a46263-208f-4649-a19d-b3388cf7d669",
   "metadata": {},
   "source": [
    "## Counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d09b0-13f0-46f1-a590-68c15d8e0304",
   "metadata": {},
   "source": [
    "Note that when $X=0$ we do not observe $C_1$, and so we do not get to form estimates of $\\mathbb{P}(C_1|X=0)$. There are, however, 4 classes if $Y\\in\\{0,1\\}$:\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\\hline\n",
    "\\text{type} & C_0 & C_1 \\\\\\hline\n",
    "\\text{Survivor} & 1 &1 \\\\ \n",
    "\\text{Responder} & 0 & 1 \\\\\n",
    "\\text{Anti-responder} &1& 0 \\\\\n",
    "\\text{Doomed} & 0 &0\\\\\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bfee8-0f2d-4b07-b2ee-3a1a8228021e",
   "metadata": {},
   "source": [
    "**Definition:** A <u>couterfactual</u> is a potential outcome, or the state of affairs that would have happened in the absence of the cause. \n",
    "\n",
    "Namely, $C_0|(X=1)$ is a counterfactual; it is the outcome (a person) would have experienced if counter to the fact of them recieving treatment 0 they had recieved treatment 1. Thus $\\mathbb{P}(C_0|X=1)$ is the probability of the outcome $C_0$ given that there was tretment given. \n",
    "\n",
    "Similarly for $C_1|X=0$. Since counterfactuals are the outcomes for events ($X$ values) that did not happen, they are fundamentally impossible to observre. Hence the name. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557096c3-abf4-4362-85e2-384597a6c43d",
   "metadata": {},
   "source": [
    "**Definition:** The <u>average causal effect</u>\n",
    "$\\theta = \\mathbb{E}(C_1) − \\mathbb{E}(C_0)$.\n",
    "\n",
    "Note that knowledge of counterfactuals is, in general, required to know the average causal effect; $\\mathbb{E}(C_0)$ is estimated over all patients including those with $X=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461dca7-6c69-4e48-ac37-036ce4903881",
   "metadata": {},
   "source": [
    "**Definition:** The <u>association</u> $\\alpha = \\mathbb{E}(Y|X = 1)−\\mathbb{E}(Y|X = 0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513ca1b-8a3d-477a-a18a-8fe53517ab45",
   "metadata": {},
   "source": [
    "**Theorem:** Association is not equal to causation.\n",
    "\n",
    "Proof (By counter example): If the data is augmented by the counterfactuals (marked with $*$ below) to yield the following table then\n",
    "$$\n",
    "\\theta = 0\\\\\n",
    "\\alpha = 1 .\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc}\n",
    "X& Y & C_0 &C_1 \\\\\\hline\n",
    "0 & 0 & 0 & 0^* \\\\ \n",
    "0 & 0 & 0 & 0^* \\\\ \\hline\n",
    "1 & 1 & 1^* & 1 \\\\ \n",
    "1 & 1 & 1^* & 1 \\\\ \n",
    "\\end{array} \n",
    "$$\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546c709-7c95-47f4-82d5-cc92c316c9c7",
   "metadata": {},
   "source": [
    "As an interpretation, say there is a medicine that has no effect on an illness; \n",
    "- rows 1 and 2 represent people who are  ill, they take a medicine, they do not get better, and they would not have gotten better without the medicine.\n",
    "- rows 3 and 4 represent people who are not ill, they do not take a medicine, they remain well, and they would  have remained well.\n",
    "\n",
    "Looking at association alone, one might think the medicine helped against the illness. It is the counterfactuals that show this is not the case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951f3cd-0c9f-40bf-aefc-4c102e1b46e8",
   "metadata": {},
   "source": [
    "**Theorem:** If we randomly assign subjects to treatment and $P(X = 0) > 0 \\wedge  P(X = 1) > 0$, then \n",
    "- $α = \\theta  $  \n",
    "- the emperical probability allow a consistent estimator of the average causal effect as the emperical association; $\\hat{\\theta} = \\hat{\\mathbb{E}}(Y|X=1)−\\hat{\\mathbb{E}}(Y|X=0)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2b69a-c1d7-41ae-92f0-3ad1bc6e3049",
   "metadata": {},
   "source": [
    "**Definition:** The <u>conditional causal effect</u> on the value $z$ of the random variable $Z$ is $\\theta_z =E(C_1|Z =z)−E(C_0|Z =z)$.\n",
    "\n",
    "For example, if $Z$ denotes gender with values $Z = 0$ (women) and $Z = 1$ (men), then $\\theta_0$ is the causal effect among women and $\\theta_1$ is the causal effect among men. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58b532-ed65-44de-a79f-d7cad0a1cb16",
   "metadata": {},
   "source": [
    "## Continuous case\n",
    "If $X$ is continuous then \n",
    "- the <u>consistency relation</u> is $Y ≡ C(X)$  \n",
    "- the <u>regression function</u>, which measures association, is $r(x) = \\mathbb{E}(Y |X = x)$, whose emperical estimator is the line of best fit to the set of $N$ points $\\cup_{i=1}^N\\{(x_i,C_i(x_i)) \\}$, which contains no counterfactuals.\n",
    "- the <u>causal regression function</u> is $\\theta  (x) = \\mathbb{E}(C(x))$, \n",
    "    - by this I mean the function whose emperical estimator is the average over the $N$ patients of the functions $C_i$ with graph $\\{(x,C_i(x))| x\\in{\\cal X}\\}$; that is $\\hat{\\theta  }(x) = \\frac1N \\sum_\\limits{_i=1}^N C_i(x)$. \n",
    "    - Patient $I$ recieves dose $x_i$, so we only observe $C_i(x_i)$ for patient $i$, and the rest of the graph of $C_i$ for each patient $i$ is counterfactual. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59760c7d-ce8f-481c-a7b7-d281cc7251d2",
   "metadata": {},
   "source": [
    "**Theorem:** In general, $\\theta  (x)\\neq r(x)$. However, when X is randomly assigned, $\\theta  (x) = r(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c738a6f-fb50-4875-b774-4c2103c72325",
   "metadata": {},
   "source": [
    "## 16.3 Observational Studies and Confounding\n",
    "**Definition:** A study in which treatment (or exposure) is  not randomly assigned is called an <u>observational study</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520e551-13c8-4046-a55e-ab13c7abe09a",
   "metadata": {},
   "source": [
    "In general, the potential outcome $C$ is not independent of treatment X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d29e18-9d84-487a-a544-fa051f2739ca",
   "metadata": {},
   "source": [
    "However, suppose we could find groupings of subjects such that, within groups, $X$ and $\\{C(x) : x ∈ {\\cal X}\\}$ are independent. This would happen if the subjects are very similar within groups. For example, suppose we find people who are very similar in age, gender, educational background, and ethnic background. Among these people we might feel it is reasonable to assume that the choice of X is essentially random. \n",
    "\n",
    "**Definition:** If the (vector of) random variable $Z$ satisfies $\\{C(x): x∈{\\cal X}\\}\\perp X|Z$  then $Z$ is called (a) <u>confounding variable(s)</u>. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c405e-c35a-4e06-82b4-106c0818f7d4",
   "metadata": {},
   "source": [
    "**Definition:** Confounding variables may exist, but we may not observe them; if we do not then we call them <u>unmeasured confounding variables</u>. \n",
    "\n",
    "**Definition:** If there are no such variables we say that there is <u>no unmeasured confounding</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b33a7c-7160-44b3-a587-e2257dbd1131",
   "metadata": {},
   "source": [
    "A note on tricky notation below; $X=x$ should be thought of as refering to data carried forward into emperical estimates. \n",
    "\n",
    "Recall that \n",
    "- $\\theta  (x) = \\mathbb{E}(C(x))=\\int \\mathbb{E}(C(x)|Z=z) dF_Z(Z=z)$ is the causal regression function. \n",
    "- $r(x) = \\mathbb{E}(Y |X = x)=\\int \\mathbb{E}(C(x)|Z=z,X=x) dF_Z(Z=z|X=x)$ is the regression function, which measures association\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897849d-550c-4583-a322-199c92105b8f",
   "metadata": {},
   "source": [
    "**Theorem:** If $Z$ is a counfounding variable (meaning $\\{C(x): x∈{\\cal X}\\}\\perp X|Z$) then \n",
    "$$\\theta(x) = \\int \\mathbb{E}[C(x)|X=x,Z=z]dF_Z(z) = \\int \\mathbb{E}(Y|X=x,Z=z)dF_Z(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f4542-d0c0-43b8-98a2-3fe7769fe294",
   "metadata": {},
   "source": [
    "That is, the condition for confounding variables is sufficient for caluculation of the causal regression function $\\theta$ from emperical data. \n",
    "\n",
    "**Definition:** The relationship $\\theta(x) =  \\int \\mathbb{E}(Y|X=x,Z=z)dF_Z(z)$ is the <u>adjusted treatment effect</u>.\n",
    "\n",
    "The process of computing adjusted treatment effects is called adjusting (or controlling) for confounding. \n",
    "\n",
    "\n",
    "Further, for any consistent estimator $\\hat{q}(x,z)$ of $\\mathbb{E}(C(x)|X=x,Z=z)$ we have the consistent emperical estimate \n",
    "$$\n",
    "\\hat{\\theta}(x) = \\frac1N \\sum_{i=1}^N \\hat{q}(x,Z_i). \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca946a-5da6-4476-b455-91757cc57e6d",
   "metadata": {},
   "source": [
    "In particular, note that if $\\hat{q}$ is linear (meaning that the causal regression function is linear so that it has a linear consistent estimator) and \n",
    "$$\\hat{q}(x,z) = a_0 +a_1 x+a_2 z$$\n",
    "then the induced consistent estimator of the causal regressin function is\n",
    "$$\\hat{\\theta}(x) = \\hat{a_0} + \\hat{a_1}x+ \\hat{a_2} \\hat{Z}_N$$\n",
    "where the coefficients $a_i$ are from ordinary least squares regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09546daa-a113-48e4-96c6-38a8a6126e2d",
   "metadata": {},
   "source": [
    "### Warning\n",
    "One single observational study is not, by itself, strong evidence. \n",
    "\n",
    "Even after adjusting for some confounders, we cannot be sure that there are no unmeasured confounding variables that we missed; observational studies must be treated with healthy skepticism. Consider them believable when\n",
    "- the results are replicated in many studies, \n",
    "- each of the studies controlled for plausible confounding variables, \n",
    "- there is a plausible scientific explanation for the existence of a causal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05881a96-124e-4135-8866-4f385b7bd9bf",
   "metadata": {},
   "source": [
    "## 16.4 Simpson's paradox\n",
    "\n",
    "Sometimes it appears that there is a treatment which is e.g. good for men, good for women, but bad overall. This may show up in the following triplet\n",
    "- $P(Y =1|X=1,Z=0)>P(Y =1|X=0,Z=0)$ (good for women)\n",
    "- $P(Y =1|X=1,Z=1)>P(Y =1|X=0,Z=1) $ ( good for men)\n",
    "- $P(Y = 1|X = 1) < P(Y = 1|X = 0)$ \n",
    "\n",
    "The problem is that the third can not be interpreted as \"bad overall\"; that statement would come from \n",
    "$$\n",
    "P(C_1 = 1) < P(C_0 = 1) \n",
    "$$. \n",
    "\n",
    "Ran outta time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820ef60-afd4-44ef-b78c-a7d88c856e42",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0313f95a-9332-425d-9fbf-71ab6657f4ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d85162b6-0a1e-4275-85f5-9428ec026276",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e20f166-87de-40bc-89cf-2837aa3f3f1f",
   "metadata": {},
   "source": [
    " # 17 Directed Graphs and Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84331cca-ce6c-4705-95f0-69b781568640",
   "metadata": {},
   "source": [
    "Aside: Baysian Networks is a poor chice of name for DAGs with probability distributions because statistical inference for such DAGs can be performed with either frequentist or Bayesian methods, so the term is misleading. \n",
    "\n",
    "---\n",
    "dfn: parent,child\n",
    "\n",
    "Notation: The set of all parents of $X$ is denoted by $\\pi_X$ or $\\pi(X)$.\n",
    "\n",
    "**Definition:** A sequence of adjacent vertices staring with $X$ and ending with $Y$ but ignoring the direction of the arrows is called an <u>undirected path</u>.\n",
    "\n",
    "**Definition:** $X$ is an <u>ancestor</u> of $Y$ if there is a directed path from X to Y $X = Y$. In such a case we also say that Y is a <u>descendant</u> of X.\n",
    "\n",
    "Dfn: collider\n",
    "\n",
    "Dfn: When the variables pointing into the collider are not adjacent, we say that the collider is <u>unshielded</u>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab0945-14ff-4e0b-9420-8af127115550",
   "metadata": {},
   "source": [
    "**Definition:** If ${\\cal G}$ is a DAG with vertices $V=(X_1,...,X_k)$ and the distribution $f_V$ for $V$ satisfies $f_V(v) = \\prod_{i=1}^k f_{X_i}(x_i|\\pi_{X_i})$ then <u>${\\cal G}$ represents $f_V$</u>. \n",
    "    \n",
    "aka $f$ is Markov to ${\\cal G}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b8c2a-cfe0-41f3-adc6-950eb29bf6ad",
   "metadata": {},
   "source": [
    "e.g. The graph below, ${\\cal G}=(V,E)$, represents $V$ if\n",
    "$$\n",
    "f_V(v)=f_V(x,y,z,w) = f(x)f(y)f(z|x,y)f(w|z).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ba6fd-e76d-4c2e-b175-af993bf531cd",
   "metadata": {},
   "source": [
    "<img src=\"images/dag.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aeea09-5c06-40ff-be77-635d782e98e9",
   "metadata": {},
   "source": [
    "**Notation:** The set of distributions represented by ${\\cal G}$ is denoted $M({\\cal G})$.\n",
    "\n",
    "I do not have any examples of two distributions represented by the same DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da496a-1cb1-478f-b225-d209c995d145",
   "metadata": {},
   "source": [
    "**Notation:** $\\tilde{W}$ denotes all the other variables except the parents and descendants of $W$.\n",
    "\n",
    "**Definition:** If ${\\cal G}=(V,E)$ has distribution $f_V$ that satisfies $W\\perp \\tilde{W} | \\pi_W$ for all $W\\in V$ then $f_V$ satisfies the <u>markov condition</u>.\n",
    "\n",
    "Roughly, the Markov condition is that each vertex depends only on its parents and descendants; no further ancestors matter. Note that children are not the only descendants that may matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6214f51-0fd0-41d2-90f5-bb91a72627f9",
   "metadata": {},
   "source": [
    "---\n",
    "e.g. In the image above \n",
    "- $\\tilde(X) = Y$ and $\\pi_X =\\{\\}$, \n",
    "- $\\tilde{W}=(X,Y)$ and $\\pi_W=\\{Z\\}$. \n",
    "\n",
    "Thus, a distribution on that DAG satisfies the Markov condition if \n",
    "- $X \\perp Y$ \n",
    "- $W \\perp(X,Y) | Z$.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b575aa2-1a98-4783-ac48-8581f92441c9",
   "metadata": {},
   "source": [
    "**Theorem:** The graph ${\\cal G}$ represents $f_V$ iff the markov condition holds.\n",
    "\n",
    "Said again: \n",
    "- $W\\perp \\tilde{W} | \\pi_W$ for all $W\\in V$ $\\Leftrightarrow$ $f_V(v) = \\prod_{i=1}^k f_{X_i}(x_i|\\pi_{X_i})$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00073f-a779-4654-9570-e861e55e7513",
   "metadata": {},
   "source": [
    "---\n",
    "e.g. In the graph ${\\cal G}=(V,E)$ below TFAE\n",
    "- $f_{A,B,C,D}(a,b,c,d) = f_A(a)f_B(b|a)f_C(c|a)f_D(d|b,c)f_E(e|d)$\n",
    "- $B\\perp C|A$ and $D\\perp A | (B,C)$ and $E\\perp(A,B,C)|D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d95b0a-2347-46b4-87f4-77e8c2be8d9e",
   "metadata": {},
   "source": [
    "<img src=\"images/dag2.png\" width=\"300\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edbc0c-d204-40a4-8a21-be9c55c467bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc164f57-ae13-4535-beda-6ff725abcf32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "814811a1-d400-4d2b-afe3-1149d7123e4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5e7f343-d920-4e23-8397-2c892350e824",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4fa3fe1-fb54-404b-88ee-a9ecefad9cc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "775c3f22-2714-4732-9231-785224a28af9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1401c1e7-b5c3-474a-a6c0-ad5e378d4609",
   "metadata": {},
   "source": [
    "# 18 Undirected Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848ca50-bf2c-4c3d-87f4-88a05e2d7c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
