{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3b7001-05a9-428a-8454-f2d0245ba339",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ch1: Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f773060-5cd5-4b09-a2aa-94abda3fc311",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let $\\Omega$ be the sample space of an experiment. Assume it is disctete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d45896-fe40-4aac-b409-63a71069faf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Definiton**: If $\\Omega$ is finite and if each outcome $\\omega \\in \\Omega$ is equally likely, then **uniform probability distribution** $\\mathbb{P}$ is the probability fuction such that $\\mathbb{P}(A) = \\frac{|A|}{|\\Omega|}$ for all events $A\\subset \\Omega$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a529b7-b018-4c8b-adea-6b51134b7cc4",
   "metadata": {},
   "source": [
    "(I would call that a *uniform probability function* because I do not want people to think about distributions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72d046-59bc-4426-8e72-016333a603da",
   "metadata": {},
   "source": [
    "**Definition**: Two events A and B are **independent** if $\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4608276-ec3a-435b-9885-1ff439319294",
   "metadata": {
    "tags": []
   },
   "source": [
    "e.g. Let $\\bar{6} = \\{1,...,6\\}$. Let $\\Omega = \\{(m,n)| m,n\\in \\bar6\\}$, and $\\mathbb{P}(\\{(a,b)\\}) = \\frac{1}{36}$.  \n",
    "Let $A = \\{(1,n)|n\\in \\bar6 \\},~B = \\{(m,1)|m\\in \\bar6 \\}$. \n",
    "\n",
    "**Claim**: The events A and B are independent.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "Note that $A\\cap B = \\{(1,1)\\}$.\n",
    "\n",
    "Thus, $\\mathbb{P}(A) \\mathbb{P}(B) = \\frac16\\frac16 = \\mathbb{P}(A\\cap B)$. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e9f18-e493-4eaf-abac-b114a5d20ef1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ch2 Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa456d0-3011-462a-81f7-b28812ddf5a5",
   "metadata": {},
   "source": [
    "Warning: the sample space often “disappears” in text books but it is really there in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ec503-7472-448a-b2e5-daf84872d4db",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Definition:** If the probability mass function of $X$ is \n",
    "$$f_X(x) = \\left\\{\\begin{array}{l} \\frac1k \\text{ if } x \\in \\{1,\\dots,k\\}\\\\ 0 \\text{ else } \\end{array} \\right.$$ then $X$ has a **uniform distribution** on $\\{1,\\dots,k\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f46101-724c-4d04-9030-65981de19930",
   "metadata": {},
   "source": [
    "**Definition:** Two random variables $X$ and $Y$ are **independent** if, for all  $A$, $B \\subseteq \\mathbb{R}$, the probability \n",
    "$\\mathbb{P}(X\\in A,Y \\in B)=\\mathbb{P} (X\\in A)P(Y \\in B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aacb6db-66bd-4f85-a2c0-7db20d9aa93d",
   "metadata": {},
   "source": [
    "Note the relationship betwee independent events and independent random variables; two random variables are independent if all events of the form $X\\in A$ are independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a42ee-c946-4f81-a9f4-c3a943e589f8",
   "metadata": {},
   "source": [
    "e.g. Let $\\bar{6} = \\{1,...,6\\}$. Let $\\Omega = \\{(m,n)| m,n\\in \\bar6\\}$, and $\\mathbb{P}(\\{(a,b)\\}) = \\frac{1}{36} ~\\forall (a,b) \\in \\Omega.$. \n",
    "\n",
    "Let $X:\\Omega \\to \\mathbb{R}, ~ X((m,n)) = m$ and \n",
    "$Y:\\Omega \\to \\mathbb{R}, ~ Y((m,n)) = n$.\n",
    "\n",
    "Let $A,B\\subset \\mathbb{R}$ be arbitrary. Let $\\bar A = A\\cap \\bar 6$ and $\\bar B = B\\cap \\bar 6$.\n",
    "\n",
    "Since $\\mathbb{P}(X\\in A) = \\mathbb{P}(\\{ (m,n) | m\\in \\bar A\\}) = \\frac{6|\\bar A|}{36}$ while similarly \n",
    "$\\mathbb{P}(Y\\in B) = \\frac{6|\\bar B|}{36}$, \n",
    "\n",
    "the product is $\\frac{|\\bar A||\\bar B|}{36}$, the probability of the conjunction \n",
    "$$\n",
    "\\mathbb{P}(X\\in A \\wedge Y\\in B) = \\mathbb{P}(\\{ (m,n) | m\\in \\bar A, n\\in \\bar B\\}) = \\frac{|\\bar A||\\bar B|}{36}. \\square \\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e974d-cb6a-4ef5-b0b7-51d2110fdc4d",
   "metadata": {},
   "source": [
    "## 2.5 Joint probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5dcbae-3d77-48f5-a8c6-21389b7a3b9c",
   "metadata": {},
   "source": [
    "**Definition:** Given a pair of discrete random variables $X$ and $Y$ , define the **joint mass function** by $f(x,y)=\\mathbb{P}(X=x \\text{ and } Y =y)$. \n",
    "\n",
    "\n",
    "**Definition:** In the continuous case, we call a function $f(x,y)$ a **joint pdf** for the random variables $(X, Y )$ if\n",
    "1. $f(x,y) \\geq 0 \\text{ for all } (x,y)$,\n",
    "2. $\\iint_{\\mathbb{R}^2} f(x,y)dx\\,dy = 1$ and,\n",
    "3. for any  $A ⊂ \\mathbb{R}^2$ the probability $\\mathbb{P}( (X, Y ) ∈ A) =\n",
    "\\iint_A f (x, y)dx\\,dy$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447f95f-b2a7-4835-9fb3-49b53086aeaf",
   "metadata": {},
   "source": [
    "**Theorem**. Let $X$ and $Y$ have joint pdf $f_{X,Y}$. \n",
    "Then $X$ and  $Y$ are independent if and only if $f_{X,Y} (x,y) = f_X(x)f_Y (y)$ for all $x$ and $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8c991-4c38-4ebf-98a0-b3a8d3c40942",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.6 Marginal Mass and Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d8c21-8ee4-408b-bdc8-fafc18d4eef7",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Definition**. If $(X, Y )$ have joint distribution with mass function $f_{X,Y}$ , then the **marginal mass function** for $X$ is defined by\n",
    "$f_X(x)= \\mathbb{P}(X =x )= \\sum\\limits_y \\mathbb{P}(X =x,Y =y)= \\sum\\limits_y f_{X,Y}(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d0d87-5609-4acb-9871-2c54fef09ef9",
   "metadata": {},
   "source": [
    "**Definition:** For continuous random variables, the **marginal density** for $X$ is \n",
    "$f_X(x) = \\int_\\mathbb{R} f_{X,Y}(x,y)dy$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621ad16-eea2-49f9-bcf8-26b0c0b0bebd",
   "metadata": {},
   "source": [
    "**Theorem:** Let $X$ and $Y$ have joint pdf $f_{X,Y}$. Then $X$ and  $Y$ are independent if and only if $f_{X,Y} (x,y) = f_X(x)f_Y (y)$ for all values $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67d6c1-c92e-4262-938b-9a40172e42a5",
   "metadata": {},
   "source": [
    "## 2.8 Conditional Mass and Distributions \n",
    "\n",
    "**Definition** The **conditional probability mass function** of $X$ given $Y=y$ is the function of $x$ given by \n",
    "$\n",
    "f_{X|Y} (x|y) \n",
    "= \\mathbb{P}(X = x|Y = y) \n",
    ":= \\frac{\\mathbb{P}(X = x,Y = y)}{\\mathbb{P}(Y = y)}\n",
    "= \\frac{f_{X,Y} (x,y) }{ f_Y (y)}\n",
    "$\n",
    "if $f_Y (y) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a59b91-9025-4666-b651-0976343196be",
   "metadata": {},
   "source": [
    "**Definition**. For continuous random variables, the **conditional probability density function** of $X$ given $Y=y$ is the function of $x$ given by\n",
    "$\n",
    "f_{X|Y} (x|y) \n",
    ":= \\frac{f_{X,Y} (x, y)} { f_Y (y)}\n",
    "$\n",
    "assuming that $f_Y (y) > 0$. Then,\n",
    "$\\mathbb{P}(X \\in A|Y = y) =\n",
    "\\int_\n",
    "A\n",
    "f_{X|Y} (x|y)dx$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d842bd6-b85f-4b1c-a430-78ab0b7b8178",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.9 Multivariate Distributions and iid Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968bd5f-d839-4efa-9184-5c708b317e0c",
   "metadata": {},
   "source": [
    "**Definition** If $X_1 ,\\dots , X_n$ are independent and each has the same cdf $F$, we say that $X_1,...,X_n$ are **independent and identically distributed**.\n",
    "\n",
    "**Notation**: We write iid for short, and denote \n",
    "that the common CDF is $F$ by\n",
    "$$X_1,...,X_n ∼F$$\n",
    "and \n",
    "that the common distribution is $f$ by \n",
    "$$X_1,...,X_n ∼ f.$$ \n",
    "Thats right, you just have to infer if the function appearing after the $\\sim$ is a CDF, mass functon, or PDF from context. \n",
    "\n",
    "**Definition:** If the random vector of random variables $X_1,...,X_n$ is iid the with common CDF $F$ then that random vector is **a random sample** of size $n$ from $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1232ca0-d22a-421b-878a-757570f5d910",
   "metadata": {},
   "source": [
    "**Definition** Two random variables $X$ and $Y$ are **equal in distribution** if $F_X(x)=F_Y(x)$ for all $x\\in \\mathbb{R}$.\n",
    "\n",
    "**Notation:** $X\\stackrel{d}{=} Y$.\n",
    "\n",
    "Note: Equal in distribution does not mean that $X$ and $Y$ are equal, and certainly does not mean that X and Y are independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c47ad93-e360-46f7-a523-0c19bb19540a",
   "metadata": {},
   "source": [
    "e.g. Let $\\text{Ran}(X)=\\text{Ran}(Y)={1,-1}$ with $P(X = 1) = P(X = −1) = 1/2$ and $Y =−X$. \n",
    "\n",
    "**Claim:** The random variables $X$ and $Y$ are equal in distribution but are not equal.\n",
    "\n",
    "**Proof:** Since \n",
    "$$P(X = 1)=P(Y = 1) = 1/2 \\\\\n",
    "\\text{ and } \\\\P(X = -1)=P(Y = -1) = 1/2,$$ \n",
    "it is proven by exhastion that $X\\stackrel{d}{=} Y$. \n",
    "\n",
    "To show that $X\\neq Y$ with the method of contradiction, assume $X=Y$. Using this and the defintion of $Y$ as $Y=-X$, one has $X=0$. This is in contradiction with $0\\notin \\text{Ran}(X)$. $\\square$.\n",
    "\n",
    "Note that further $P(X = Y) = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afab5d7-2528-48e7-9cc9-8b6c0fac5a57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.11 Transformations of Random Variables\n",
    "\n",
    "Three steps to compute the pdf and cdf of $Y = r(X)$ from the CDF of $X$ follow.\n",
    "1. For each $y$, find the set $A_y =\\{x: r(x)≤y\\}$.\n",
    "2. Find the cdf $F_Y(y) =\\int_{A_y} f_X (x)\\,dx$\n",
    "3. The pdf is $f_Y(y)=F_Y'(y)$.\n",
    "\n",
    "For the multivariable $Z=r(X,Y)$ the sets $A_z$ will be 2 dimensional and the cdf $F_Z(z) =\\iint_{A_z} f_{X,Y} (x,y)\\,dx\\,dy$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00878d3-55cf-4476-9797-a11b7e24d5a6",
   "metadata": {},
   "source": [
    "**Dafinition** A **statistic** is any function $T_n = g(X_1, . . . , X_n)$ of a vector of IID variables.\n",
    "\n",
    "That is, a statistic is a function the data, but formulated in terms of a random sample so that is independent of the particulars of the data via random variables with distributions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8ae67-5c2d-4582-86aa-718ebdf2dc36",
   "metadata": {},
   "source": [
    "Note that the CDF $F_{T_n}$ for a statistic $T_n$ is then \n",
    "$F_{T_n}(t) =\\idotsint_{A_t} f_{X_1,\\dots,X_n} (x_1,\\dots,x_n)\\,dx_1\\dots\\,dx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b1654-35f9-4dda-a860-c8a7c8b0ac7b",
   "metadata": {},
   "source": [
    "**e.g.** Let $X_1,...,X_n ∼ \\text{Uniform}(0,\\theta)$ be iid and let $\\hat{\\theta}_n= \\text{max}\\{X_1,...,X_n\\}$.    \n",
    "Note that \n",
    "$\\text{PDF}$ is \n",
    "$$\n",
    "f_X(x) = \n",
    "\\left\\{ \n",
    "\\begin{array}{ll} \n",
    "\\frac{1}{\\theta} & \\text{ if } & x\\in [0,\\theta]\\\\\n",
    "0 & \\text{ else}\n",
    "\\end{array}\n",
    "\\right.,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8904b5d3-5d13-429d-9cc7-111821e1d87c",
   "metadata": {},
   "source": [
    "the $\\text{CDF}$ is \n",
    "$$\n",
    "F_X(x) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll} \n",
    "0 & \\text{ if } & x \\in (-\\infty,0)\\\\\n",
    "\\frac{x}{\\theta} & \\text{ if } & x\\in[0,\\theta]\\\\\n",
    "1 & \\text{ else }\n",
    "\\end{array}\n",
    "\\right..\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c14141-dc47-4220-9f13-5763318f0e6b",
   "metadata": {},
   "source": [
    "We follow the three steps above.\n",
    "1. The set \n",
    "$$A_y =\\left\\{(x_1,\\dots,x_n)\\in [0,\\theta]^n\\,:\\, \\hat\\theta(x_1,\\dots,x_n)≤y\\right\\}\\\\\n",
    "=\\left\\{(x_1,\\dots,x_n)\\in [0,\\theta]^n \\,|\\, \\text{max}\\{x_1,\\dots,x_n)≤y\\right\\}\\\\\n",
    "=\\left\\{\n",
    "(x_1,\\dots,x_n)\\in [0,\\theta]^n)\n",
    "\\,| \\, \n",
    "x_1\\leq y \\wedge \\dots \\wedge x_n\\leq y \n",
    "\\right\\}\\\\\n",
    "=\n",
    "\\left\\{ \n",
    "\\begin{array}{l}\n",
    "\\{\\} & \\text{ if } & y \\in (\\infty,0),\\\\\n",
    "[0,y]^n & \\text{ if } & y\\in [0,\\theta],\\\\\n",
    "[0,\\theta]^n & \\text{ if } & y\\in (\\theta,\\infty)\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43cf6d-5c47-47b7-9b80-35d52e70a16b",
   "metadata": {},
   "source": [
    "2. The cdf of $\\hat{\\theta}$ on $[0,\\theta]$ is \n",
    "$$F_{\\hat\\theta}(y) =\\idotsint_{A_y} f_{X_1,\\dots,X_n} (x_1,\\dots,x_n)\\,dx_1\\,\\dots\\,dx_n\\\\  \n",
    "\\stackrel{indep}{=} \\left(\\int_0^y \\frac{1}{\\theta} dx\\right)^n \n",
    "= \\frac{y^{n}} {\\theta^n}.$$\n",
    "Thus \n",
    "$$\n",
    "F_{\\hat\\theta}(y) = \n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "0 & \\text{ if } & y \\in (-\\infty,0),\\\\\n",
    "\\frac{y^{n}} {\\theta^n} & \\text{ if } & y \\in [0, \\theta],\\\\\n",
    "1 & \\text{ if } & y \\in (\\theta,\\infty)\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "is piecewise polynomial.\n",
    "3. The pdf \n",
    "$$f_{\\hat\\theta}(y) =\n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "0 &\\text{ if }& y \\in (-\\infty,0),\\\\\n",
    "n \\frac{y^{n-1} }{\\theta^n} &\\text{ if }& y \\in [0, \\theta],\\\\\n",
    "0 &\\text{ if }& y \\in (\\theta,\\infty)\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "is zero outside the support of $X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d174e-6927-473f-b948-221410127112",
   "metadata": {},
   "source": [
    "**e.g.** Let $X_1,\\dots,X_n\\sim N(1,0)$ and let $\\hat\\mu = \\bar{X}_n = \\frac1n \\sum\\limits_{i=1}^nX_i $.\n",
    "1. The set \n",
    "$$\n",
    "A_y \n",
    "= \\left\\{  (x_1,\\dots,x_n)\\in\\mathbb{R}^n\\, | \\, \\frac1n\\sum\\limits_{i=1}^n x_i<y\\right\\} \n",
    "= \\left\\{  (x_1,\\dots,x_n)\\in\\mathbb{R}^n\\, | \\,  x_n < ny - \\sum\\limits_{i=1}^{n-1} x_i\\right\\} $$\n",
    "2. The CDF \n",
    "$$F_{\\hat\\theta}(y) \n",
    "=\\idotsint_{A_y} f_{X_1,\\dots,X_n} (x_1,\\dots,x_n)\\,dx_1\\,\\dots\\,dx_n\\\\  \n",
    "\\stackrel{indep}{=} \\int_{-\\infty}^y\n",
    "\\dots\n",
    "\\int_{-\\infty}^{ny-\\sum\\limits_{i<n-1} x_i}\n",
    "\\int_{-\\infty}^{ny-\\sum\\limits_{i<n} x_i} \n",
    "\\prod_{i=1}^n \n",
    "\\left(\n",
    "\\frac{1}{\\sqrt{2\\pi} }e^{-x_i^2/2}\n",
    "\\right) \n",
    "dx_n dx_{n-1} \\dots dx_1\\\\\n",
    "\\stackrel{indep}{=} \\int_{-\\infty}^y\n",
    "\\dots\n",
    "\\int_{-\\infty}^{ny-\\sum\\limits_{i<n-1} x_i}\n",
    "\\int_{-\\infty}^{ny-\\sum\\limits_{i<n} x_i} \n",
    "\\left(\n",
    "\\frac{1}{\\sqrt{2\\pi} }e^{- \\sum\\limits_{i=1}^n x_i^2/2}\n",
    "\\right) \n",
    "dx_n dx_{n-1} \\dots dx_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838d4d4-6d41-4904-b4e0-e185d4b60575",
   "metadata": {},
   "source": [
    "Change coordiates with a rotation, under which the dependence of the$X_i$ is invariant,  so that instead of the normal to the hyperplane pointing toward $(1,\\dots,1)^T$ the normal points toward $(0,\\dots,0,1)$. Using the fact that the distance from the origin to the hyperplane is $\\left|\\frac{1}{\\sqrt{n}}(1,\\dots,1)^T \\cdot (0,\\dots,0,ny)^T\\right| = \n",
    "\\sqrt{n}y$, in the new coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e6ef8-01b4-450d-8763-b6a8e8ab6082",
   "metadata": {},
   "source": [
    "$$F_{\\hat\\theta}(y) \n",
    "=\\int_{-\\infty}^{\\sqrt{n}y}\n",
    "\\dots\n",
    "\\int_{-\\infty}^\\infty\n",
    "\\int_{-\\infty}^\\infty\n",
    "\\prod_{i=1}^n \n",
    "\\left(\n",
    "\\frac{1}{\\sqrt{2\\pi} }e^{-x_i^2/2}\n",
    "\\right) \n",
    "dx_1 \\dots dx_n\\\\\n",
    "=\\int_{-\\infty}^{\\sqrt{n}y}\n",
    "\\frac{1}{\\sqrt{2\\pi} }e^{-x_1^2/2}\n",
    "dx_1\n",
    "\\prod_{i=2}^n \n",
    "\\int_{-\\infty}^\\infty\n",
    "\\frac{1}{\\sqrt{2\\pi} }e^{-x_i^2/2}\n",
    "dx_i\n",
    "\\\\\n",
    "=\\int_{-\\infty}^{\\sqrt{n}y}\n",
    "\\frac{1}{\\sqrt{2\\pi} }e^{-x_1^2/2}\n",
    "dx_1\n",
    "\\\\\n",
    "\\Rightarrow \n",
    "f_{\\bar{X}_n}(y) \\stackrel{FTOC}{=} \n",
    "\\frac{\\sqrt{n}}{\\sqrt{2\\pi} }e^{-n y^2/{2}}\n",
    "\\\\\n",
    "= \n",
    "\\frac{1}{ \\sigma_n\\sqrt{2\\pi} }e^{-\\frac12 \\frac{y^2}{ \\sigma_n^2} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ba8a4-3fb5-422f-8825-4ce6b663ce4c",
   "metadata": {},
   "source": [
    "where $\\sigma_n =\\frac1{\\sqrt{n}}$. \n",
    "That is $\\bar{X}_n \\sim N\\left(0,\\frac1{\\sqrt{n}}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b5b5c5-f0db-4c5b-85a2-f73c885b00f7",
   "metadata": {},
   "source": [
    "**Thoughts** We see here that the variance of the sample mean distribution is $\\sqrt{n}$. The central llimit theorem implies that the same should hold for starting with distributions other than $N(0,1)$. However, the rotation invariance of the x dependece will not happen for other distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73719542-09f4-4204-84d2-39beab7d0428",
   "metadata": {},
   "source": [
    "## Sum of RVs, convolution of PDFS\n",
    "From Cassella and Berger: The sum of two independent random variables has distribution equal to the convollution of the two distributions. \n",
    " \n",
    "**Theorem** If $X\\sim f_X,Y\\sim f_Y$ are independent continuous random variables then the PDF of $Z := X+Y$ is \n",
    "$$F_Z(z) = \\int_{\\mathbb R} f_X(w)f_Y(z-w) dw.$$\n",
    "\n",
    "**Proof:** \n",
    "\n",
    "Let \n",
    "$$\n",
    "\\left(\\begin{array}{c}\n",
    "Z(X,Y)\\\\\n",
    "W(X,Y)\\end{array}\\right) \n",
    "=\n",
    "\\left(\\begin{array}{c}\n",
    "X+Y\\\\\n",
    "X\\end{array}\\right). \n",
    "$$\n",
    "Then the Jacobian \n",
    "$$\n",
    "\\nabla \\left(\\begin{array}{c}\n",
    "Z(X,Y)\\\\\n",
    "W(X,Y)\\end{array}\\right) \n",
    "=\n",
    "\\nabla \\left(\\begin{array}{c}X+Y\\\\\n",
    "X\\end{array}\\right) \n",
    "=\n",
    "\\left(\\begin{array}{cc}1&1\\\\\n",
    "1&0 \\end{array}\\right) \n",
    "$$\n",
    "has determinant -1. \n",
    "By independence of $X$ and $Y$, the joint distribution $f_{Z,W}(z,w) = f_{X,Y} (x,y)= f_{X,Y} (w,z-w)$ is expressable as a product $f_{X} (w)f_{Y}(z-w)$. Integrating out $w$ gives the marginal PDF of $Z$ as \n",
    "$$F_Z(z) = \\int_{\\mathbb R} f_X(w)f_Y(z-w) dw. \\square$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a55e5-ba6c-4a31-97f0-442d1be2f73e",
   "metadata": {},
   "source": [
    "# Ch3 Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5b753-88f8-4519-ab5d-d9256c90a028",
   "metadata": {},
   "source": [
    "The notation $\\mathbb{E}(X)=\\int xdF(x) $ covers both the discrete and continuous case. \n",
    "\n",
    "Let $r$ be a function of $X$. Note that $\\mathbb{E}(r(X)) := \\int r(x) dF_r(x)$. \n",
    "\n",
    "Note that many texts mistakenly present the following theorem as the definition of expectation vale. \n",
    "\n",
    "**Theorem** (The Rule of the Lazy/Unconcious Statistician, LOTUS). $\\mathbb{E}(r(X)) = \\int r(x)\\,dF_X(x)$.\n",
    "\n",
    "The proof is subtle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605ad29-c082-4508-9258-d081fc16c16c",
   "metadata": {},
   "source": [
    "**Proof:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b358ad-6025-46bf-9b89-b8f695df7219",
   "metadata": {},
   "source": [
    "Let $A_y = \\{x\\, | \\, r(x) < y \\}$. Then $F_r(y) = \\int_{A_y}F_X(x)dx =\\int_{A_y} dF_X(x)$. \n",
    "\n",
    "By definition, $\\mathbb{E}[r(X)]=\\int_{y\\in \\mathbb{R}} dF_r(y)$, and so \n",
    "$$\n",
    "\\mathbb{E}[r(X)] = \\int_{y\\in \\mathbb{R}}y d\\left(  \\int_{A_y}dF_X(x) \\right)\\\\\n",
    "=\\int_{y\\in \\mathbb{R}} d  \\left( \\int_{A_y} y dF_X(x) \\right)\\\\\n",
    "=\\int_{y\\in \\mathbb{R}}  \\left( \\int_{\\partial A_y}r(x) dF_X(x) \\right).\n",
    "$$\n",
    "where $\\partial A_y = \\{x\\, | \\, r(x) = y\\}$.\n",
    "\n",
    "Next, note that $\\{ \\partial A_y \\,|\\, y\\in \\mathbb{R}\\}$ is a partition of all $x$ values, so that the integrals together form an integral over all $x$ values;\n",
    "$$\\mathbb{E}[r(X)] = \\int_{x\\in \\mathbb{R}}r(x)dF_X(x). \\square$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693e2f9-4d23-423d-b6f1-3c928fcd5671",
   "metadata": {},
   "source": [
    "**Definition:** The $k$th **moment** of $X$  is  $\\mathbb{E}\\left((X)^k \\right)$.\n",
    "\n",
    "**Definition:** The $k$th **central moment** of $X$  is  $\\mathbb{E}\\left((X − μ)^k \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d895e1f-0f96-4cdd-8b86-04c28ab963b4",
   "metadata": {},
   "source": [
    "**Definition:** The **sample mean** of the random variables $X_1, . . . , X_n$ is the random variable\n",
    "$\\bar{X_n} = \\frac1n \\sum\\limits_{i=1}^n X_n.$\n",
    "\n",
    "**Definition:** The **sample variance** of the random variables $X_1, . . . , X_n$ is the random variable\n",
    "$S^2_n = \\frac{1}{n-1} \\sum\\limits_{i=1}^n (X_i − \\bar{X_n} )^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c7e09-5121-4c72-ad0c-0e826561fbad",
   "metadata": {},
   "source": [
    "Why is the sample variance defined with a $n-1$? The more intuitive answer you hear is that calculation of the mean kills of a degree of freedom. Another answer is that \n",
    "\n",
    "**Claim** with this definition of sample variance, the $\\mathbb{E}(S_n^2) = \\sigma^2$. \n",
    "\n",
    "**Proof** \n",
    "\n",
    "$$\n",
    "\\frac1{n-1}\\mathbb{E}\\left( \\sum\\limits_{i=1}^n (X_i - \\bar X_n)^2\\right)\\\\\n",
    "=\\frac1{n-1}\\mathbb{E}\\left( \\sum\\limits_{i=1}^n (X_i^2 - 2X_i \\bar X_n+ \\bar X_n^2) \\right)\\\\\n",
    "=\\frac1{n-1}\\mathbb{E}\\left(   \\sum\\limits_{i=1}^n X_i^2 - 2 n \\bar X_n \\bar X_n+ n\\bar X_n^2 \\right)\\\\\n",
    "=\\frac1{n-1}\\mathbb{E}\\left(   \\sum\\limits_{i=1}^n X_i^2 -  n \\bar X_n^2   \\right)\\\\\n",
    "=\\frac1{n-1}\\mathbb{E}\\left[  \\sum\\limits_{i=1}^n X_i^2 -  n \\frac{1}{n^2} \\left( \\sum_{i=j} X_i^2 + \\sum_{i\\neq j} X_i X_j  \\right)  \\right]\\\\\n",
    "=\\frac1{n-1}\\left[  \n",
    "\\left( 1 - \\frac1n \\right) \n",
    "\\sum\\limits_{i=1}^n \\mathbb{E}(X_i^2 )\n",
    "-   \\frac{1}{n}  2\\sum_{i\\leq j} \\mathbb{E}(X_i X_j)    \\right]\\\\\n",
    "\\stackrel{1,2,3}\n",
    "{=}\n",
    "\\frac1{n-1}\n",
    "\\left[  \n",
    "    \\left( \\frac{n-1}{n} \\right) \n",
    "    n(\\sigma^2+\\mu^2)\n",
    "    - \\frac{1}{n}2  \\frac{n(n-1) }{2} \\mu^2    \n",
    "\\right]\\\\\n",
    "=\\sigma^2 $$\n",
    "\n",
    "where I have used that  \n",
    "1. $ \\mathbb{V}(X) = \\sigma^2 = \\mathbb{E}(X^2)-\\mu^2$\n",
    "2. $\\mathbb{E}(X_i X_j) = \\mathbb{E}(X_i)\\mathbb{E}(X_j)$ if $X_i,X_j$ \n",
    "are independent.\n",
    "3. $\\mathbb{E}(\\bar X_n) = \\mu$. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485de30e-b99b-4cc6-be8d-a2e8e5b7a917",
   "metadata": {},
   "source": [
    "See how the $n-1$ was needed? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721bbd40-e290-4f3f-8d79-61c03c696509",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}((\\bar X_n)^2) \\\\\n",
    "= \\mathbb{E} \\frac1{n^2} \\left(   \\sum X_i^2 + \\sum_{i \\neq j} X_iX_j\\right )\\\\\n",
    "= \\frac1{n^2}\\left(   n(\\sigma^2 +\\mu^2) + 2\\frac{n(n-1)}{2}\\mu^2 \\right)\\\\\n",
    "= \\frac1{n}\\left(   \\sigma^2 + \\left(1+2\\frac{n-1}{2}\\right)\\mu^2 \\right)\\\\\n",
    "=    \\frac{\\sigma^2}{n} + \\mu^2 \\\\\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301edcbd-a863-4fd8-ba59-7ac912d64318",
   "metadata": {},
   "source": [
    "## Covariance and Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e84efc-f656-4f42-ab1e-2065feaab419",
   "metadata": {},
   "source": [
    "**Definition**. The **covariance** between\n",
    "$X$ and $Y$ is\n",
    "$\\text{Cov}(X,Y)= \\mathbb{E} (X−\\mu_X)(Y −\\mu_Y)$. \n",
    "\n",
    "**Definition**. The **correlation** between\n",
    "$X$ and $Y$ is \n",
    "$\\rho = \\rho_{X,Y} = \\rho(X,Y ) = \\frac{\\text{Cov}(X,Y)}{ \\sigma_X \\sigma_Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3afcea-e382-4977-ba89-ea03a91dfa25",
   "metadata": {},
   "source": [
    "**Definition:** The **variance-covariance matrix** $Σ$ of the random vector $X = (X_1,...,X_k)^T$ is \n",
    "$$\n",
    "\\mathbb{V}(X) = \n",
    "\\left(\n",
    "\\begin{array}{ccCc}\n",
    "\\mathbb{V}(X_1)     & \\text{Cov}(X_1,X_2) & \\dots & \\text{Cov}(X_1,X_k) \\\\  \n",
    "\\text{Cov}(X_2,X_1) & \\mathbb{V}(X_2) & \\dots & \\text{Cov}(X_2,X_k) \\\\  \n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\  \n",
    "\\text{Cov}(X_k,X_1)  & \\text{Cov}(X_k,X_2) &  \\dots & \\mathbb{V}(X_k) & \\\\  \n",
    "\\end{array}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf424b4-f137-48db-ac66-cdf53e76560c",
   "metadata": {},
   "source": [
    "## 3.6 Moment Generating Functions\n",
    "\n",
    "**Definition.** The **moment generating function** (mgf, or Laplace transform), of the random variable $X$ is \n",
    "$\\psi_X (t) = \\mathbb{E}( e^{tX} ) = \\int e^{tx}dF(x)$\n",
    "where $t$ varies over the real numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30931e9c-643e-49f0-8724-e1a1a2953a9e",
   "metadata": {},
   "source": [
    "Of course, this thing is cool because $\\psi^{(k)}(0) = \\mathbb{E}(X^k)$... but further MGF transformation is a homomorphism from functions $\\Omega\\to\\mathbb{R}$ with addition to functions $\\mathbb{R} \\to \\mathbb{R}$ with multiplication, as seen in the next theorem. \n",
    "\n",
    "**Lemma** If $X_1,...,X_n$ are independent and $Y = \\sum\\limits_{i=1}^n X_i$, then $\\psi_Y (t) = \\prod\\limits_{i=1}^n\\psi_{X_i}(t)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6601c2ec-9f40-4a5d-a28c-46c70f0dcb90",
   "metadata": {},
   "source": [
    "MGFs strongly (<-my word) uniquely sepcify PDFs.\n",
    "\n",
    "**Theorem.** Let $X$ and $Y$ be random variables. If $\\psi_X (t) = \\psi_Y (t)$ for all $t$ in an open interval around 0, then $X \\stackrel{dist}{=} Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b8650-d10b-4396-8af9-537d9feca58b",
   "metadata": {},
   "source": [
    "**Example**. Let $Y_1 \\sim \\text{Poisson}(\\lambda_1)$ and $Y_2 \\sim \\text{Poisson}(\\lambda_2)$ be independent. The moment generating function of \n",
    "$Y = Y_1 + Y_2$ is \n",
    "$$\n",
    "\\psi_Y (t) = \\psi_{Y_1} (t)\\psi_{Y_2} (t) \\\\\n",
    "= e^{\\lambda_1 (e^t −1)} e^{\\lambda_2 (e^t −1)}\\\\\n",
    "= e^{(\\lambda_1 +\\lambda_2 )(e^t −1)}\n",
    "$$\n",
    "\n",
    " which is the moment generating function of a $\\text{Poisson}(\\lambda_1 + \\lambda_2)$. We have thus proved that the sum of two independent Poisson random variables has a Poisson distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58882b78-5a84-421c-8c84-893c1b203edb",
   "metadata": {},
   "source": [
    "From outside the book: The Laplace transform, like the fourier transform, maps convolutions to products.  The sum of two independent random variables has distribution equal to the convollution of the two distributions, and MGF the product of the two MGFs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8f463-1556-44d6-86af-ba73a4c52383",
   "metadata": {},
   "source": [
    "**Claim** The MGF of a convolution of PDFs is the product of the MGFs of the PDFs. \n",
    "\n",
    "**Proof** \n",
    "$$\n",
    "MGF\\left( \\int_{\\mathbb R} f_X(w)f_Y(z-w) dw \\right) (t)\\\\\n",
    "=\\int_\\mathbb{R} \\int_{\\mathbb R} e^{zt} f_X(w)f_Y(z-w) \\, dz\\, dw\\\\\n",
    "\\stackrel{\\stackrel{s = z-w}{r = w}}{=}\n",
    "\\int_\\mathbb{R} \\int_{\\mathbb R} e^{(s+r)t} f_X(r)f_Y(s) \n",
    "\\det \\nabla_{s,r} \n",
    "\\left(\\begin{array}{c} \n",
    "s+r \\\\\n",
    "r\n",
    "\\end{array} \\right)ds dr\\\\\n",
    "=\n",
    "\\int_\\mathbb{R} e^{rt} f_X(r)dr \\int_{\\mathbb R} e^{st}f_Y(s) ds \\\\\n",
    "= MGF\\left( f_X \\right)(t) MGF\\left( f_Y \\right)(t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6103a8f-1742-4bc5-996b-76cacfd22c7a",
   "metadata": {},
   "source": [
    "# Ch4 Inequalities\n",
    "\n",
    "My impression is that the inequalities presented here do nothing more than provide a foundation for proof of the central limit theorem and hypothesis testing intervals. \n",
    "- Markov's inequality\n",
    "- Chebyshev’s inequality\n",
    "- Hoeffding’s Inequality\n",
    "- Mill’s Inequality\n",
    "- Jensen’s inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d128b-ff99-4599-8acb-9d98a9d39d15",
   "metadata": {},
   "source": [
    "**Theorem 4.5** (Hoeffding’s Inequality, part 2) Let $X_1 , . . . , X_n \\sim \\text{Bernoulli}(p)$. Then, for any $\\epsilon > 0$,\n",
    "\n",
    "$$P ( |\\bar{X}_n − p| > \\epsilon ) \n",
    "\\leq  2e^{−2n \\epsilon^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61721659-5ec6-4ea4-bf59-c1c890b9222f",
   "metadata": {},
   "source": [
    "I suspect this is a wide reaching theorem because of the ability to interpret so many things as Bernoulli(n,p)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae61820-9c88-47db-8827-ea47ff96e81a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ch5: Convergence of Random Varables\n",
    "\n",
    "- In probabillity: $\\stackrel{P}{\\to}$\n",
    "$$\\mathbb{P} ( |X_n − X| > \\epsilon)\\to 0$$\n",
    "- in distribution $\\approx$ , if the distributions are $F_n$ converging to $F$ \n",
    "$$ \\lim_{n\\to \\infty} F_n(t) = F(t) ~\\forall ~ t $$\n",
    "- in quadratic mean $\\stackrel{q.m.}{\\to}$(aka \"in L2\")\n",
    "$$\\lim_{n\\to\\infty} \\mathbb{E}(X_n − X)^2 =0 $$ \n",
    "- and almost surely $\\stackrel{a.s.}{\\to}$ (aka almost everywhere $\\stackrel{a.e.}{\\to}$)\n",
    "$$\\mathbb{P}\\left(\\{s : X_n(s) \\to X(s)\\}\\right) = 1$$\n",
    "\n",
    "**Thm 5.4**: qm $\\Rightarrow$ in P $\\Rightarrow$ in dist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773f4ce-3472-4d31-ae5b-143da37058ae",
   "metadata": {},
   "source": [
    "## Weak law of large numbers\n",
    "If $X_1,...,X_n$ are iid and $\\bar{X} = \\frac1n \\sum\\limits_{i=1}^n X_i$, then $\\bar{X}_n \\stackrel{P}{\\to} \\mathbb{E}(X_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5cd99-01c9-415b-bcb0-50b760271138",
   "metadata": {},
   "source": [
    "My interpretation: \n",
    "- Easiest: The average of a large number of experiments is the population mean. \n",
    "- Harder: As the size of a sample goes to infinity, the probability of the sample mean being within epsilon of the population mean goes to zero for any epsilon. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e05b74-6c0e-4e3c-ae95-7f25228bbc61",
   "metadata": {},
   "source": [
    "## The strong law of large numbers\n",
    "Let $X_1, X_2, \\dots$ be iid. If $\\mu = \\mathbb{E}|X_1| < \\infty$ then $\\bar{X}_n\\stackrel{a.s.}{\\to} \\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec85cd-7ab8-42a2-98bc-581432fdf4d8",
   "metadata": {},
   "source": [
    "My interpretation:\n",
    "- Note that $\\mu$ here is a constant random variable vector. \n",
    "- The set of sequences $S = \\{s = x_1,..| x_i\\in\\Omega\\}$ **does** have  sequences for which the average is not $\\mu$, but the set  $N = \\{s = x_1,... | \\lim\\limits_{n\\to\\infty} n^{-1} \\sum x_i \\neq \\mu \\}$ of such sequences is of measure zero in $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3b170-da7a-486c-af91-a1a63ad5dbf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Central limit theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4960b-8625-4e9f-9f55-bd8c7aa761b3",
   "metadata": {},
   "source": [
    "**Theorem** (The central limit theorem) Let $X_1,\\dots, X_n$ be iid with mean $\\mu$ and variance $\\sigma^2$, and  let $\\bar{X}_n = \\frac1n \\sum\\limits_{i=1}^n X_i $ while $Z_n = \\frac{\\bar{X}_n -\\mu}{\\sigma}$ and $Z\\sim N(0,1)$, the unit normal distribution. \n",
    "Then $Z_n \\stackrel{In Dist}{\\to} Z$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb4bb3f-55a8-4817-836d-8ee25f420ed2",
   "metadata": {},
   "source": [
    "My Interpretation:\n",
    "- Pointwise, for every point, in the limit that the size of a sample is infinite, the scaled sampling distribution is equal to the unit normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc2b28-ba38-4f82-aeae-b811d46e77c6",
   "metadata": {},
   "source": [
    "## Alternative wordings of CLT \n",
    "- In terms of CDFs:\n",
    "$$\\lim\\limits_{n\\to \\infty} \\mathbb{P}(Z_n \\leq z) = \\int_{\\infty}^z \\frac{1}{2\\pi}  e^{-x^2/2} dx.$$\n",
    "- With sample variance $S_n$ instead of population variance $\\sigma$:\n",
    "$$\\frac{\\sqrt{n}(\\bar X_n −\\mu) }{S_n} \\stackrel{\\text{in dist} }{\\to} N(0,1).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96d7b5-3c8b-4a4f-bc3f-5744dc56bb42",
   "metadata": {},
   "source": [
    "The rate of convergence is described in terms of how close the CDFs are by the following. In short, the biggest gap between the CDFs shrinks as $\\frac{1}{\\sqrt n}$.\n",
    "## Theorem (The Berry-Ess'een Inequality)\n",
    "Let $\\Phi$ be the CDF of the unit normal distribution. Then\n",
    "$\\sup\\limits_z|\\mathbb{P}(Z_n \\leq z)− \\Phi(z)|≤ \\frac{1}{\\sqrt{n}} \\frac{33}{4}\\frac{\\mathbb{E}| X_1-\\mu|^3 }{\\sigma^3}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7967352c-7a4a-4775-aae8-ae044ceebc2e",
   "metadata": {},
   "source": [
    "## Multivariate central limit theorem\n",
    "\n",
    "Let $X_1, \\dots, X_n$ be iid random vectors where\n",
    "$$\n",
    "X_i = \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "X_{1i} \\\\ X_{2i}\\\\\\vdots\\\\X_{ki} \n",
    "\\end{array} \n",
    "\\right)\n",
    "$$\n",
    "with mean of $X_1$ the vector $\\mu \\in \\mathbb{R}^k$ and with the variance of $X_1$ the symmetric positive definite matrix $ \\Sigma$. Then $\\sqrt{n}(\\bar{X}_n − \\mu) \\stackrel{dist}{\\to} N (0, \\Sigma).$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
